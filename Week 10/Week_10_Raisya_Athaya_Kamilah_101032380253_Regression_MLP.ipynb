{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/vmDZlUdjvtskPeKywsHo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinobu357/TugasMLRaisya/blob/main/Week%2010/Week_10_Raisya_Athaya_Kamilah_101032380253_Regression_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPV-_DPsexvw",
        "outputId": "b99b4727-b252-4283-e30a-c27fea6c09d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Hubungkan ke Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch (Versi terbaru yang kompatibel dengan Google Colab)\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "# Install scikit-learn dan pandas untuk manipulasi data dan model\n",
        "!pip install -U scikit-learn pandas matplotlib\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNzCJbTpgCx6",
        "outputId": "53ce9e9f-f93b-4711-dcdc-2d0fa32203cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dBjKVHvBgaDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/drive/MyDrive/Week 10/Real estate valuation data set.xlsx\")\n",
        "\n",
        "# Tampilkan beberapa baris data\n",
        "data.head()\n",
        "\n",
        "# Drop kolom yang tidak diperlukan\n",
        "data = data.drop(['X1 transaction date'], axis=1)  # Kolom 'X1 transaction date' tidak diperlukan\n",
        "\n",
        "# Pisahkan fitur (X) dan target (Y)\n",
        "features = data.drop(['Y house price of unit area'], axis=1)\n",
        "target = data['Y house price of unit area']\n",
        "\n",
        "# Normalisasi fitur menggunakan StandardScaler\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Pisahkan data menjadi training dan test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "QciueHuJgiB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, neurons_per_layer, activation_function):\n",
        "        super(VanillaMLP, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Input ke lapisan tersembunyi pertama\n",
        "        self.hidden_layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "\n",
        "        # Lapisan tersembunyi lainnya\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            self.hidden_layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(neurons_per_layer, 1)\n",
        "\n",
        "        # Fungsi aktivasi\n",
        "        self.activation = self.get_activation_function(activation_function)\n",
        "\n",
        "    def get_activation_function(self, activation_function):\n",
        "        if activation_function == 'ReLU':\n",
        "            return nn.ReLU()\n",
        "        elif activation_function == 'Sigmoid':\n",
        "            return nn.Sigmoid()\n",
        "        elif activation_function == 'Tanh':\n",
        "            return nn.Tanh()\n",
        "        elif activation_function == 'Softmax':\n",
        "            return nn.Softmax(dim=1)  # Untuk multi-class classification (jika perlu)\n",
        "        else:  # 'Linear'\n",
        "            return nn.Identity()  # Tidak ada aktivasi untuk output layer (regresi)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.activation(layer(x))\n",
        "        return self.output_layer(x)\n"
      ],
      "metadata": {
        "id": "TBUeTscagyxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size, lr):\n",
        "    # Convert data ke tensor PyTorch\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Tentukan loss function dan optimizer\n",
        "    criterion = nn.MSELoss()  # Untuk regresi\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Latih model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(X_train_tensor.size()[0])\n",
        "        for i in range(0, X_train_tensor.size()[0], batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluasi model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_output = model(X_test_tensor)\n",
        "        test_loss = criterion(test_output, y_test_tensor)\n",
        "        print(f'Test Loss: {test_loss.item():.4f}')\n",
        "        return test_loss.item()\n"
      ],
      "metadata": {
        "id": "-y8yvK_Gg0aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi hyperparameters\n",
        "hidden_layer_choices = [1, 2, 3]\n",
        "neurons_per_layer_choices = [4, 8, 16, 32, 64]\n",
        "activation_function_choices = ['Linear', 'Sigmoid', 'ReLU', 'Softmax', 'Tanh']\n",
        "epoch_choices = [1, 10, 25, 50, 100, 250]\n",
        "lr_choices = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_size_choices = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Iterasi semua kombinasi hyperparameters\n",
        "for hidden_layers in hidden_layer_choices:\n",
        "    for neurons_per_layer in neurons_per_layer_choices:\n",
        "        for activation_function in activation_function_choices:\n",
        "            for epochs in epoch_choices:\n",
        "                for lr in lr_choices:\n",
        "                    for batch_size in batch_size_choices:\n",
        "                        print(f\"Testing: Hidden Layers={hidden_layers}, Neurons={neurons_per_layer}, Activation={activation_function}, Epochs={epochs}, LR={lr}, Batch Size={batch_size}\")\n",
        "\n",
        "                        # Membuat model dengan hyperparameter saat ini\n",
        "                        model = VanillaMLP(input_dim=X_train.shape[1],\n",
        "                                            hidden_layers=hidden_layers,\n",
        "                                            neurons_per_layer=neurons_per_layer,\n",
        "                                            activation_function=activation_function)\n",
        "                        model = model.float()  # Convert model to float32\n",
        "\n",
        "                        # Latih dan evaluasi model\n",
        "                        test_loss = train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size, lr)\n",
        "\n",
        "                        # Simpan hasil\n",
        "                        results.append({\n",
        "                            'Hidden Layers': hidden_layers,\n",
        "                            'Neurons Per Layer': neurons_per_layer,\n",
        "                            'Activation Function': activation_function,\n",
        "                            'Epochs': epochs,\n",
        "                            'Learning Rate': lr,\n",
        "                            'Batch Size': batch_size,\n",
        "                            'Test Loss': test_loss\n",
        "                        })\n",
        "\n",
        "# Konversi hasil ke DataFrame untuk analisis\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkT-7TUEg2OW",
        "outputId": "4888bca1-4229-48cd-b8e2-c7b7cfec51bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [70/100], Loss: 869.2717\n",
            "Epoch [80/100], Loss: 407.1915\n",
            "Epoch [90/100], Loss: 218.8699\n",
            "Epoch [100/100], Loss: 55.5446\n",
            "Test Loss: 161.8884\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=100, LR=0.0001, Batch Size=128\n",
            "Epoch [10/100], Loss: 1617.3279\n",
            "Epoch [20/100], Loss: 1614.6083\n",
            "Epoch [30/100], Loss: 1805.7067\n",
            "Epoch [40/100], Loss: 1389.0765\n",
            "Epoch [50/100], Loss: 1671.4528\n",
            "Epoch [60/100], Loss: 1549.2642\n",
            "Epoch [70/100], Loss: 1676.8149\n",
            "Epoch [80/100], Loss: 1229.1342\n",
            "Epoch [90/100], Loss: 1222.4729\n",
            "Epoch [100/100], Loss: 1256.3611\n",
            "Test Loss: 1034.2607\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=100, LR=0.0001, Batch Size=256\n",
            "Epoch [10/100], Loss: 1613.6138\n",
            "Epoch [20/100], Loss: 1661.9111\n",
            "Epoch [30/100], Loss: 1509.1940\n",
            "Epoch [40/100], Loss: 1663.5529\n",
            "Epoch [50/100], Loss: 1686.0442\n",
            "Epoch [60/100], Loss: 1514.2965\n",
            "Epoch [70/100], Loss: 1679.5139\n",
            "Epoch [80/100], Loss: 1565.1101\n",
            "Epoch [90/100], Loss: 1660.1996\n",
            "Epoch [100/100], Loss: 1397.8815\n",
            "Test Loss: 1337.8966\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=100, LR=0.0001, Batch Size=512\n",
            "Epoch [10/100], Loss: 1658.9603\n",
            "Epoch [20/100], Loss: 1655.9556\n",
            "Epoch [30/100], Loss: 1652.7451\n",
            "Epoch [40/100], Loss: 1649.2181\n",
            "Epoch [50/100], Loss: 1645.2344\n",
            "Epoch [60/100], Loss: 1640.6171\n",
            "Epoch [70/100], Loss: 1635.1969\n",
            "Epoch [80/100], Loss: 1628.8195\n",
            "Epoch [90/100], Loss: 1621.2793\n",
            "Epoch [100/100], Loss: 1612.3474\n",
            "Test Loss: 1441.6298\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=16\n",
            "Epoch [10/250], Loss: 18341132.0000\n",
            "Epoch [20/250], Loss: 1011550.5625\n",
            "Epoch [30/250], Loss: 541959.7500\n",
            "Epoch [40/250], Loss: 183308.3594\n",
            "Epoch [50/250], Loss: 121147.0078\n",
            "Epoch [60/250], Loss: 76513.9453\n",
            "Epoch [70/250], Loss: 42789.5703\n",
            "Epoch [80/250], Loss: 21879.9531\n",
            "Epoch [90/250], Loss: 10733.5850\n",
            "Epoch [100/250], Loss: 4549.3945\n",
            "Epoch [110/250], Loss: 2207.2288\n",
            "Epoch [120/250], Loss: 826.3882\n",
            "Epoch [130/250], Loss: 322.8252\n",
            "Epoch [140/250], Loss: 284.2578\n",
            "Epoch [150/250], Loss: 109.1022\n",
            "Epoch [160/250], Loss: 158.9064\n",
            "Epoch [170/250], Loss: 156.5094\n",
            "Epoch [180/250], Loss: 188.7431\n",
            "Epoch [190/250], Loss: 342.2007\n",
            "Epoch [200/250], Loss: 1263.7150\n",
            "Epoch [210/250], Loss: 135.8447\n",
            "Epoch [220/250], Loss: 198.3206\n",
            "Epoch [230/250], Loss: 174.5567\n",
            "Epoch [240/250], Loss: 120.0471\n",
            "Epoch [250/250], Loss: 168.6585\n",
            "Test Loss: 172.1830\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=32\n",
            "Epoch [10/250], Loss: 6124.2686\n",
            "Epoch [20/250], Loss: 4638.6182\n",
            "Epoch [30/250], Loss: 3195.9897\n",
            "Epoch [40/250], Loss: 3486.3362\n",
            "Epoch [50/250], Loss: 4204.8789\n",
            "Epoch [60/250], Loss: 3888.5664\n",
            "Epoch [70/250], Loss: 1801.3922\n",
            "Epoch [80/250], Loss: 2536.5349\n",
            "Epoch [90/250], Loss: 3037.2673\n",
            "Epoch [100/250], Loss: 3643.2815\n",
            "Epoch [110/250], Loss: 4132.9761\n",
            "Epoch [120/250], Loss: 5079.3604\n",
            "Epoch [130/250], Loss: 2506.9624\n",
            "Epoch [140/250], Loss: 5808.5786\n",
            "Epoch [150/250], Loss: 4513.5386\n",
            "Epoch [160/250], Loss: 2976.6221\n",
            "Epoch [170/250], Loss: 3734.8787\n",
            "Epoch [180/250], Loss: 4822.4727\n",
            "Epoch [190/250], Loss: 4973.0776\n",
            "Epoch [200/250], Loss: 5257.9888\n",
            "Epoch [210/250], Loss: 4258.8979\n",
            "Epoch [220/250], Loss: 4107.2041\n",
            "Epoch [230/250], Loss: 2810.5020\n",
            "Epoch [240/250], Loss: 4413.8184\n",
            "Epoch [250/250], Loss: 5000.6108\n",
            "Test Loss: 4424.3140\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=64\n",
            "Epoch [10/250], Loss: 7262.2778\n",
            "Epoch [20/250], Loss: 7135.9717\n",
            "Epoch [30/250], Loss: 7499.5107\n",
            "Epoch [40/250], Loss: 7085.3706\n",
            "Epoch [50/250], Loss: 6878.4731\n",
            "Epoch [60/250], Loss: 7248.8047\n",
            "Epoch [70/250], Loss: 7700.2393\n",
            "Epoch [80/250], Loss: 8179.6802\n",
            "Epoch [90/250], Loss: 7098.3921\n",
            "Epoch [100/250], Loss: 7227.8403\n",
            "Epoch [110/250], Loss: 7632.4453\n",
            "Epoch [120/250], Loss: 6715.8105\n",
            "Epoch [130/250], Loss: 5931.9746\n",
            "Epoch [140/250], Loss: 8664.1631\n",
            "Epoch [150/250], Loss: 7471.2295\n",
            "Epoch [160/250], Loss: 6434.3047\n",
            "Epoch [170/250], Loss: 7126.3545\n",
            "Epoch [180/250], Loss: 7229.7988\n",
            "Epoch [190/250], Loss: 7324.3735\n",
            "Epoch [200/250], Loss: 7489.1611\n",
            "Epoch [210/250], Loss: 7613.6455\n",
            "Epoch [220/250], Loss: 6743.2104\n",
            "Epoch [230/250], Loss: 6876.1641\n",
            "Epoch [240/250], Loss: 7384.8481\n",
            "Epoch [250/250], Loss: 8844.2637\n",
            "Test Loss: 15618.7783\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=128\n",
            "Epoch [10/250], Loss: 107607.8984\n",
            "Epoch [20/250], Loss: 7015.8667\n",
            "Epoch [30/250], Loss: 7011.3833\n",
            "Epoch [40/250], Loss: 7366.8516\n",
            "Epoch [50/250], Loss: 7821.0615\n",
            "Epoch [60/250], Loss: 7317.2935\n",
            "Epoch [70/250], Loss: 7427.1416\n",
            "Epoch [80/250], Loss: 6952.0586\n",
            "Epoch [90/250], Loss: 7528.5376\n",
            "Epoch [100/250], Loss: 7035.5498\n",
            "Epoch [110/250], Loss: 7071.6543\n",
            "Epoch [120/250], Loss: 7380.0259\n",
            "Epoch [130/250], Loss: 7414.2974\n",
            "Epoch [140/250], Loss: 7459.1216\n",
            "Epoch [150/250], Loss: 7369.5884\n",
            "Epoch [160/250], Loss: 7080.3750\n",
            "Epoch [170/250], Loss: 7103.9883\n",
            "Epoch [180/250], Loss: 7277.0908\n",
            "Epoch [190/250], Loss: 7384.7651\n",
            "Epoch [200/250], Loss: 7414.4517\n",
            "Epoch [210/250], Loss: 7004.8550\n",
            "Epoch [220/250], Loss: 7190.6084\n",
            "Epoch [230/250], Loss: 7503.7617\n",
            "Epoch [240/250], Loss: 7473.5625\n",
            "Epoch [250/250], Loss: 7412.8125\n",
            "Test Loss: 6925.3223\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=256\n",
            "Epoch [10/250], Loss: 17833105408.0000\n",
            "Epoch [20/250], Loss: 14612.8633\n",
            "Epoch [30/250], Loss: 11303.4736\n",
            "Epoch [40/250], Loss: 11813.6680\n",
            "Epoch [50/250], Loss: 422.9872\n",
            "Epoch [60/250], Loss: 427.3769\n",
            "Epoch [70/250], Loss: 159.2903\n",
            "Epoch [80/250], Loss: 345.4267\n",
            "Epoch [90/250], Loss: 351.5274\n",
            "Epoch [100/250], Loss: 322.8254\n",
            "Epoch [110/250], Loss: 313.1430\n",
            "Epoch [120/250], Loss: 313.7952\n",
            "Epoch [130/250], Loss: 332.7515\n",
            "Epoch [140/250], Loss: 250.8510\n",
            "Epoch [150/250], Loss: 327.8690\n",
            "Epoch [160/250], Loss: 264.6707\n",
            "Epoch [170/250], Loss: 267.4388\n",
            "Epoch [180/250], Loss: 162.6133\n",
            "Epoch [190/250], Loss: 227.1027\n",
            "Epoch [200/250], Loss: 332.8431\n",
            "Epoch [210/250], Loss: 388.8256\n",
            "Epoch [220/250], Loss: 140.6369\n",
            "Epoch [230/250], Loss: 249.4473\n",
            "Epoch [240/250], Loss: 215.4445\n",
            "Epoch [250/250], Loss: 261.1806\n",
            "Test Loss: 169.4445\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=10, Batch Size=512\n",
            "Epoch [10/250], Loss: 192.8999\n",
            "Epoch [20/250], Loss: 2448273.2500\n",
            "Epoch [30/250], Loss: 25021100.0000\n",
            "Epoch [40/250], Loss: 16549384.0000\n",
            "Epoch [50/250], Loss: 13430555.0000\n",
            "Epoch [60/250], Loss: 9840263.0000\n",
            "Epoch [70/250], Loss: 6703218.5000\n",
            "Epoch [80/250], Loss: 4305250.0000\n",
            "Epoch [90/250], Loss: 2624435.0000\n",
            "Epoch [100/250], Loss: 1522754.1250\n",
            "Epoch [110/250], Loss: 841539.6875\n",
            "Epoch [120/250], Loss: 442647.6250\n",
            "Epoch [130/250], Loss: 221211.9375\n",
            "Epoch [140/250], Loss: 104750.7422\n",
            "Epoch [150/250], Loss: 46837.9805\n",
            "Epoch [160/250], Loss: 19702.0977\n",
            "Epoch [170/250], Loss: 7781.7637\n",
            "Epoch [180/250], Loss: 7259.0205\n",
            "Epoch [190/250], Loss: 7258.9790\n",
            "Epoch [200/250], Loss: 7258.9409\n",
            "Epoch [210/250], Loss: 7258.9019\n",
            "Epoch [220/250], Loss: 7258.8643\n",
            "Epoch [230/250], Loss: 7258.8257\n",
            "Epoch [240/250], Loss: 7258.7861\n",
            "Epoch [250/250], Loss: 7258.7471\n",
            "Test Loss: 6897.5649\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=16\n",
            "Epoch [10/250], Loss: 1547.5941\n",
            "Epoch [20/250], Loss: 782.0857\n",
            "Epoch [30/250], Loss: 1214.5897\n",
            "Epoch [40/250], Loss: 604.6405\n",
            "Epoch [50/250], Loss: 391.4283\n",
            "Epoch [60/250], Loss: 292.3291\n",
            "Epoch [70/250], Loss: 353.2980\n",
            "Epoch [80/250], Loss: 323.2124\n",
            "Epoch [90/250], Loss: 258.9292\n",
            "Epoch [100/250], Loss: 284.5778\n",
            "Epoch [110/250], Loss: 267.6464\n",
            "Epoch [120/250], Loss: 283.8446\n",
            "Epoch [130/250], Loss: 184.6470\n",
            "Epoch [140/250], Loss: 66.3602\n",
            "Epoch [150/250], Loss: 154.7227\n",
            "Epoch [160/250], Loss: 164.1111\n",
            "Epoch [170/250], Loss: 598.7881\n",
            "Epoch [180/250], Loss: 148.7355\n",
            "Epoch [190/250], Loss: 231.8974\n",
            "Epoch [200/250], Loss: 151.4332\n",
            "Epoch [210/250], Loss: 125.4742\n",
            "Epoch [220/250], Loss: 189.4249\n",
            "Epoch [230/250], Loss: 72.6853\n",
            "Epoch [240/250], Loss: 160.9544\n",
            "Epoch [250/250], Loss: 314.8172\n",
            "Test Loss: 171.9314\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=32\n",
            "Epoch [10/250], Loss: 98.4059\n",
            "Epoch [20/250], Loss: 199.0959\n",
            "Epoch [30/250], Loss: 257.7854\n",
            "Epoch [40/250], Loss: 69.3694\n",
            "Epoch [50/250], Loss: 141.2333\n",
            "Epoch [60/250], Loss: 152.6667\n",
            "Epoch [70/250], Loss: 222.4510\n",
            "Epoch [80/250], Loss: 435.5977\n",
            "Epoch [90/250], Loss: 111.8753\n",
            "Epoch [100/250], Loss: 170.1584\n",
            "Epoch [110/250], Loss: 200.3642\n",
            "Epoch [120/250], Loss: 178.9651\n",
            "Epoch [130/250], Loss: 218.6849\n",
            "Epoch [140/250], Loss: 92.6712\n",
            "Epoch [150/250], Loss: 135.4893\n",
            "Epoch [160/250], Loss: 81.6619\n",
            "Epoch [170/250], Loss: 165.2462\n",
            "Epoch [180/250], Loss: 158.8028\n",
            "Epoch [190/250], Loss: 174.6204\n",
            "Epoch [200/250], Loss: 156.1471\n",
            "Epoch [210/250], Loss: 66.7626\n",
            "Epoch [220/250], Loss: 92.8566\n",
            "Epoch [230/250], Loss: 134.3290\n",
            "Epoch [240/250], Loss: 396.7259\n",
            "Epoch [250/250], Loss: 163.1671\n",
            "Test Loss: 219.0087\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=64\n",
            "Epoch [10/250], Loss: 1948.3977\n",
            "Epoch [20/250], Loss: 1721.1360\n",
            "Epoch [30/250], Loss: 2086.5071\n",
            "Epoch [40/250], Loss: 2512.9773\n",
            "Epoch [50/250], Loss: 2238.6252\n",
            "Epoch [60/250], Loss: 1788.9556\n",
            "Epoch [70/250], Loss: 2069.0698\n",
            "Epoch [80/250], Loss: 1727.9012\n",
            "Epoch [90/250], Loss: 1511.8276\n",
            "Epoch [100/250], Loss: 2340.3831\n",
            "Epoch [110/250], Loss: 1365.8843\n",
            "Epoch [120/250], Loss: 1295.7070\n",
            "Epoch [130/250], Loss: 1386.2797\n",
            "Epoch [140/250], Loss: 1129.1296\n",
            "Epoch [150/250], Loss: 1654.3613\n",
            "Epoch [160/250], Loss: 1723.1360\n",
            "Epoch [170/250], Loss: 1265.7412\n",
            "Epoch [180/250], Loss: 702.1285\n",
            "Epoch [190/250], Loss: 891.6229\n",
            "Epoch [200/250], Loss: 730.9367\n",
            "Epoch [210/250], Loss: 747.2921\n",
            "Epoch [220/250], Loss: 924.3601\n",
            "Epoch [230/250], Loss: 897.1609\n",
            "Epoch [240/250], Loss: 827.8080\n",
            "Epoch [250/250], Loss: 756.3533\n",
            "Test Loss: 668.0439\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=128\n",
            "Epoch [10/250], Loss: 18086.9609\n",
            "Epoch [20/250], Loss: 1840.2267\n",
            "Epoch [30/250], Loss: 1919.3800\n",
            "Epoch [40/250], Loss: 1844.7954\n",
            "Epoch [50/250], Loss: 2157.6570\n",
            "Epoch [60/250], Loss: 2146.8772\n",
            "Epoch [70/250], Loss: 2002.2954\n",
            "Epoch [80/250], Loss: 1793.1073\n",
            "Epoch [90/250], Loss: 1869.2163\n",
            "Epoch [100/250], Loss: 2021.1663\n",
            "Epoch [110/250], Loss: 1997.1877\n",
            "Epoch [120/250], Loss: 1856.5144\n",
            "Epoch [130/250], Loss: 1896.1215\n",
            "Epoch [140/250], Loss: 1496.0507\n",
            "Epoch [150/250], Loss: 1751.0298\n",
            "Epoch [160/250], Loss: 1735.1384\n",
            "Epoch [170/250], Loss: 1680.5912\n",
            "Epoch [180/250], Loss: 1746.6306\n",
            "Epoch [190/250], Loss: 1633.9791\n",
            "Epoch [200/250], Loss: 1540.9329\n",
            "Epoch [210/250], Loss: 1535.0259\n",
            "Epoch [220/250], Loss: 1427.9695\n",
            "Epoch [230/250], Loss: 1280.2677\n",
            "Epoch [240/250], Loss: 1397.8074\n",
            "Epoch [250/250], Loss: 1326.2766\n",
            "Test Loss: 1192.1646\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=256\n",
            "Epoch [10/250], Loss: 904.8069\n",
            "Epoch [20/250], Loss: 821.5768\n",
            "Epoch [30/250], Loss: 157.1455\n",
            "Epoch [40/250], Loss: 177.6412\n",
            "Epoch [50/250], Loss: 135.7523\n",
            "Epoch [60/250], Loss: 168.1338\n",
            "Epoch [70/250], Loss: 141.4228\n",
            "Epoch [80/250], Loss: 196.9373\n",
            "Epoch [90/250], Loss: 182.9683\n",
            "Epoch [100/250], Loss: 156.5977\n",
            "Epoch [110/250], Loss: 179.5008\n",
            "Epoch [120/250], Loss: 158.6773\n",
            "Epoch [130/250], Loss: 202.0492\n",
            "Epoch [140/250], Loss: 173.8294\n",
            "Epoch [150/250], Loss: 208.6405\n",
            "Epoch [160/250], Loss: 132.8200\n",
            "Epoch [170/250], Loss: 234.2791\n",
            "Epoch [180/250], Loss: 191.4600\n",
            "Epoch [190/250], Loss: 247.4826\n",
            "Epoch [200/250], Loss: 172.8562\n",
            "Epoch [210/250], Loss: 169.0866\n",
            "Epoch [220/250], Loss: 189.6425\n",
            "Epoch [230/250], Loss: 115.2787\n",
            "Epoch [240/250], Loss: 166.6539\n",
            "Epoch [250/250], Loss: 174.5674\n",
            "Test Loss: 170.3012\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=1, Batch Size=512\n",
            "Epoch [10/250], Loss: 1322.8024\n",
            "Epoch [20/250], Loss: 214.4358\n",
            "Epoch [30/250], Loss: 188.1097\n",
            "Epoch [40/250], Loss: 223.7159\n",
            "Epoch [50/250], Loss: 214.3932\n",
            "Epoch [60/250], Loss: 190.6024\n",
            "Epoch [70/250], Loss: 189.5641\n",
            "Epoch [80/250], Loss: 189.0461\n",
            "Epoch [90/250], Loss: 188.1847\n",
            "Epoch [100/250], Loss: 188.2038\n",
            "Epoch [110/250], Loss: 188.1221\n",
            "Epoch [120/250], Loss: 188.0933\n",
            "Epoch [130/250], Loss: 188.0979\n",
            "Epoch [140/250], Loss: 188.0916\n",
            "Epoch [150/250], Loss: 188.0905\n",
            "Epoch [160/250], Loss: 188.0907\n",
            "Epoch [170/250], Loss: 188.0906\n",
            "Epoch [180/250], Loss: 188.0905\n",
            "Epoch [190/250], Loss: 188.0905\n",
            "Epoch [200/250], Loss: 188.0905\n",
            "Epoch [210/250], Loss: 188.0905\n",
            "Epoch [220/250], Loss: 188.0905\n",
            "Epoch [230/250], Loss: 188.0905\n",
            "Epoch [240/250], Loss: 188.0905\n",
            "Epoch [250/250], Loss: 188.0905\n",
            "Test Loss: 171.9696\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=16\n",
            "Epoch [10/250], Loss: 19.4586\n",
            "Epoch [20/250], Loss: 34.8672\n",
            "Epoch [30/250], Loss: 54.0604\n",
            "Epoch [40/250], Loss: 47.6291\n",
            "Epoch [50/250], Loss: 56.2969\n",
            "Epoch [60/250], Loss: 107.0323\n",
            "Epoch [70/250], Loss: 21.1499\n",
            "Epoch [80/250], Loss: 17.2512\n",
            "Epoch [90/250], Loss: 18.8215\n",
            "Epoch [100/250], Loss: 42.0250\n",
            "Epoch [110/250], Loss: 16.2090\n",
            "Epoch [120/250], Loss: 54.5228\n",
            "Epoch [130/250], Loss: 615.3065\n",
            "Epoch [140/250], Loss: 85.9344\n",
            "Epoch [150/250], Loss: 12.5969\n",
            "Epoch [160/250], Loss: 61.8028\n",
            "Epoch [170/250], Loss: 50.3590\n",
            "Epoch [180/250], Loss: 80.2229\n",
            "Epoch [190/250], Loss: 87.1799\n",
            "Epoch [200/250], Loss: 86.6843\n",
            "Epoch [210/250], Loss: 9.0248\n",
            "Epoch [220/250], Loss: 16.7790\n",
            "Epoch [230/250], Loss: 53.2659\n",
            "Epoch [240/250], Loss: 86.6071\n",
            "Epoch [250/250], Loss: 45.6761\n",
            "Test Loss: 55.5211\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=32\n",
            "Epoch [10/250], Loss: 43.1981\n",
            "Epoch [20/250], Loss: 24.0304\n",
            "Epoch [30/250], Loss: 121.6003\n",
            "Epoch [40/250], Loss: 15.8679\n",
            "Epoch [50/250], Loss: 45.7441\n",
            "Epoch [60/250], Loss: 63.5658\n",
            "Epoch [70/250], Loss: 92.3973\n",
            "Epoch [80/250], Loss: 58.6825\n",
            "Epoch [90/250], Loss: 41.7813\n",
            "Epoch [100/250], Loss: 53.2188\n",
            "Epoch [110/250], Loss: 45.6689\n",
            "Epoch [120/250], Loss: 18.5118\n",
            "Epoch [130/250], Loss: 108.0642\n",
            "Epoch [140/250], Loss: 148.7252\n",
            "Epoch [150/250], Loss: 130.1697\n",
            "Epoch [160/250], Loss: 13.1452\n",
            "Epoch [170/250], Loss: 23.1546\n",
            "Epoch [180/250], Loss: 66.1269\n",
            "Epoch [190/250], Loss: 45.3581\n",
            "Epoch [200/250], Loss: 85.4238\n",
            "Epoch [210/250], Loss: 79.4464\n",
            "Epoch [220/250], Loss: 110.4244\n",
            "Epoch [230/250], Loss: 29.1291\n",
            "Epoch [240/250], Loss: 21.9246\n",
            "Epoch [250/250], Loss: 39.4905\n",
            "Test Loss: 57.0978\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=64\n",
            "Epoch [10/250], Loss: 1132.9153\n",
            "Epoch [20/250], Loss: 1380.7621\n",
            "Epoch [30/250], Loss: 576.1985\n",
            "Epoch [40/250], Loss: 362.7983\n",
            "Epoch [50/250], Loss: 380.8138\n",
            "Epoch [60/250], Loss: 350.7921\n",
            "Epoch [70/250], Loss: 604.7756\n",
            "Epoch [80/250], Loss: 465.7112\n",
            "Epoch [90/250], Loss: 312.6229\n",
            "Epoch [100/250], Loss: 155.9924\n",
            "Epoch [110/250], Loss: 144.7789\n",
            "Epoch [120/250], Loss: 231.8264\n",
            "Epoch [130/250], Loss: 159.5734\n",
            "Epoch [140/250], Loss: 719.9415\n",
            "Epoch [150/250], Loss: 704.8754\n",
            "Epoch [160/250], Loss: 212.3713\n",
            "Epoch [170/250], Loss: 95.5372\n",
            "Epoch [180/250], Loss: 106.6839\n",
            "Epoch [190/250], Loss: 830.8712\n",
            "Epoch [200/250], Loss: 107.4174\n",
            "Epoch [210/250], Loss: 163.9790\n",
            "Epoch [220/250], Loss: 176.1846\n",
            "Epoch [230/250], Loss: 194.5352\n",
            "Epoch [240/250], Loss: 163.8286\n",
            "Epoch [250/250], Loss: 201.5660\n",
            "Test Loss: 171.7419\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=128\n",
            "Epoch [10/250], Loss: 212.7670\n",
            "Epoch [20/250], Loss: 126.5362\n",
            "Epoch [30/250], Loss: 41.0736\n",
            "Epoch [40/250], Loss: 42.1442\n",
            "Epoch [50/250], Loss: 111.9568\n",
            "Epoch [60/250], Loss: 47.5974\n",
            "Epoch [70/250], Loss: 35.7192\n",
            "Epoch [80/250], Loss: 50.6264\n",
            "Epoch [90/250], Loss: 28.9372\n",
            "Epoch [100/250], Loss: 98.7055\n",
            "Epoch [110/250], Loss: 43.6261\n",
            "Epoch [120/250], Loss: 79.4142\n",
            "Epoch [130/250], Loss: 57.6076\n",
            "Epoch [140/250], Loss: 100.6775\n",
            "Epoch [150/250], Loss: 34.2270\n",
            "Epoch [160/250], Loss: 36.7573\n",
            "Epoch [170/250], Loss: 38.8277\n",
            "Epoch [180/250], Loss: 39.0732\n",
            "Epoch [190/250], Loss: 81.4069\n",
            "Epoch [200/250], Loss: 92.2016\n",
            "Epoch [210/250], Loss: 23.4170\n",
            "Epoch [220/250], Loss: 94.5154\n",
            "Epoch [230/250], Loss: 45.4944\n",
            "Epoch [240/250], Loss: 34.8719\n",
            "Epoch [250/250], Loss: 85.1066\n",
            "Test Loss: 38.9710\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=256\n",
            "Epoch [10/250], Loss: 993.5865\n",
            "Epoch [20/250], Loss: 81.7295\n",
            "Epoch [30/250], Loss: 65.9082\n",
            "Epoch [40/250], Loss: 74.6594\n",
            "Epoch [50/250], Loss: 46.6070\n",
            "Epoch [60/250], Loss: 99.9818\n",
            "Epoch [70/250], Loss: 37.3185\n",
            "Epoch [80/250], Loss: 56.1135\n",
            "Epoch [90/250], Loss: 53.1934\n",
            "Epoch [100/250], Loss: 48.4072\n",
            "Epoch [110/250], Loss: 116.9409\n",
            "Epoch [120/250], Loss: 48.3212\n",
            "Epoch [130/250], Loss: 42.3863\n",
            "Epoch [140/250], Loss: 38.7470\n",
            "Epoch [150/250], Loss: 38.2619\n",
            "Epoch [160/250], Loss: 108.6817\n",
            "Epoch [170/250], Loss: 43.5844\n",
            "Epoch [180/250], Loss: 44.8651\n",
            "Epoch [190/250], Loss: 28.6491\n",
            "Epoch [200/250], Loss: 87.6964\n",
            "Epoch [210/250], Loss: 29.9118\n",
            "Epoch [220/250], Loss: 63.0535\n",
            "Epoch [230/250], Loss: 36.0161\n",
            "Epoch [240/250], Loss: 40.6630\n",
            "Epoch [250/250], Loss: 43.0617\n",
            "Test Loss: 52.5108\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.1, Batch Size=512\n",
            "Epoch [10/250], Loss: 1639.2423\n",
            "Epoch [20/250], Loss: 1539.0532\n",
            "Epoch [30/250], Loss: 1247.8898\n",
            "Epoch [40/250], Loss: 793.1852\n",
            "Epoch [50/250], Loss: 355.3467\n",
            "Epoch [60/250], Loss: 188.5725\n",
            "Epoch [70/250], Loss: 210.5774\n",
            "Epoch [80/250], Loss: 193.4442\n",
            "Epoch [90/250], Loss: 188.6027\n",
            "Epoch [100/250], Loss: 189.3805\n",
            "Epoch [110/250], Loss: 188.1262\n",
            "Epoch [120/250], Loss: 188.2107\n",
            "Epoch [130/250], Loss: 188.1184\n",
            "Epoch [140/250], Loss: 188.0972\n",
            "Epoch [150/250], Loss: 188.0965\n",
            "Epoch [160/250], Loss: 188.0907\n",
            "Epoch [170/250], Loss: 188.0914\n",
            "Epoch [180/250], Loss: 188.0905\n",
            "Epoch [190/250], Loss: 188.0906\n",
            "Epoch [200/250], Loss: 188.0905\n",
            "Epoch [210/250], Loss: 188.0905\n",
            "Epoch [220/250], Loss: 188.0905\n",
            "Epoch [230/250], Loss: 188.0905\n",
            "Epoch [240/250], Loss: 188.0905\n",
            "Epoch [250/250], Loss: 188.0905\n",
            "Test Loss: 171.9713\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=16\n",
            "Epoch [10/250], Loss: 29.5448\n",
            "Epoch [20/250], Loss: 22.8771\n",
            "Epoch [30/250], Loss: 21.5095\n",
            "Epoch [40/250], Loss: 7.0070\n",
            "Epoch [50/250], Loss: 73.4513\n",
            "Epoch [60/250], Loss: 44.6007\n",
            "Epoch [70/250], Loss: 54.8538\n",
            "Epoch [80/250], Loss: 25.4536\n",
            "Epoch [90/250], Loss: 52.4616\n",
            "Epoch [100/250], Loss: 60.5919\n",
            "Epoch [110/250], Loss: 32.4364\n",
            "Epoch [120/250], Loss: 24.8455\n",
            "Epoch [130/250], Loss: 54.1012\n",
            "Epoch [140/250], Loss: 16.9650\n",
            "Epoch [150/250], Loss: 13.6759\n",
            "Epoch [160/250], Loss: 8.8387\n",
            "Epoch [170/250], Loss: 13.3422\n",
            "Epoch [180/250], Loss: 15.0960\n",
            "Epoch [190/250], Loss: 13.9361\n",
            "Epoch [200/250], Loss: 18.2591\n",
            "Epoch [210/250], Loss: 4.5499\n",
            "Epoch [220/250], Loss: 9.3002\n",
            "Epoch [230/250], Loss: 24.4133\n",
            "Epoch [240/250], Loss: 181.3149\n",
            "Epoch [250/250], Loss: 20.3588\n",
            "Test Loss: 47.3994\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=32\n",
            "Epoch [10/250], Loss: 22.0171\n",
            "Epoch [20/250], Loss: 34.8222\n",
            "Epoch [30/250], Loss: 18.1212\n",
            "Epoch [40/250], Loss: 37.1897\n",
            "Epoch [50/250], Loss: 46.1214\n",
            "Epoch [60/250], Loss: 87.8066\n",
            "Epoch [70/250], Loss: 15.5261\n",
            "Epoch [80/250], Loss: 32.6764\n",
            "Epoch [90/250], Loss: 92.7393\n",
            "Epoch [100/250], Loss: 24.9193\n",
            "Epoch [110/250], Loss: 10.5744\n",
            "Epoch [120/250], Loss: 25.7107\n",
            "Epoch [130/250], Loss: 24.5230\n",
            "Epoch [140/250], Loss: 9.7116\n",
            "Epoch [150/250], Loss: 33.6258\n",
            "Epoch [160/250], Loss: 70.8388\n",
            "Epoch [170/250], Loss: 27.3217\n",
            "Epoch [180/250], Loss: 8.9778\n",
            "Epoch [190/250], Loss: 13.2956\n",
            "Epoch [200/250], Loss: 11.0288\n",
            "Epoch [210/250], Loss: 336.1630\n",
            "Epoch [220/250], Loss: 58.4194\n",
            "Epoch [230/250], Loss: 23.6528\n",
            "Epoch [240/250], Loss: 9.5109\n",
            "Epoch [250/250], Loss: 66.8807\n",
            "Test Loss: 58.9116\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=64\n",
            "Epoch [10/250], Loss: 109.4964\n",
            "Epoch [20/250], Loss: 47.8441\n",
            "Epoch [30/250], Loss: 49.3244\n",
            "Epoch [40/250], Loss: 19.1415\n",
            "Epoch [50/250], Loss: 62.0170\n",
            "Epoch [60/250], Loss: 24.6565\n",
            "Epoch [70/250], Loss: 20.7226\n",
            "Epoch [80/250], Loss: 171.1366\n",
            "Epoch [90/250], Loss: 22.1343\n",
            "Epoch [100/250], Loss: 31.3482\n",
            "Epoch [110/250], Loss: 34.8892\n",
            "Epoch [120/250], Loss: 100.9637\n",
            "Epoch [130/250], Loss: 7.9482\n",
            "Epoch [140/250], Loss: 51.3922\n",
            "Epoch [150/250], Loss: 63.9130\n",
            "Epoch [160/250], Loss: 22.4175\n",
            "Epoch [170/250], Loss: 35.8807\n",
            "Epoch [180/250], Loss: 21.2644\n",
            "Epoch [190/250], Loss: 65.2110\n",
            "Epoch [200/250], Loss: 30.5498\n",
            "Epoch [210/250], Loss: 13.0320\n",
            "Epoch [220/250], Loss: 70.0237\n",
            "Epoch [230/250], Loss: 25.9716\n",
            "Epoch [240/250], Loss: 20.4875\n",
            "Epoch [250/250], Loss: 20.8746\n",
            "Test Loss: 41.0064\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=128\n",
            "Epoch [10/250], Loss: 97.9407\n",
            "Epoch [20/250], Loss: 145.7495\n",
            "Epoch [30/250], Loss: 114.0693\n",
            "Epoch [40/250], Loss: 36.4407\n",
            "Epoch [50/250], Loss: 47.2318\n",
            "Epoch [60/250], Loss: 28.4258\n",
            "Epoch [70/250], Loss: 126.0604\n",
            "Epoch [80/250], Loss: 86.6147\n",
            "Epoch [90/250], Loss: 30.5752\n",
            "Epoch [100/250], Loss: 41.5506\n",
            "Epoch [110/250], Loss: 30.0745\n",
            "Epoch [120/250], Loss: 37.2571\n",
            "Epoch [130/250], Loss: 44.2330\n",
            "Epoch [140/250], Loss: 94.3229\n",
            "Epoch [150/250], Loss: 66.5854\n",
            "Epoch [160/250], Loss: 37.3242\n",
            "Epoch [170/250], Loss: 46.1630\n",
            "Epoch [180/250], Loss: 44.3116\n",
            "Epoch [190/250], Loss: 32.9190\n",
            "Epoch [200/250], Loss: 29.7019\n",
            "Epoch [210/250], Loss: 51.4751\n",
            "Epoch [220/250], Loss: 32.8640\n",
            "Epoch [230/250], Loss: 35.4197\n",
            "Epoch [240/250], Loss: 74.9457\n",
            "Epoch [250/250], Loss: 35.3980\n",
            "Test Loss: 54.8789\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=256\n",
            "Epoch [10/250], Loss: 141.7795\n",
            "Epoch [20/250], Loss: 74.9392\n",
            "Epoch [30/250], Loss: 87.5550\n",
            "Epoch [40/250], Loss: 119.1796\n",
            "Epoch [50/250], Loss: 122.6209\n",
            "Epoch [60/250], Loss: 50.5844\n",
            "Epoch [70/250], Loss: 44.8468\n",
            "Epoch [80/250], Loss: 104.7375\n",
            "Epoch [90/250], Loss: 57.8113\n",
            "Epoch [100/250], Loss: 50.8012\n",
            "Epoch [110/250], Loss: 88.4327\n",
            "Epoch [120/250], Loss: 36.4106\n",
            "Epoch [130/250], Loss: 58.0124\n",
            "Epoch [140/250], Loss: 29.4611\n",
            "Epoch [150/250], Loss: 50.8833\n",
            "Epoch [160/250], Loss: 106.8298\n",
            "Epoch [170/250], Loss: 27.6204\n",
            "Epoch [180/250], Loss: 39.2549\n",
            "Epoch [190/250], Loss: 50.0497\n",
            "Epoch [200/250], Loss: 36.2706\n",
            "Epoch [210/250], Loss: 39.4783\n",
            "Epoch [220/250], Loss: 23.8382\n",
            "Epoch [230/250], Loss: 26.3575\n",
            "Epoch [240/250], Loss: 29.0651\n",
            "Epoch [250/250], Loss: 98.5058\n",
            "Test Loss: 46.0857\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.01, Batch Size=512\n",
            "Epoch [10/250], Loss: 564.1342\n",
            "Epoch [20/250], Loss: 175.9621\n",
            "Epoch [30/250], Loss: 126.5473\n",
            "Epoch [40/250], Loss: 84.3372\n",
            "Epoch [50/250], Loss: 74.3829\n",
            "Epoch [60/250], Loss: 69.6578\n",
            "Epoch [70/250], Loss: 66.5917\n",
            "Epoch [80/250], Loss: 65.1241\n",
            "Epoch [90/250], Loss: 63.8232\n",
            "Epoch [100/250], Loss: 62.7941\n",
            "Epoch [110/250], Loss: 61.9372\n",
            "Epoch [120/250], Loss: 61.2113\n",
            "Epoch [130/250], Loss: 60.4879\n",
            "Epoch [140/250], Loss: 59.7824\n",
            "Epoch [150/250], Loss: 59.0430\n",
            "Epoch [160/250], Loss: 58.2820\n",
            "Epoch [170/250], Loss: 57.5637\n",
            "Epoch [180/250], Loss: 56.9194\n",
            "Epoch [190/250], Loss: 56.2976\n",
            "Epoch [200/250], Loss: 55.7147\n",
            "Epoch [210/250], Loss: 55.0960\n",
            "Epoch [220/250], Loss: 54.4777\n",
            "Epoch [230/250], Loss: 53.8991\n",
            "Epoch [240/250], Loss: 53.3346\n",
            "Epoch [250/250], Loss: 52.7541\n",
            "Test Loss: 39.7526\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=16\n",
            "Epoch [10/250], Loss: 22.9501\n",
            "Epoch [20/250], Loss: 16.0681\n",
            "Epoch [30/250], Loss: 24.9476\n",
            "Epoch [40/250], Loss: 84.7972\n",
            "Epoch [50/250], Loss: 143.0085\n",
            "Epoch [60/250], Loss: 6.2316\n",
            "Epoch [70/250], Loss: 32.7742\n",
            "Epoch [80/250], Loss: 35.4473\n",
            "Epoch [90/250], Loss: 24.6905\n",
            "Epoch [100/250], Loss: 24.0064\n",
            "Epoch [110/250], Loss: 43.5270\n",
            "Epoch [120/250], Loss: 28.8499\n",
            "Epoch [130/250], Loss: 447.4156\n",
            "Epoch [140/250], Loss: 46.4335\n",
            "Epoch [150/250], Loss: 20.0233\n",
            "Epoch [160/250], Loss: 8.4602\n",
            "Epoch [170/250], Loss: 20.1399\n",
            "Epoch [180/250], Loss: 436.2193\n",
            "Epoch [190/250], Loss: 16.9061\n",
            "Epoch [200/250], Loss: 80.3099\n",
            "Epoch [210/250], Loss: 10.8172\n",
            "Epoch [220/250], Loss: 14.1912\n",
            "Epoch [230/250], Loss: 18.7349\n",
            "Epoch [240/250], Loss: 51.7943\n",
            "Epoch [250/250], Loss: 111.8179\n",
            "Test Loss: 39.8158\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=32\n",
            "Epoch [10/250], Loss: 59.1953\n",
            "Epoch [20/250], Loss: 53.6901\n",
            "Epoch [30/250], Loss: 54.9635\n",
            "Epoch [40/250], Loss: 51.7589\n",
            "Epoch [50/250], Loss: 63.4165\n",
            "Epoch [60/250], Loss: 103.3527\n",
            "Epoch [70/250], Loss: 16.2940\n",
            "Epoch [80/250], Loss: 21.9630\n",
            "Epoch [90/250], Loss: 58.8008\n",
            "Epoch [100/250], Loss: 100.5275\n",
            "Epoch [110/250], Loss: 38.6946\n",
            "Epoch [120/250], Loss: 53.6420\n",
            "Epoch [130/250], Loss: 35.2006\n",
            "Epoch [140/250], Loss: 41.6902\n",
            "Epoch [150/250], Loss: 23.7484\n",
            "Epoch [160/250], Loss: 53.3351\n",
            "Epoch [170/250], Loss: 24.4830\n",
            "Epoch [180/250], Loss: 10.0004\n",
            "Epoch [190/250], Loss: 11.6166\n",
            "Epoch [200/250], Loss: 58.5041\n",
            "Epoch [210/250], Loss: 25.9536\n",
            "Epoch [220/250], Loss: 12.2656\n",
            "Epoch [230/250], Loss: 49.9676\n",
            "Epoch [240/250], Loss: 12.2467\n",
            "Epoch [250/250], Loss: 487.1172\n",
            "Test Loss: 37.8464\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=64\n",
            "Epoch [10/250], Loss: 294.5060\n",
            "Epoch [20/250], Loss: 32.4371\n",
            "Epoch [30/250], Loss: 24.6992\n",
            "Epoch [40/250], Loss: 47.0073\n",
            "Epoch [50/250], Loss: 122.8624\n",
            "Epoch [60/250], Loss: 67.2837\n",
            "Epoch [70/250], Loss: 29.1733\n",
            "Epoch [80/250], Loss: 29.9980\n",
            "Epoch [90/250], Loss: 492.0192\n",
            "Epoch [100/250], Loss: 27.6701\n",
            "Epoch [110/250], Loss: 160.4525\n",
            "Epoch [120/250], Loss: 19.5439\n",
            "Epoch [130/250], Loss: 34.5865\n",
            "Epoch [140/250], Loss: 17.5803\n",
            "Epoch [150/250], Loss: 9.9777\n",
            "Epoch [160/250], Loss: 35.7054\n",
            "Epoch [170/250], Loss: 15.0284\n",
            "Epoch [180/250], Loss: 58.6519\n",
            "Epoch [190/250], Loss: 65.2077\n",
            "Epoch [200/250], Loss: 14.3363\n",
            "Epoch [210/250], Loss: 46.0457\n",
            "Epoch [220/250], Loss: 27.5156\n",
            "Epoch [230/250], Loss: 34.4952\n",
            "Epoch [240/250], Loss: 57.9830\n",
            "Epoch [250/250], Loss: 27.9091\n",
            "Test Loss: 47.3284\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1304.0736\n",
            "Epoch [20/250], Loss: 332.8644\n",
            "Epoch [30/250], Loss: 65.5150\n",
            "Epoch [40/250], Loss: 70.3306\n",
            "Epoch [50/250], Loss: 42.4565\n",
            "Epoch [60/250], Loss: 59.5429\n",
            "Epoch [70/250], Loss: 128.2942\n",
            "Epoch [80/250], Loss: 55.1634\n",
            "Epoch [90/250], Loss: 137.1607\n",
            "Epoch [100/250], Loss: 52.8621\n",
            "Epoch [110/250], Loss: 72.8275\n",
            "Epoch [120/250], Loss: 51.4162\n",
            "Epoch [130/250], Loss: 34.7080\n",
            "Epoch [140/250], Loss: 46.1922\n",
            "Epoch [150/250], Loss: 41.6133\n",
            "Epoch [160/250], Loss: 34.0237\n",
            "Epoch [170/250], Loss: 34.8436\n",
            "Epoch [180/250], Loss: 56.0845\n",
            "Epoch [190/250], Loss: 55.1082\n",
            "Epoch [200/250], Loss: 95.8631\n",
            "Epoch [210/250], Loss: 52.0066\n",
            "Epoch [220/250], Loss: 28.0531\n",
            "Epoch [230/250], Loss: 40.3985\n",
            "Epoch [240/250], Loss: 98.6261\n",
            "Epoch [250/250], Loss: 45.6215\n",
            "Test Loss: 40.0785\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1520.8695\n",
            "Epoch [20/250], Loss: 1363.7145\n",
            "Epoch [30/250], Loss: 665.9911\n",
            "Epoch [40/250], Loss: 191.1344\n",
            "Epoch [50/250], Loss: 96.7693\n",
            "Epoch [60/250], Loss: 150.0364\n",
            "Epoch [70/250], Loss: 73.5169\n",
            "Epoch [80/250], Loss: 83.5457\n",
            "Epoch [90/250], Loss: 65.6685\n",
            "Epoch [100/250], Loss: 47.4404\n",
            "Epoch [110/250], Loss: 63.1215\n",
            "Epoch [120/250], Loss: 49.2208\n",
            "Epoch [130/250], Loss: 131.8454\n",
            "Epoch [140/250], Loss: 46.2425\n",
            "Epoch [150/250], Loss: 49.2555\n",
            "Epoch [160/250], Loss: 59.6925\n",
            "Epoch [170/250], Loss: 120.3937\n",
            "Epoch [180/250], Loss: 59.4458\n",
            "Epoch [190/250], Loss: 52.0245\n",
            "Epoch [200/250], Loss: 64.7171\n",
            "Epoch [210/250], Loss: 28.0996\n",
            "Epoch [220/250], Loss: 47.6627\n",
            "Epoch [230/250], Loss: 59.9006\n",
            "Epoch [240/250], Loss: 48.5975\n",
            "Epoch [250/250], Loss: 26.8881\n",
            "Test Loss: 51.5643\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1639.7131\n",
            "Epoch [20/250], Loss: 1586.0062\n",
            "Epoch [30/250], Loss: 1463.6454\n",
            "Epoch [40/250], Loss: 1216.1104\n",
            "Epoch [50/250], Loss: 812.9553\n",
            "Epoch [60/250], Loss: 359.3432\n",
            "Epoch [70/250], Loss: 149.6693\n",
            "Epoch [80/250], Loss: 139.3747\n",
            "Epoch [90/250], Loss: 107.0116\n",
            "Epoch [100/250], Loss: 103.8832\n",
            "Epoch [110/250], Loss: 95.7617\n",
            "Epoch [120/250], Loss: 91.4249\n",
            "Epoch [130/250], Loss: 87.4586\n",
            "Epoch [140/250], Loss: 84.4979\n",
            "Epoch [150/250], Loss: 81.8856\n",
            "Epoch [160/250], Loss: 79.6950\n",
            "Epoch [170/250], Loss: 77.8324\n",
            "Epoch [180/250], Loss: 76.2543\n",
            "Epoch [190/250], Loss: 74.8825\n",
            "Epoch [200/250], Loss: 73.6616\n",
            "Epoch [210/250], Loss: 72.5760\n",
            "Epoch [220/250], Loss: 71.6190\n",
            "Epoch [230/250], Loss: 70.7492\n",
            "Epoch [240/250], Loss: 69.9403\n",
            "Epoch [250/250], Loss: 69.1721\n",
            "Test Loss: 55.1754\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=16\n",
            "Epoch [10/250], Loss: 1026.9620\n",
            "Epoch [20/250], Loss: 254.8610\n",
            "Epoch [30/250], Loss: 739.1375\n",
            "Epoch [40/250], Loss: 103.4507\n",
            "Epoch [50/250], Loss: 39.2216\n",
            "Epoch [60/250], Loss: 41.1725\n",
            "Epoch [70/250], Loss: 127.0061\n",
            "Epoch [80/250], Loss: 31.2175\n",
            "Epoch [90/250], Loss: 20.5444\n",
            "Epoch [100/250], Loss: 103.2542\n",
            "Epoch [110/250], Loss: 39.3983\n",
            "Epoch [120/250], Loss: 161.3487\n",
            "Epoch [130/250], Loss: 32.0329\n",
            "Epoch [140/250], Loss: 35.9011\n",
            "Epoch [150/250], Loss: 72.4437\n",
            "Epoch [160/250], Loss: 21.6387\n",
            "Epoch [170/250], Loss: 8.8026\n",
            "Epoch [180/250], Loss: 55.5250\n",
            "Epoch [190/250], Loss: 107.5264\n",
            "Epoch [200/250], Loss: 14.2104\n",
            "Epoch [210/250], Loss: 19.4215\n",
            "Epoch [220/250], Loss: 30.4233\n",
            "Epoch [230/250], Loss: 67.7362\n",
            "Epoch [240/250], Loss: 13.6211\n",
            "Epoch [250/250], Loss: 52.5092\n",
            "Test Loss: 50.3307\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=32\n",
            "Epoch [10/250], Loss: 1237.3556\n",
            "Epoch [20/250], Loss: 1465.3169\n",
            "Epoch [30/250], Loss: 1831.6847\n",
            "Epoch [40/250], Loss: 218.6640\n",
            "Epoch [50/250], Loss: 59.9064\n",
            "Epoch [60/250], Loss: 34.5763\n",
            "Epoch [70/250], Loss: 54.7182\n",
            "Epoch [80/250], Loss: 31.2194\n",
            "Epoch [90/250], Loss: 46.7289\n",
            "Epoch [100/250], Loss: 28.2254\n",
            "Epoch [110/250], Loss: 36.3158\n",
            "Epoch [120/250], Loss: 48.3493\n",
            "Epoch [130/250], Loss: 615.7073\n",
            "Epoch [140/250], Loss: 88.3183\n",
            "Epoch [150/250], Loss: 32.6511\n",
            "Epoch [160/250], Loss: 160.0878\n",
            "Epoch [170/250], Loss: 56.3960\n",
            "Epoch [180/250], Loss: 154.3148\n",
            "Epoch [190/250], Loss: 36.5868\n",
            "Epoch [200/250], Loss: 33.7330\n",
            "Epoch [210/250], Loss: 59.5519\n",
            "Epoch [220/250], Loss: 78.6559\n",
            "Epoch [230/250], Loss: 26.0782\n",
            "Epoch [240/250], Loss: 30.0570\n",
            "Epoch [250/250], Loss: 20.4948\n",
            "Test Loss: 54.8933\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=64\n",
            "Epoch [10/250], Loss: 1825.4181\n",
            "Epoch [20/250], Loss: 1419.2168\n",
            "Epoch [30/250], Loss: 1636.0804\n",
            "Epoch [40/250], Loss: 1283.8135\n",
            "Epoch [50/250], Loss: 816.2910\n",
            "Epoch [60/250], Loss: 388.4645\n",
            "Epoch [70/250], Loss: 432.6687\n",
            "Epoch [80/250], Loss: 287.5660\n",
            "Epoch [90/250], Loss: 74.4430\n",
            "Epoch [100/250], Loss: 107.4706\n",
            "Epoch [110/250], Loss: 127.0461\n",
            "Epoch [120/250], Loss: 75.6213\n",
            "Epoch [130/250], Loss: 118.0037\n",
            "Epoch [140/250], Loss: 100.5597\n",
            "Epoch [150/250], Loss: 61.5816\n",
            "Epoch [160/250], Loss: 49.9869\n",
            "Epoch [170/250], Loss: 43.5871\n",
            "Epoch [180/250], Loss: 81.0466\n",
            "Epoch [190/250], Loss: 56.4603\n",
            "Epoch [200/250], Loss: 30.8512\n",
            "Epoch [210/250], Loss: 43.6900\n",
            "Epoch [220/250], Loss: 46.8519\n",
            "Epoch [230/250], Loss: 90.8718\n",
            "Epoch [240/250], Loss: 33.2534\n",
            "Epoch [250/250], Loss: 624.9159\n",
            "Test Loss: 58.6003\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1795.5875\n",
            "Epoch [20/250], Loss: 1633.7635\n",
            "Epoch [30/250], Loss: 1633.1398\n",
            "Epoch [40/250], Loss: 1545.0211\n",
            "Epoch [50/250], Loss: 1589.2218\n",
            "Epoch [60/250], Loss: 1511.3788\n",
            "Epoch [70/250], Loss: 1515.9948\n",
            "Epoch [80/250], Loss: 1361.0635\n",
            "Epoch [90/250], Loss: 1515.6793\n",
            "Epoch [100/250], Loss: 1301.2507\n",
            "Epoch [110/250], Loss: 1007.1773\n",
            "Epoch [120/250], Loss: 912.8389\n",
            "Epoch [130/250], Loss: 667.8475\n",
            "Epoch [140/250], Loss: 513.0151\n",
            "Epoch [150/250], Loss: 375.1284\n",
            "Epoch [160/250], Loss: 308.5841\n",
            "Epoch [170/250], Loss: 160.9129\n",
            "Epoch [180/250], Loss: 118.0371\n",
            "Epoch [190/250], Loss: 129.4560\n",
            "Epoch [200/250], Loss: 81.8279\n",
            "Epoch [210/250], Loss: 98.7571\n",
            "Epoch [220/250], Loss: 166.8656\n",
            "Epoch [230/250], Loss: 62.1835\n",
            "Epoch [240/250], Loss: 62.7185\n",
            "Epoch [250/250], Loss: 89.0722\n",
            "Test Loss: 77.0182\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1562.8826\n",
            "Epoch [20/250], Loss: 1793.1958\n",
            "Epoch [30/250], Loss: 1481.6548\n",
            "Epoch [40/250], Loss: 1852.5846\n",
            "Epoch [50/250], Loss: 1701.6801\n",
            "Epoch [60/250], Loss: 1760.7427\n",
            "Epoch [70/250], Loss: 1339.2866\n",
            "Epoch [80/250], Loss: 1500.5162\n",
            "Epoch [90/250], Loss: 1518.6152\n",
            "Epoch [100/250], Loss: 1559.2913\n",
            "Epoch [110/250], Loss: 1432.1891\n",
            "Epoch [120/250], Loss: 1365.7959\n",
            "Epoch [130/250], Loss: 1402.8525\n",
            "Epoch [140/250], Loss: 970.2198\n",
            "Epoch [150/250], Loss: 1048.2665\n",
            "Epoch [160/250], Loss: 1100.5339\n",
            "Epoch [170/250], Loss: 852.1888\n",
            "Epoch [180/250], Loss: 877.8965\n",
            "Epoch [190/250], Loss: 644.3567\n",
            "Epoch [200/250], Loss: 681.1786\n",
            "Epoch [210/250], Loss: 513.8386\n",
            "Epoch [220/250], Loss: 307.2985\n",
            "Epoch [230/250], Loss: 367.7169\n",
            "Epoch [240/250], Loss: 395.1014\n",
            "Epoch [250/250], Loss: 213.2266\n",
            "Test Loss: 215.6529\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=ReLU, Epochs=250, LR=0.0001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1659.4371\n",
            "Epoch [20/250], Loss: 1657.4067\n",
            "Epoch [30/250], Loss: 1655.3081\n",
            "Epoch [40/250], Loss: 1653.0374\n",
            "Epoch [50/250], Loss: 1650.4976\n",
            "Epoch [60/250], Loss: 1647.5731\n",
            "Epoch [70/250], Loss: 1644.1332\n",
            "Epoch [80/250], Loss: 1640.0525\n",
            "Epoch [90/250], Loss: 1635.1923\n",
            "Epoch [100/250], Loss: 1629.3867\n",
            "Epoch [110/250], Loss: 1622.3969\n",
            "Epoch [120/250], Loss: 1613.9390\n",
            "Epoch [130/250], Loss: 1603.8059\n",
            "Epoch [140/250], Loss: 1591.9375\n",
            "Epoch [150/250], Loss: 1578.2502\n",
            "Epoch [160/250], Loss: 1562.5607\n",
            "Epoch [170/250], Loss: 1544.7415\n",
            "Epoch [180/250], Loss: 1524.5917\n",
            "Epoch [190/250], Loss: 1501.9464\n",
            "Epoch [200/250], Loss: 1476.6091\n",
            "Epoch [210/250], Loss: 1448.3832\n",
            "Epoch [220/250], Loss: 1417.1379\n",
            "Epoch [230/250], Loss: 1382.8185\n",
            "Epoch [240/250], Loss: 1345.4126\n",
            "Epoch [250/250], Loss: 1304.9528\n",
            "Test Loss: 1152.3553\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=16\n",
            "Test Loss: 169.8568\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=32\n",
            "Test Loss: 191.4230\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=64\n",
            "Test Loss: 142.3515\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=128\n",
            "Test Loss: 473.2526\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=256\n",
            "Test Loss: 177.3123\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=10, Batch Size=512\n",
            "Test Loss: 436.2490\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=16\n",
            "Test Loss: 171.9453\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=32\n",
            "Test Loss: 394.6571\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=64\n",
            "Test Loss: 773.8553\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=128\n",
            "Test Loss: 1085.9592\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=256\n",
            "Test Loss: 1206.5245\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=1, Batch Size=512\n",
            "Test Loss: 1350.9786\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=16\n",
            "Test Loss: 1185.2773\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=32\n",
            "Test Loss: 1317.8615\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=64\n",
            "Test Loss: 1401.2568\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=128\n",
            "Test Loss: 1443.4003\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=256\n",
            "Test Loss: 1466.5228\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.1, Batch Size=512\n",
            "Test Loss: 1475.1265\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=16\n",
            "Test Loss: 1453.7257\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=32\n",
            "Test Loss: 1477.0859\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=64\n",
            "Test Loss: 1480.7959\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=128\n",
            "Test Loss: 1476.6077\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=256\n",
            "Test Loss: 1484.3477\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.01, Batch Size=512\n",
            "Test Loss: 1492.6931\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=16\n",
            "Test Loss: 1490.0792\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=32\n",
            "Test Loss: 1487.8921\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=64\n",
            "Test Loss: 1477.7545\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=128\n",
            "Test Loss: 1489.0015\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=256\n",
            "Test Loss: 1490.7922\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.001, Batch Size=512\n",
            "Test Loss: 1491.0188\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=16\n",
            "Test Loss: 1488.2711\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=32\n",
            "Test Loss: 1490.9451\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=64\n",
            "Test Loss: 1495.8584\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=128\n",
            "Test Loss: 1480.3564\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=256\n",
            "Test Loss: 1491.3777\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=1, LR=0.0001, Batch Size=512\n",
            "Test Loss: 1480.6538\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=16\n",
            "Epoch [10/10], Loss: 248.5436\n",
            "Test Loss: 243.5637\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=32\n",
            "Epoch [10/10], Loss: 178.5730\n",
            "Test Loss: 179.0023\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=64\n",
            "Epoch [10/10], Loss: 129.1496\n",
            "Test Loss: 97.6074\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=128\n",
            "Epoch [10/10], Loss: 183.5824\n",
            "Test Loss: 134.7540\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=256\n",
            "Epoch [10/10], Loss: 145.2266\n",
            "Test Loss: 199.9651\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=10, Batch Size=512\n",
            "Epoch [10/10], Loss: 197.2537\n",
            "Test Loss: 181.5888\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=16\n",
            "Epoch [10/10], Loss: 697.7631\n",
            "Test Loss: 167.7775\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=32\n",
            "Epoch [10/10], Loss: 52.1126\n",
            "Test Loss: 174.8625\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=64\n",
            "Epoch [10/10], Loss: 113.3765\n",
            "Test Loss: 168.6603\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=128\n",
            "Epoch [10/10], Loss: 167.6445\n",
            "Test Loss: 235.5672\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=256\n",
            "Epoch [10/10], Loss: 204.4965\n",
            "Test Loss: 168.2820\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=1, Batch Size=512\n",
            "Epoch [10/10], Loss: 637.6611\n",
            "Test Loss: 465.7070\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=16\n",
            "Epoch [10/10], Loss: 418.4922\n",
            "Test Loss: 172.1667\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=32\n",
            "Epoch [10/10], Loss: 115.3582\n",
            "Test Loss: 330.0154\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=64\n",
            "Epoch [10/10], Loss: 1125.2277\n",
            "Test Loss: 690.8001\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=128\n",
            "Epoch [10/10], Loss: 1275.6034\n",
            "Test Loss: 1063.8916\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=256\n",
            "Epoch [10/10], Loss: 1255.0031\n",
            "Test Loss: 1206.4807\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.1, Batch Size=512\n",
            "Epoch [10/10], Loss: 1524.8950\n",
            "Test Loss: 1344.4163\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=16\n",
            "Epoch [10/10], Loss: 1142.2827\n",
            "Test Loss: 1132.2523\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=32\n",
            "Epoch [10/10], Loss: 1272.6405\n",
            "Test Loss: 1310.8043\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=64\n",
            "Epoch [10/10], Loss: 1949.2397\n",
            "Test Loss: 1391.9299\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=128\n",
            "Epoch [10/10], Loss: 1436.6807\n",
            "Test Loss: 1439.0764\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=256\n",
            "Epoch [10/10], Loss: 1708.5210\n",
            "Test Loss: 1462.8849\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.01, Batch Size=512\n",
            "Epoch [10/10], Loss: 1644.4731\n",
            "Test Loss: 1470.2144\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=16\n",
            "Epoch [10/10], Loss: 1541.4438\n",
            "Test Loss: 1446.8010\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=32\n",
            "Epoch [10/10], Loss: 1719.7914\n",
            "Test Loss: 1465.8350\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=64\n",
            "Epoch [10/10], Loss: 1527.3457\n",
            "Test Loss: 1485.3608\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=128\n",
            "Epoch [10/10], Loss: 1599.7494\n",
            "Test Loss: 1491.3954\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=256\n",
            "Epoch [10/10], Loss: 1670.7351\n",
            "Test Loss: 1487.4230\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.001, Batch Size=512\n",
            "Epoch [10/10], Loss: 1660.3190\n",
            "Test Loss: 1486.5909\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=16\n",
            "Epoch [10/10], Loss: 1648.0273\n",
            "Test Loss: 1482.6066\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=32\n",
            "Epoch [10/10], Loss: 2820.8928\n",
            "Test Loss: 1479.9375\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=64\n",
            "Epoch [10/10], Loss: 1631.9199\n",
            "Test Loss: 1495.2671\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=128\n",
            "Epoch [10/10], Loss: 1700.9375\n",
            "Test Loss: 1489.9685\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=256\n",
            "Epoch [10/10], Loss: 1721.4379\n",
            "Test Loss: 1495.6782\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=10, LR=0.0001, Batch Size=512\n",
            "Epoch [10/10], Loss: 1658.7670\n",
            "Test Loss: 1485.2614\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=16\n",
            "Epoch [10/25], Loss: 86.2067\n",
            "Epoch [20/25], Loss: 49.9929\n",
            "Test Loss: 131.2896\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=32\n",
            "Epoch [10/25], Loss: 699.7883\n",
            "Epoch [20/25], Loss: 131.8237\n",
            "Test Loss: 94.7454\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=64\n",
            "Epoch [10/25], Loss: 190.8607\n",
            "Epoch [20/25], Loss: 750.5888\n",
            "Test Loss: 187.7870\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=128\n",
            "Epoch [10/25], Loss: 175.3863\n",
            "Epoch [20/25], Loss: 150.6902\n",
            "Test Loss: 179.3265\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=256\n",
            "Epoch [10/25], Loss: 221.5074\n",
            "Epoch [20/25], Loss: 180.7810\n",
            "Test Loss: 172.3097\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=10, Batch Size=512\n",
            "Epoch [10/25], Loss: 182.9758\n",
            "Epoch [20/25], Loss: 186.5261\n",
            "Test Loss: 143.6478\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=16\n",
            "Epoch [10/25], Loss: 236.3009\n",
            "Epoch [20/25], Loss: 248.8629\n",
            "Test Loss: 173.4395\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=32\n",
            "Epoch [10/25], Loss: 249.6541\n",
            "Epoch [20/25], Loss: 182.5265\n",
            "Test Loss: 172.1022\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=64\n",
            "Epoch [10/25], Loss: 145.2980\n",
            "Epoch [20/25], Loss: 261.6376\n",
            "Test Loss: 179.2331\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=128\n",
            "Epoch [10/25], Loss: 197.6607\n",
            "Epoch [20/25], Loss: 272.0146\n",
            "Test Loss: 174.9232\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=256\n",
            "Epoch [10/25], Loss: 162.9143\n",
            "Epoch [20/25], Loss: 288.1296\n",
            "Test Loss: 168.2408\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=1, Batch Size=512\n",
            "Epoch [10/25], Loss: 619.8832\n",
            "Epoch [20/25], Loss: 195.6289\n",
            "Test Loss: 209.0184\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=16\n",
            "Epoch [10/25], Loss: 163.1770\n",
            "Epoch [20/25], Loss: 113.8670\n",
            "Test Loss: 120.8356\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=32\n",
            "Epoch [10/25], Loss: 503.6385\n",
            "Epoch [20/25], Loss: 149.6130\n",
            "Test Loss: 168.0044\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=64\n",
            "Epoch [10/25], Loss: 1355.5083\n",
            "Epoch [20/25], Loss: 174.1232\n",
            "Test Loss: 220.8908\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=128\n",
            "Epoch [10/25], Loss: 1288.5658\n",
            "Epoch [20/25], Loss: 974.0035\n",
            "Test Loss: 539.0706\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=256\n",
            "Epoch [10/25], Loss: 1305.4597\n",
            "Epoch [20/25], Loss: 1127.4503\n",
            "Test Loss: 809.4054\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.1, Batch Size=512\n",
            "Epoch [10/25], Loss: 1521.0736\n",
            "Epoch [20/25], Loss: 1372.0538\n",
            "Test Loss: 1132.4425\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=16\n",
            "Epoch [10/25], Loss: 1380.1042\n",
            "Epoch [20/25], Loss: 861.3735\n",
            "Test Loss: 704.0895\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=32\n",
            "Epoch [10/25], Loss: 1800.0925\n",
            "Epoch [20/25], Loss: 1423.6283\n",
            "Test Loss: 1007.3973\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=64\n",
            "Epoch [10/25], Loss: 1509.1868\n",
            "Epoch [20/25], Loss: 1398.6735\n",
            "Test Loss: 1247.6219\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=128\n",
            "Epoch [10/25], Loss: 1611.0267\n",
            "Epoch [20/25], Loss: 1470.8240\n",
            "Test Loss: 1372.5739\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=256\n",
            "Epoch [10/25], Loss: 1551.4197\n",
            "Epoch [20/25], Loss: 1745.4169\n",
            "Test Loss: 1408.2333\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.01, Batch Size=512\n",
            "Epoch [10/25], Loss: 1651.6316\n",
            "Epoch [20/25], Loss: 1635.7177\n",
            "Test Loss: 1454.4728\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=16\n",
            "Epoch [10/25], Loss: 1515.5874\n",
            "Epoch [20/25], Loss: 1379.8958\n",
            "Test Loss: 1396.8624\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=32\n",
            "Epoch [10/25], Loss: 1578.1450\n",
            "Epoch [20/25], Loss: 1310.5878\n",
            "Test Loss: 1451.6688\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=64\n",
            "Epoch [10/25], Loss: 1721.2026\n",
            "Epoch [20/25], Loss: 1467.7928\n",
            "Test Loss: 1471.3842\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=128\n",
            "Epoch [10/25], Loss: 1506.4127\n",
            "Epoch [20/25], Loss: 1659.2567\n",
            "Test Loss: 1476.0969\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=256\n",
            "Epoch [10/25], Loss: 1566.7866\n",
            "Epoch [20/25], Loss: 1485.8965\n",
            "Test Loss: 1476.0051\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.001, Batch Size=512\n",
            "Epoch [10/25], Loss: 1654.1940\n",
            "Epoch [20/25], Loss: 1652.5649\n",
            "Test Loss: 1478.4822\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=16\n",
            "Epoch [10/25], Loss: 1740.1090\n",
            "Epoch [20/25], Loss: 1379.5514\n",
            "Test Loss: 1484.2931\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=32\n",
            "Epoch [10/25], Loss: 1720.6938\n",
            "Epoch [20/25], Loss: 1402.2587\n",
            "Test Loss: 1481.1808\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=64\n",
            "Epoch [10/25], Loss: 1705.7635\n",
            "Epoch [20/25], Loss: 1762.0332\n",
            "Test Loss: 1495.1223\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=128\n",
            "Epoch [10/25], Loss: 1953.0566\n",
            "Epoch [20/25], Loss: 1771.1619\n",
            "Test Loss: 1493.7524\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=256\n",
            "Epoch [10/25], Loss: 1512.0898\n",
            "Epoch [20/25], Loss: 1613.7340\n",
            "Test Loss: 1492.6976\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=25, LR=0.0001, Batch Size=512\n",
            "Epoch [10/25], Loss: 1657.1672\n",
            "Epoch [20/25], Loss: 1657.0034\n",
            "Test Loss: 1483.5146\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=16\n",
            "Epoch [10/50], Loss: 154.8255\n",
            "Epoch [20/50], Loss: 129.6014\n",
            "Epoch [30/50], Loss: 208.3793\n",
            "Epoch [40/50], Loss: 164.9274\n",
            "Epoch [50/50], Loss: 260.8575\n",
            "Test Loss: 153.7727\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=32\n",
            "Epoch [10/50], Loss: 348.2930\n",
            "Epoch [20/50], Loss: 198.9092\n",
            "Epoch [30/50], Loss: 196.7638\n",
            "Epoch [40/50], Loss: 261.7204\n",
            "Epoch [50/50], Loss: 187.0671\n",
            "Test Loss: 167.7935\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=64\n",
            "Epoch [10/50], Loss: 112.7248\n",
            "Epoch [20/50], Loss: 95.4029\n",
            "Epoch [30/50], Loss: 174.1911\n",
            "Epoch [40/50], Loss: 105.0971\n",
            "Epoch [50/50], Loss: 227.9692\n",
            "Test Loss: 171.2660\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=128\n",
            "Epoch [10/50], Loss: 125.5097\n",
            "Epoch [20/50], Loss: 168.6561\n",
            "Epoch [30/50], Loss: 213.7748\n",
            "Epoch [40/50], Loss: 127.2058\n",
            "Epoch [50/50], Loss: 184.5943\n",
            "Test Loss: 129.8196\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=256\n",
            "Epoch [10/50], Loss: 124.1585\n",
            "Epoch [20/50], Loss: 156.3099\n",
            "Epoch [30/50], Loss: 149.2957\n",
            "Epoch [40/50], Loss: 174.8631\n",
            "Epoch [50/50], Loss: 128.4721\n",
            "Test Loss: 145.7143\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=10, Batch Size=512\n",
            "Epoch [10/50], Loss: 222.4170\n",
            "Epoch [20/50], Loss: 178.1423\n",
            "Epoch [30/50], Loss: 188.4793\n",
            "Epoch [40/50], Loss: 192.2777\n",
            "Epoch [50/50], Loss: 188.1731\n",
            "Test Loss: 168.7788\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=16\n",
            "Epoch [10/50], Loss: 194.4212\n",
            "Epoch [20/50], Loss: 220.8631\n",
            "Epoch [30/50], Loss: 55.6855\n",
            "Epoch [40/50], Loss: 210.3781\n",
            "Epoch [50/50], Loss: 291.3840\n",
            "Test Loss: 167.8829\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=32\n",
            "Epoch [10/50], Loss: 142.5013\n",
            "Epoch [20/50], Loss: 126.5761\n",
            "Epoch [30/50], Loss: 242.2510\n",
            "Epoch [40/50], Loss: 139.4025\n",
            "Epoch [50/50], Loss: 165.3561\n",
            "Test Loss: 170.4152\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=64\n",
            "Epoch [10/50], Loss: 156.7945\n",
            "Epoch [20/50], Loss: 125.1694\n",
            "Epoch [30/50], Loss: 117.0652\n",
            "Epoch [40/50], Loss: 205.8928\n",
            "Epoch [50/50], Loss: 123.9855\n",
            "Test Loss: 168.2817\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=128\n",
            "Epoch [10/50], Loss: 233.2580\n",
            "Epoch [20/50], Loss: 170.0398\n",
            "Epoch [30/50], Loss: 252.5894\n",
            "Epoch [40/50], Loss: 274.7052\n",
            "Epoch [50/50], Loss: 261.7885\n",
            "Test Loss: 171.4788\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=256\n",
            "Epoch [10/50], Loss: 157.7693\n",
            "Epoch [20/50], Loss: 279.0967\n",
            "Epoch [30/50], Loss: 147.5360\n",
            "Epoch [40/50], Loss: 161.9239\n",
            "Epoch [50/50], Loss: 162.5902\n",
            "Test Loss: 171.7978\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=1, Batch Size=512\n",
            "Epoch [10/50], Loss: 626.5800\n",
            "Epoch [20/50], Loss: 196.4266\n",
            "Epoch [30/50], Loss: 223.8510\n",
            "Epoch [40/50], Loss: 199.7123\n",
            "Epoch [50/50], Loss: 189.3916\n",
            "Test Loss: 168.2026\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=16\n",
            "Epoch [10/50], Loss: 161.7576\n",
            "Epoch [20/50], Loss: 648.6212\n",
            "Epoch [30/50], Loss: 228.4319\n",
            "Epoch [40/50], Loss: 137.0797\n",
            "Epoch [50/50], Loss: 182.9788\n",
            "Test Loss: 172.3034\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=32\n",
            "Epoch [10/50], Loss: 223.2726\n",
            "Epoch [20/50], Loss: 89.7417\n",
            "Epoch [30/50], Loss: 180.9056\n",
            "Epoch [40/50], Loss: 125.7090\n",
            "Epoch [50/50], Loss: 201.1851\n",
            "Test Loss: 172.0775\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=64\n",
            "Epoch [10/50], Loss: 786.1013\n",
            "Epoch [20/50], Loss: 212.6203\n",
            "Epoch [30/50], Loss: 267.1586\n",
            "Epoch [40/50], Loss: 168.0130\n",
            "Epoch [50/50], Loss: 173.2736\n",
            "Test Loss: 168.6786\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=128\n",
            "Epoch [10/50], Loss: 1241.9615\n",
            "Epoch [20/50], Loss: 634.1282\n",
            "Epoch [30/50], Loss: 436.9417\n",
            "Epoch [40/50], Loss: 303.9730\n",
            "Epoch [50/50], Loss: 223.4962\n",
            "Test Loss: 221.5387\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=256\n",
            "Epoch [10/50], Loss: 1365.3506\n",
            "Epoch [20/50], Loss: 923.1225\n",
            "Epoch [30/50], Loss: 810.6955\n",
            "Epoch [40/50], Loss: 587.8958\n",
            "Epoch [50/50], Loss: 409.6332\n",
            "Test Loss: 368.6165\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.1, Batch Size=512\n",
            "Epoch [10/50], Loss: 1523.9137\n",
            "Epoch [20/50], Loss: 1375.4792\n",
            "Epoch [30/50], Loss: 1230.4652\n",
            "Epoch [40/50], Loss: 1085.5189\n",
            "Epoch [50/50], Loss: 946.8019\n",
            "Test Loss: 805.0538\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=16\n",
            "Epoch [10/50], Loss: 1158.6481\n",
            "Epoch [20/50], Loss: 1059.2045\n",
            "Epoch [30/50], Loss: 626.0958\n",
            "Epoch [40/50], Loss: 561.4070\n",
            "Epoch [50/50], Loss: 571.1905\n",
            "Test Loss: 375.5243\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=32\n",
            "Epoch [10/50], Loss: 1853.8773\n",
            "Epoch [20/50], Loss: 1537.7172\n",
            "Epoch [30/50], Loss: 694.6876\n",
            "Epoch [40/50], Loss: 824.7271\n",
            "Epoch [50/50], Loss: 748.5302\n",
            "Test Loss: 696.8233\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=64\n",
            "Epoch [10/50], Loss: 1447.1884\n",
            "Epoch [20/50], Loss: 1252.7773\n",
            "Epoch [30/50], Loss: 1561.6067\n",
            "Epoch [40/50], Loss: 1008.2895\n",
            "Epoch [50/50], Loss: 996.3066\n",
            "Test Loss: 961.5660\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=128\n",
            "Epoch [10/50], Loss: 1385.9408\n",
            "Epoch [20/50], Loss: 1674.4655\n",
            "Epoch [30/50], Loss: 1298.2440\n",
            "Epoch [40/50], Loss: 1466.6129\n",
            "Epoch [50/50], Loss: 1156.4852\n",
            "Test Loss: 1243.1179\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=256\n",
            "Epoch [10/50], Loss: 1744.0281\n",
            "Epoch [20/50], Loss: 1738.9944\n",
            "Epoch [30/50], Loss: 1601.3669\n",
            "Epoch [40/50], Loss: 1463.1305\n",
            "Epoch [50/50], Loss: 1614.7207\n",
            "Test Loss: 1336.8583\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.01, Batch Size=512\n",
            "Epoch [10/50], Loss: 1640.3533\n",
            "Epoch [20/50], Loss: 1624.1547\n",
            "Epoch [30/50], Loss: 1608.0002\n",
            "Epoch [40/50], Loss: 1591.7677\n",
            "Epoch [50/50], Loss: 1575.3561\n",
            "Test Loss: 1404.8319\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=16\n",
            "Epoch [10/50], Loss: 1307.2535\n",
            "Epoch [20/50], Loss: 1377.9006\n",
            "Epoch [30/50], Loss: 1838.8687\n",
            "Epoch [40/50], Loss: 1929.1718\n",
            "Epoch [50/50], Loss: 1225.9731\n",
            "Test Loss: 1320.2657\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=32\n",
            "Epoch [10/50], Loss: 1496.9116\n",
            "Epoch [20/50], Loss: 1458.7429\n",
            "Epoch [30/50], Loss: 1331.8993\n",
            "Epoch [40/50], Loss: 1960.6031\n",
            "Epoch [50/50], Loss: 1776.6932\n",
            "Test Loss: 1408.5442\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=64\n",
            "Epoch [10/50], Loss: 2342.9221\n",
            "Epoch [20/50], Loss: 1809.2892\n",
            "Epoch [30/50], Loss: 1750.0847\n",
            "Epoch [40/50], Loss: 1301.2917\n",
            "Epoch [50/50], Loss: 2705.9565\n",
            "Test Loss: 1450.1528\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=128\n",
            "Epoch [10/50], Loss: 1786.4434\n",
            "Epoch [20/50], Loss: 1583.1284\n",
            "Epoch [30/50], Loss: 1606.1692\n",
            "Epoch [40/50], Loss: 1705.3782\n",
            "Epoch [50/50], Loss: 1598.2950\n",
            "Test Loss: 1464.4669\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=256\n",
            "Epoch [10/50], Loss: 1450.7980\n",
            "Epoch [20/50], Loss: 1557.2932\n",
            "Epoch [30/50], Loss: 1593.3065\n",
            "Epoch [40/50], Loss: 1707.1826\n",
            "Epoch [50/50], Loss: 1668.5697\n",
            "Test Loss: 1471.1782\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.001, Batch Size=512\n",
            "Epoch [10/50], Loss: 1660.0334\n",
            "Epoch [20/50], Loss: 1658.4058\n",
            "Epoch [30/50], Loss: 1656.7783\n",
            "Epoch [40/50], Loss: 1655.1516\n",
            "Epoch [50/50], Loss: 1653.5248\n",
            "Test Loss: 1480.1617\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=16\n",
            "Epoch [10/50], Loss: 2094.0393\n",
            "Epoch [20/50], Loss: 1670.6620\n",
            "Epoch [30/50], Loss: 1379.1858\n",
            "Epoch [40/50], Loss: 1781.8612\n",
            "Epoch [50/50], Loss: 1044.2164\n",
            "Test Loss: 1465.9388\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=32\n",
            "Epoch [10/50], Loss: 972.7039\n",
            "Epoch [20/50], Loss: 1549.3224\n",
            "Epoch [30/50], Loss: 1203.8495\n",
            "Epoch [40/50], Loss: 1954.5856\n",
            "Epoch [50/50], Loss: 1671.0050\n",
            "Test Loss: 1481.8802\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=64\n",
            "Epoch [10/50], Loss: 1694.9279\n",
            "Epoch [20/50], Loss: 1908.3558\n",
            "Epoch [30/50], Loss: 1464.3254\n",
            "Epoch [40/50], Loss: 3159.6736\n",
            "Epoch [50/50], Loss: 2143.4583\n",
            "Test Loss: 1474.5261\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=128\n",
            "Epoch [10/50], Loss: 1556.8353\n",
            "Epoch [20/50], Loss: 1558.4235\n",
            "Epoch [30/50], Loss: 1775.4696\n",
            "Epoch [40/50], Loss: 1743.5291\n",
            "Epoch [50/50], Loss: 1640.8662\n",
            "Test Loss: 1491.1907\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=256\n",
            "Epoch [10/50], Loss: 1670.6735\n",
            "Epoch [20/50], Loss: 1761.6315\n",
            "Epoch [30/50], Loss: 1619.4225\n",
            "Epoch [40/50], Loss: 1571.6758\n",
            "Epoch [50/50], Loss: 1705.4332\n",
            "Test Loss: 1480.4550\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=50, LR=0.0001, Batch Size=512\n",
            "Epoch [10/50], Loss: 1666.9561\n",
            "Epoch [20/50], Loss: 1666.7917\n",
            "Epoch [30/50], Loss: 1666.6274\n",
            "Epoch [40/50], Loss: 1666.4631\n",
            "Epoch [50/50], Loss: 1666.2987\n",
            "Test Loss: 1492.3905\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=16\n",
            "Epoch [10/100], Loss: 225.2905\n",
            "Epoch [20/100], Loss: 145.5060\n",
            "Epoch [30/100], Loss: 106.5587\n",
            "Epoch [40/100], Loss: 187.0366\n",
            "Epoch [50/100], Loss: 354.8633\n",
            "Epoch [60/100], Loss: 79.3131\n",
            "Epoch [70/100], Loss: 218.8999\n",
            "Epoch [80/100], Loss: 220.2880\n",
            "Epoch [90/100], Loss: 248.0499\n",
            "Epoch [100/100], Loss: 180.9755\n",
            "Test Loss: 192.3648\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=32\n",
            "Epoch [10/100], Loss: 120.4311\n",
            "Epoch [20/100], Loss: 259.4537\n",
            "Epoch [30/100], Loss: 110.5389\n",
            "Epoch [40/100], Loss: 164.1831\n",
            "Epoch [50/100], Loss: 73.4423\n",
            "Epoch [60/100], Loss: 277.7454\n",
            "Epoch [70/100], Loss: 202.4838\n",
            "Epoch [80/100], Loss: 113.7125\n",
            "Epoch [90/100], Loss: 289.7511\n",
            "Epoch [100/100], Loss: 142.2837\n",
            "Test Loss: 168.1576\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=64\n",
            "Epoch [10/100], Loss: 162.9461\n",
            "Epoch [20/100], Loss: 65.6348\n",
            "Epoch [30/100], Loss: 63.4144\n",
            "Epoch [40/100], Loss: 126.5648\n",
            "Epoch [50/100], Loss: 550.9191\n",
            "Epoch [60/100], Loss: 63.6306\n",
            "Epoch [70/100], Loss: 179.0355\n",
            "Epoch [80/100], Loss: 181.4693\n",
            "Epoch [90/100], Loss: 69.3928\n",
            "Epoch [100/100], Loss: 551.8077\n",
            "Test Loss: 142.8179\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=128\n",
            "Epoch [10/100], Loss: 194.7214\n",
            "Epoch [20/100], Loss: 248.3669\n",
            "Epoch [30/100], Loss: 191.5955\n",
            "Epoch [40/100], Loss: 139.6479\n",
            "Epoch [50/100], Loss: 252.2161\n",
            "Epoch [60/100], Loss: 191.4663\n",
            "Epoch [70/100], Loss: 157.3764\n",
            "Epoch [80/100], Loss: 182.8879\n",
            "Epoch [90/100], Loss: 190.5654\n",
            "Epoch [100/100], Loss: 169.7129\n",
            "Test Loss: 168.2743\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=256\n",
            "Epoch [10/100], Loss: 159.8872\n",
            "Epoch [20/100], Loss: 200.3430\n",
            "Epoch [30/100], Loss: 87.3358\n",
            "Epoch [40/100], Loss: 84.6642\n",
            "Epoch [50/100], Loss: 107.1964\n",
            "Epoch [60/100], Loss: 185.3897\n",
            "Epoch [70/100], Loss: 94.7496\n",
            "Epoch [80/100], Loss: 89.5397\n",
            "Epoch [90/100], Loss: 75.6310\n",
            "Epoch [100/100], Loss: 113.3272\n",
            "Test Loss: 81.9076\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=10, Batch Size=512\n",
            "Epoch [10/100], Loss: 164.5706\n",
            "Epoch [20/100], Loss: 132.9250\n",
            "Epoch [30/100], Loss: 121.7865\n",
            "Epoch [40/100], Loss: 125.8964\n",
            "Epoch [50/100], Loss: 127.5456\n",
            "Epoch [60/100], Loss: 127.2428\n",
            "Epoch [70/100], Loss: 127.0481\n",
            "Epoch [80/100], Loss: 126.9907\n",
            "Epoch [90/100], Loss: 126.9734\n",
            "Epoch [100/100], Loss: 126.9590\n",
            "Test Loss: 93.9737\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=16\n",
            "Epoch [10/100], Loss: 283.3513\n",
            "Epoch [20/100], Loss: 110.0465\n",
            "Epoch [30/100], Loss: 131.8032\n",
            "Epoch [40/100], Loss: 173.0755\n",
            "Epoch [50/100], Loss: 129.2543\n",
            "Epoch [60/100], Loss: 117.1862\n",
            "Epoch [70/100], Loss: 234.2696\n",
            "Epoch [80/100], Loss: 69.2529\n",
            "Epoch [90/100], Loss: 79.9204\n",
            "Epoch [100/100], Loss: 143.1749\n",
            "Test Loss: 170.9810\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=32\n",
            "Epoch [10/100], Loss: 284.2519\n",
            "Epoch [20/100], Loss: 187.2687\n",
            "Epoch [30/100], Loss: 67.0574\n",
            "Epoch [40/100], Loss: 306.8047\n",
            "Epoch [50/100], Loss: 278.2240\n",
            "Epoch [60/100], Loss: 177.9922\n",
            "Epoch [70/100], Loss: 183.8117\n",
            "Epoch [80/100], Loss: 101.0225\n",
            "Epoch [90/100], Loss: 123.7735\n",
            "Epoch [100/100], Loss: 119.6826\n",
            "Test Loss: 169.0823\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=64\n",
            "Epoch [10/100], Loss: 209.8404\n",
            "Epoch [20/100], Loss: 139.7402\n",
            "Epoch [30/100], Loss: 152.1243\n",
            "Epoch [40/100], Loss: 243.2407\n",
            "Epoch [50/100], Loss: 138.9077\n",
            "Epoch [60/100], Loss: 101.4041\n",
            "Epoch [70/100], Loss: 224.6361\n",
            "Epoch [80/100], Loss: 305.2122\n",
            "Epoch [90/100], Loss: 154.3722\n",
            "Epoch [100/100], Loss: 133.6212\n",
            "Test Loss: 181.4798\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=128\n",
            "Epoch [10/100], Loss: 179.1645\n",
            "Epoch [20/100], Loss: 166.5244\n",
            "Epoch [30/100], Loss: 134.5901\n",
            "Epoch [40/100], Loss: 165.3834\n",
            "Epoch [50/100], Loss: 162.5157\n",
            "Epoch [60/100], Loss: 172.9706\n",
            "Epoch [70/100], Loss: 236.6419\n",
            "Epoch [80/100], Loss: 164.8877\n",
            "Epoch [90/100], Loss: 186.3129\n",
            "Epoch [100/100], Loss: 157.8995\n",
            "Test Loss: 170.8925\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=256\n",
            "Epoch [10/100], Loss: 202.8847\n",
            "Epoch [20/100], Loss: 149.4550\n",
            "Epoch [30/100], Loss: 249.0291\n",
            "Epoch [40/100], Loss: 225.8038\n",
            "Epoch [50/100], Loss: 196.6986\n",
            "Epoch [60/100], Loss: 257.9431\n",
            "Epoch [70/100], Loss: 162.6190\n",
            "Epoch [80/100], Loss: 202.3173\n",
            "Epoch [90/100], Loss: 177.3616\n",
            "Epoch [100/100], Loss: 145.7643\n",
            "Test Loss: 172.0666\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=1, Batch Size=512\n",
            "Epoch [10/100], Loss: 627.8220\n",
            "Epoch [20/100], Loss: 195.7222\n",
            "Epoch [30/100], Loss: 225.1950\n",
            "Epoch [40/100], Loss: 194.4393\n",
            "Epoch [50/100], Loss: 191.3117\n",
            "Epoch [60/100], Loss: 189.2414\n",
            "Epoch [70/100], Loss: 188.5182\n",
            "Epoch [80/100], Loss: 188.2001\n",
            "Epoch [90/100], Loss: 188.1717\n",
            "Epoch [100/100], Loss: 188.0934\n",
            "Test Loss: 171.8943\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=16\n",
            "Epoch [10/100], Loss: 119.5309\n",
            "Epoch [20/100], Loss: 230.9117\n",
            "Epoch [30/100], Loss: 114.5450\n",
            "Epoch [40/100], Loss: 176.4849\n",
            "Epoch [50/100], Loss: 238.0744\n",
            "Epoch [60/100], Loss: 157.4035\n",
            "Epoch [70/100], Loss: 64.8432\n",
            "Epoch [80/100], Loss: 88.0991\n",
            "Epoch [90/100], Loss: 571.1806\n",
            "Epoch [100/100], Loss: 53.4398\n",
            "Test Loss: 73.9773\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=32\n",
            "Epoch [10/100], Loss: 334.0449\n",
            "Epoch [20/100], Loss: 165.3168\n",
            "Epoch [30/100], Loss: 300.6947\n",
            "Epoch [40/100], Loss: 134.8103\n",
            "Epoch [50/100], Loss: 110.0506\n",
            "Epoch [60/100], Loss: 169.3553\n",
            "Epoch [70/100], Loss: 135.7687\n",
            "Epoch [80/100], Loss: 160.8021\n",
            "Epoch [90/100], Loss: 202.1783\n",
            "Epoch [100/100], Loss: 174.5864\n",
            "Test Loss: 171.7458\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=64\n",
            "Epoch [10/100], Loss: 589.2406\n",
            "Epoch [20/100], Loss: 244.4284\n",
            "Epoch [30/100], Loss: 298.6484\n",
            "Epoch [40/100], Loss: 245.3712\n",
            "Epoch [50/100], Loss: 63.3853\n",
            "Epoch [60/100], Loss: 317.6272\n",
            "Epoch [70/100], Loss: 62.0440\n",
            "Epoch [80/100], Loss: 31.8407\n",
            "Epoch [90/100], Loss: 31.8386\n",
            "Epoch [100/100], Loss: 136.0345\n",
            "Test Loss: 74.1258\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=128\n",
            "Epoch [10/100], Loss: 1276.8843\n",
            "Epoch [20/100], Loss: 750.6528\n",
            "Epoch [30/100], Loss: 569.7636\n",
            "Epoch [40/100], Loss: 232.6777\n",
            "Epoch [50/100], Loss: 346.5576\n",
            "Epoch [60/100], Loss: 188.9152\n",
            "Epoch [70/100], Loss: 153.8820\n",
            "Epoch [80/100], Loss: 174.0913\n",
            "Epoch [90/100], Loss: 146.8170\n",
            "Epoch [100/100], Loss: 144.0545\n",
            "Test Loss: 169.2868\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=256\n",
            "Epoch [10/100], Loss: 1370.9946\n",
            "Epoch [20/100], Loss: 1115.6893\n",
            "Epoch [30/100], Loss: 987.6340\n",
            "Epoch [40/100], Loss: 524.2654\n",
            "Epoch [50/100], Loss: 567.3793\n",
            "Epoch [60/100], Loss: 339.8704\n",
            "Epoch [70/100], Loss: 285.4615\n",
            "Epoch [80/100], Loss: 262.3331\n",
            "Epoch [90/100], Loss: 274.9097\n",
            "Epoch [100/100], Loss: 161.3038\n",
            "Test Loss: 176.4365\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.1, Batch Size=512\n",
            "Epoch [10/100], Loss: 1527.3091\n",
            "Epoch [20/100], Loss: 1378.7053\n",
            "Epoch [30/100], Loss: 1233.5664\n",
            "Epoch [40/100], Loss: 1089.4105\n",
            "Epoch [50/100], Loss: 951.0977\n",
            "Epoch [60/100], Loss: 824.8408\n",
            "Epoch [70/100], Loss: 708.0555\n",
            "Epoch [80/100], Loss: 609.2231\n",
            "Epoch [90/100], Loss: 527.9543\n",
            "Epoch [100/100], Loss: 461.5573\n",
            "Test Loss: 372.3868\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=16\n",
            "Epoch [10/100], Loss: 1490.7379\n",
            "Epoch [20/100], Loss: 993.3893\n",
            "Epoch [30/100], Loss: 451.8683\n",
            "Epoch [40/100], Loss: 537.4343\n",
            "Epoch [50/100], Loss: 220.1236\n",
            "Epoch [60/100], Loss: 309.2061\n",
            "Epoch [70/100], Loss: 91.6946\n",
            "Epoch [80/100], Loss: 203.5758\n",
            "Epoch [90/100], Loss: 133.0885\n",
            "Epoch [100/100], Loss: 194.0249\n",
            "Test Loss: 179.1761\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=32\n",
            "Epoch [10/100], Loss: 1700.9956\n",
            "Epoch [20/100], Loss: 1873.0938\n",
            "Epoch [30/100], Loss: 1115.8613\n",
            "Epoch [40/100], Loss: 888.4602\n",
            "Epoch [50/100], Loss: 1599.1316\n",
            "Epoch [60/100], Loss: 280.0699\n",
            "Epoch [70/100], Loss: 657.4592\n",
            "Epoch [80/100], Loss: 401.1706\n",
            "Epoch [90/100], Loss: 1188.3306\n",
            "Epoch [100/100], Loss: 276.4846\n",
            "Test Loss: 361.3963\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=64\n",
            "Epoch [10/100], Loss: 1512.8915\n",
            "Epoch [20/100], Loss: 1457.2501\n",
            "Epoch [30/100], Loss: 899.1291\n",
            "Epoch [40/100], Loss: 751.0642\n",
            "Epoch [50/100], Loss: 937.6503\n",
            "Epoch [60/100], Loss: 646.3493\n",
            "Epoch [70/100], Loss: 687.3372\n",
            "Epoch [80/100], Loss: 900.0746\n",
            "Epoch [90/100], Loss: 784.1230\n",
            "Epoch [100/100], Loss: 880.0107\n",
            "Test Loss: 644.0890\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=128\n",
            "Epoch [10/100], Loss: 1535.5343\n",
            "Epoch [20/100], Loss: 1501.0685\n",
            "Epoch [30/100], Loss: 1713.3385\n",
            "Epoch [40/100], Loss: 1537.4042\n",
            "Epoch [50/100], Loss: 1534.7295\n",
            "Epoch [60/100], Loss: 1500.1650\n",
            "Epoch [70/100], Loss: 1240.5764\n",
            "Epoch [80/100], Loss: 1310.4563\n",
            "Epoch [90/100], Loss: 1160.4432\n",
            "Epoch [100/100], Loss: 1148.7406\n",
            "Test Loss: 954.3332\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=256\n",
            "Epoch [10/100], Loss: 1575.5081\n",
            "Epoch [20/100], Loss: 1556.1239\n",
            "Epoch [30/100], Loss: 1532.7064\n",
            "Epoch [40/100], Loss: 1600.3600\n",
            "Epoch [50/100], Loss: 1440.2104\n",
            "Epoch [60/100], Loss: 1392.0460\n",
            "Epoch [70/100], Loss: 1412.7900\n",
            "Epoch [80/100], Loss: 1662.9694\n",
            "Epoch [90/100], Loss: 1333.5927\n",
            "Epoch [100/100], Loss: 1219.3800\n",
            "Test Loss: 1143.1570\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.01, Batch Size=512\n",
            "Epoch [10/100], Loss: 1651.9794\n",
            "Epoch [20/100], Loss: 1635.7506\n",
            "Epoch [30/100], Loss: 1619.5228\n",
            "Epoch [40/100], Loss: 1603.1544\n",
            "Epoch [50/100], Loss: 1586.5345\n",
            "Epoch [60/100], Loss: 1569.6357\n",
            "Epoch [70/100], Loss: 1552.5181\n",
            "Epoch [80/100], Loss: 1535.2922\n",
            "Epoch [90/100], Loss: 1518.0597\n",
            "Epoch [100/100], Loss: 1500.8514\n",
            "Test Loss: 1334.4272\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=16\n",
            "Epoch [10/100], Loss: 1608.2046\n",
            "Epoch [20/100], Loss: 1749.7250\n",
            "Epoch [30/100], Loss: 1548.3403\n",
            "Epoch [40/100], Loss: 2023.2141\n",
            "Epoch [50/100], Loss: 1693.0840\n",
            "Epoch [60/100], Loss: 1514.5726\n",
            "Epoch [70/100], Loss: 1373.5172\n",
            "Epoch [80/100], Loss: 1622.6637\n",
            "Epoch [90/100], Loss: 1344.1348\n",
            "Epoch [100/100], Loss: 1166.6844\n",
            "Test Loss: 1117.1018\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=32\n",
            "Epoch [10/100], Loss: 1217.7781\n",
            "Epoch [20/100], Loss: 1575.8132\n",
            "Epoch [30/100], Loss: 2177.4624\n",
            "Epoch [40/100], Loss: 1760.5874\n",
            "Epoch [50/100], Loss: 1616.6023\n",
            "Epoch [60/100], Loss: 1200.6479\n",
            "Epoch [70/100], Loss: 1514.0891\n",
            "Epoch [80/100], Loss: 1582.0179\n",
            "Epoch [90/100], Loss: 1382.1428\n",
            "Epoch [100/100], Loss: 3049.3391\n",
            "Test Loss: 1309.4126\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=64\n",
            "Epoch [10/100], Loss: 1303.3348\n",
            "Epoch [20/100], Loss: 3223.5374\n",
            "Epoch [30/100], Loss: 1415.8457\n",
            "Epoch [40/100], Loss: 1649.6893\n",
            "Epoch [50/100], Loss: 1954.6294\n",
            "Epoch [60/100], Loss: 1605.8254\n",
            "Epoch [70/100], Loss: 1520.6630\n",
            "Epoch [80/100], Loss: 1503.8380\n",
            "Epoch [90/100], Loss: 1819.2928\n",
            "Epoch [100/100], Loss: 1900.5171\n",
            "Test Loss: 1393.9681\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=128\n",
            "Epoch [10/100], Loss: 1618.5680\n",
            "Epoch [20/100], Loss: 1622.7500\n",
            "Epoch [30/100], Loss: 1377.8280\n",
            "Epoch [40/100], Loss: 1807.7131\n",
            "Epoch [50/100], Loss: 1892.9877\n",
            "Epoch [60/100], Loss: 1481.8540\n",
            "Epoch [70/100], Loss: 1487.9911\n",
            "Epoch [80/100], Loss: 1579.3699\n",
            "Epoch [90/100], Loss: 1468.2870\n",
            "Epoch [100/100], Loss: 1544.4806\n",
            "Test Loss: 1434.8889\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=256\n",
            "Epoch [10/100], Loss: 1507.8285\n",
            "Epoch [20/100], Loss: 1610.4528\n",
            "Epoch [30/100], Loss: 1633.0447\n",
            "Epoch [40/100], Loss: 1538.1344\n",
            "Epoch [50/100], Loss: 1760.4410\n",
            "Epoch [60/100], Loss: 1473.6908\n",
            "Epoch [70/100], Loss: 1748.7250\n",
            "Epoch [80/100], Loss: 1609.2201\n",
            "Epoch [90/100], Loss: 1378.8650\n",
            "Epoch [100/100], Loss: 1767.0648\n",
            "Test Loss: 1463.0402\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.001, Batch Size=512\n",
            "Epoch [10/100], Loss: 1656.7802\n",
            "Epoch [20/100], Loss: 1655.1569\n",
            "Epoch [30/100], Loss: 1653.5342\n",
            "Epoch [40/100], Loss: 1651.9122\n",
            "Epoch [50/100], Loss: 1650.2904\n",
            "Epoch [60/100], Loss: 1648.6686\n",
            "Epoch [70/100], Loss: 1647.0469\n",
            "Epoch [80/100], Loss: 1645.4241\n",
            "Epoch [90/100], Loss: 1643.8004\n",
            "Epoch [100/100], Loss: 1642.1758\n",
            "Test Loss: 1469.4221\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=16\n",
            "Epoch [10/100], Loss: 1678.9418\n",
            "Epoch [20/100], Loss: 1347.3683\n",
            "Epoch [30/100], Loss: 1657.0758\n",
            "Epoch [40/100], Loss: 1826.4890\n",
            "Epoch [50/100], Loss: 1765.9786\n",
            "Epoch [60/100], Loss: 1649.1132\n",
            "Epoch [70/100], Loss: 1326.2085\n",
            "Epoch [80/100], Loss: 1525.2343\n",
            "Epoch [90/100], Loss: 1556.6410\n",
            "Epoch [100/100], Loss: 3126.2986\n",
            "Test Loss: 1464.1438\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=32\n",
            "Epoch [10/100], Loss: 1649.9473\n",
            "Epoch [20/100], Loss: 1691.5579\n",
            "Epoch [30/100], Loss: 1507.4872\n",
            "Epoch [40/100], Loss: 1416.1980\n",
            "Epoch [50/100], Loss: 1364.9886\n",
            "Epoch [60/100], Loss: 1283.2324\n",
            "Epoch [70/100], Loss: 1834.5195\n",
            "Epoch [80/100], Loss: 1484.8210\n",
            "Epoch [90/100], Loss: 1598.3055\n",
            "Epoch [100/100], Loss: 2880.4089\n",
            "Test Loss: 1479.2490\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=64\n",
            "Epoch [10/100], Loss: 2040.4368\n",
            "Epoch [20/100], Loss: 1761.5438\n",
            "Epoch [30/100], Loss: 2051.2666\n",
            "Epoch [40/100], Loss: 1742.4435\n",
            "Epoch [50/100], Loss: 1509.7972\n",
            "Epoch [60/100], Loss: 1727.8774\n",
            "Epoch [70/100], Loss: 1817.0731\n",
            "Epoch [80/100], Loss: 1294.3208\n",
            "Epoch [90/100], Loss: 1699.7864\n",
            "Epoch [100/100], Loss: 1582.7789\n",
            "Test Loss: 1485.2043\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=128\n",
            "Epoch [10/100], Loss: 1898.2850\n",
            "Epoch [20/100], Loss: 1763.4449\n",
            "Epoch [30/100], Loss: 1602.1711\n",
            "Epoch [40/100], Loss: 1487.5336\n",
            "Epoch [50/100], Loss: 1456.0543\n",
            "Epoch [60/100], Loss: 1640.7720\n",
            "Epoch [70/100], Loss: 1585.9908\n",
            "Epoch [80/100], Loss: 1704.2129\n",
            "Epoch [90/100], Loss: 1787.7869\n",
            "Epoch [100/100], Loss: 1739.0430\n",
            "Test Loss: 1475.0924\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=256\n",
            "Epoch [10/100], Loss: 1774.9019\n",
            "Epoch [20/100], Loss: 1804.7675\n",
            "Epoch [30/100], Loss: 1811.0363\n",
            "Epoch [40/100], Loss: 1713.5712\n",
            "Epoch [50/100], Loss: 1775.3279\n",
            "Epoch [60/100], Loss: 1567.8978\n",
            "Epoch [70/100], Loss: 1544.3046\n",
            "Epoch [80/100], Loss: 1639.0681\n",
            "Epoch [90/100], Loss: 1355.7852\n",
            "Epoch [100/100], Loss: 1509.3485\n",
            "Test Loss: 1480.0171\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=100, LR=0.0001, Batch Size=512\n",
            "Epoch [10/100], Loss: 1667.7498\n",
            "Epoch [20/100], Loss: 1667.5853\n",
            "Epoch [30/100], Loss: 1667.4213\n",
            "Epoch [40/100], Loss: 1667.2573\n",
            "Epoch [50/100], Loss: 1667.0931\n",
            "Epoch [60/100], Loss: 1666.9290\n",
            "Epoch [70/100], Loss: 1666.7649\n",
            "Epoch [80/100], Loss: 1666.6006\n",
            "Epoch [90/100], Loss: 1666.4368\n",
            "Epoch [100/100], Loss: 1666.2728\n",
            "Test Loss: 1492.3660\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=16\n",
            "Epoch [10/250], Loss: 226.4823\n",
            "Epoch [20/250], Loss: 249.8980\n",
            "Epoch [30/250], Loss: 320.2426\n",
            "Epoch [40/250], Loss: 242.8874\n",
            "Epoch [50/250], Loss: 251.8924\n",
            "Epoch [60/250], Loss: 298.4267\n",
            "Epoch [70/250], Loss: 152.5019\n",
            "Epoch [80/250], Loss: 298.0754\n",
            "Epoch [90/250], Loss: 56.6143\n",
            "Epoch [100/250], Loss: 184.8458\n",
            "Epoch [110/250], Loss: 153.8368\n",
            "Epoch [120/250], Loss: 274.0598\n",
            "Epoch [130/250], Loss: 129.6015\n",
            "Epoch [140/250], Loss: 280.2069\n",
            "Epoch [150/250], Loss: 116.0411\n",
            "Epoch [160/250], Loss: 161.1931\n",
            "Epoch [170/250], Loss: 128.3698\n",
            "Epoch [180/250], Loss: 125.2732\n",
            "Epoch [190/250], Loss: 105.6210\n",
            "Epoch [200/250], Loss: 121.9435\n",
            "Epoch [210/250], Loss: 174.5212\n",
            "Epoch [220/250], Loss: 93.3992\n",
            "Epoch [230/250], Loss: 153.1191\n",
            "Epoch [240/250], Loss: 127.3696\n",
            "Epoch [250/250], Loss: 182.6915\n",
            "Test Loss: 170.5000\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=32\n",
            "Epoch [10/250], Loss: 149.1340\n",
            "Epoch [20/250], Loss: 63.5786\n",
            "Epoch [30/250], Loss: 100.8382\n",
            "Epoch [40/250], Loss: 167.8112\n",
            "Epoch [50/250], Loss: 593.8688\n",
            "Epoch [60/250], Loss: 233.4879\n",
            "Epoch [70/250], Loss: 113.4602\n",
            "Epoch [80/250], Loss: 66.5593\n",
            "Epoch [90/250], Loss: 234.4228\n",
            "Epoch [100/250], Loss: 132.0907\n",
            "Epoch [110/250], Loss: 119.8317\n",
            "Epoch [120/250], Loss: 253.6444\n",
            "Epoch [130/250], Loss: 242.4236\n",
            "Epoch [140/250], Loss: 224.4720\n",
            "Epoch [150/250], Loss: 118.7445\n",
            "Epoch [160/250], Loss: 179.7155\n",
            "Epoch [170/250], Loss: 258.8076\n",
            "Epoch [180/250], Loss: 822.2156\n",
            "Epoch [190/250], Loss: 193.4886\n",
            "Epoch [200/250], Loss: 156.4760\n",
            "Epoch [210/250], Loss: 101.8708\n",
            "Epoch [220/250], Loss: 191.0781\n",
            "Epoch [230/250], Loss: 214.1248\n",
            "Epoch [240/250], Loss: 273.3048\n",
            "Epoch [250/250], Loss: 149.8135\n",
            "Test Loss: 218.4483\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=64\n",
            "Epoch [10/250], Loss: 126.7435\n",
            "Epoch [20/250], Loss: 245.1238\n",
            "Epoch [30/250], Loss: 177.3841\n",
            "Epoch [40/250], Loss: 112.7936\n",
            "Epoch [50/250], Loss: 96.7086\n",
            "Epoch [60/250], Loss: 217.5309\n",
            "Epoch [70/250], Loss: 185.4149\n",
            "Epoch [80/250], Loss: 77.6564\n",
            "Epoch [90/250], Loss: 156.0335\n",
            "Epoch [100/250], Loss: 226.1603\n",
            "Epoch [110/250], Loss: 144.4258\n",
            "Epoch [120/250], Loss: 131.6450\n",
            "Epoch [130/250], Loss: 175.3192\n",
            "Epoch [140/250], Loss: 187.5317\n",
            "Epoch [150/250], Loss: 109.4168\n",
            "Epoch [160/250], Loss: 252.3290\n",
            "Epoch [170/250], Loss: 467.7513\n",
            "Epoch [180/250], Loss: 157.0306\n",
            "Epoch [190/250], Loss: 152.7241\n",
            "Epoch [200/250], Loss: 283.3737\n",
            "Epoch [210/250], Loss: 147.6241\n",
            "Epoch [220/250], Loss: 92.0756\n",
            "Epoch [230/250], Loss: 132.6245\n",
            "Epoch [240/250], Loss: 142.3311\n",
            "Epoch [250/250], Loss: 219.3465\n",
            "Test Loss: 168.0011\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=128\n",
            "Epoch [10/250], Loss: 170.4841\n",
            "Epoch [20/250], Loss: 175.2781\n",
            "Epoch [30/250], Loss: 270.0258\n",
            "Epoch [40/250], Loss: 84.7482\n",
            "Epoch [50/250], Loss: 189.2593\n",
            "Epoch [60/250], Loss: 185.6345\n",
            "Epoch [70/250], Loss: 267.8236\n",
            "Epoch [80/250], Loss: 144.4028\n",
            "Epoch [90/250], Loss: 236.9838\n",
            "Epoch [100/250], Loss: 247.7558\n",
            "Epoch [110/250], Loss: 211.5941\n",
            "Epoch [120/250], Loss: 179.6562\n",
            "Epoch [130/250], Loss: 259.3216\n",
            "Epoch [140/250], Loss: 140.6818\n",
            "Epoch [150/250], Loss: 272.7418\n",
            "Epoch [160/250], Loss: 173.1831\n",
            "Epoch [170/250], Loss: 160.4525\n",
            "Epoch [180/250], Loss: 232.6153\n",
            "Epoch [190/250], Loss: 171.9359\n",
            "Epoch [200/250], Loss: 148.0883\n",
            "Epoch [210/250], Loss: 175.6103\n",
            "Epoch [220/250], Loss: 231.8963\n",
            "Epoch [230/250], Loss: 286.7064\n",
            "Epoch [240/250], Loss: 161.0863\n",
            "Epoch [250/250], Loss: 169.3252\n",
            "Test Loss: 170.8956\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=256\n",
            "Epoch [10/250], Loss: 149.1999\n",
            "Epoch [20/250], Loss: 258.5836\n",
            "Epoch [30/250], Loss: 185.6162\n",
            "Epoch [40/250], Loss: 162.6719\n",
            "Epoch [50/250], Loss: 162.7660\n",
            "Epoch [60/250], Loss: 274.7904\n",
            "Epoch [70/250], Loss: 201.7864\n",
            "Epoch [80/250], Loss: 206.0157\n",
            "Epoch [90/250], Loss: 192.9283\n",
            "Epoch [100/250], Loss: 292.6521\n",
            "Epoch [110/250], Loss: 227.3397\n",
            "Epoch [120/250], Loss: 277.5988\n",
            "Epoch [130/250], Loss: 215.8465\n",
            "Epoch [140/250], Loss: 175.7321\n",
            "Epoch [150/250], Loss: 223.1319\n",
            "Epoch [160/250], Loss: 246.9392\n",
            "Epoch [170/250], Loss: 179.0735\n",
            "Epoch [180/250], Loss: 165.9156\n",
            "Epoch [190/250], Loss: 151.1202\n",
            "Epoch [200/250], Loss: 155.1956\n",
            "Epoch [210/250], Loss: 149.3832\n",
            "Epoch [220/250], Loss: 278.9477\n",
            "Epoch [230/250], Loss: 312.8066\n",
            "Epoch [240/250], Loss: 155.5156\n",
            "Epoch [250/250], Loss: 258.8085\n",
            "Test Loss: 175.3577\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=10, Batch Size=512\n",
            "Epoch [10/250], Loss: 246.7888\n",
            "Epoch [20/250], Loss: 189.9714\n",
            "Epoch [30/250], Loss: 188.5989\n",
            "Epoch [40/250], Loss: 189.7910\n",
            "Epoch [50/250], Loss: 188.1925\n",
            "Epoch [60/250], Loss: 188.3252\n",
            "Epoch [70/250], Loss: 188.1029\n",
            "Epoch [80/250], Loss: 188.0929\n",
            "Epoch [90/250], Loss: 188.0945\n",
            "Epoch [100/250], Loss: 188.0922\n",
            "Epoch [110/250], Loss: 188.0908\n",
            "Epoch [120/250], Loss: 188.0905\n",
            "Epoch [130/250], Loss: 188.0906\n",
            "Epoch [140/250], Loss: 188.0906\n",
            "Epoch [150/250], Loss: 188.0905\n",
            "Epoch [160/250], Loss: 188.0905\n",
            "Epoch [170/250], Loss: 188.0905\n",
            "Epoch [180/250], Loss: 188.0905\n",
            "Epoch [190/250], Loss: 188.0905\n",
            "Epoch [200/250], Loss: 188.0905\n",
            "Epoch [210/250], Loss: 188.0905\n",
            "Epoch [220/250], Loss: 188.0905\n",
            "Epoch [230/250], Loss: 188.0905\n",
            "Epoch [240/250], Loss: 188.0905\n",
            "Epoch [250/250], Loss: 188.0905\n",
            "Test Loss: 171.9697\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=16\n",
            "Epoch [10/250], Loss: 352.0765\n",
            "Epoch [20/250], Loss: 104.2787\n",
            "Epoch [30/250], Loss: 135.4332\n",
            "Epoch [40/250], Loss: 56.6155\n",
            "Epoch [50/250], Loss: 182.8088\n",
            "Epoch [60/250], Loss: 110.2898\n",
            "Epoch [70/250], Loss: 304.7582\n",
            "Epoch [80/250], Loss: 252.3926\n",
            "Epoch [90/250], Loss: 193.3683\n",
            "Epoch [100/250], Loss: 705.0118\n",
            "Epoch [110/250], Loss: 104.2630\n",
            "Epoch [120/250], Loss: 183.6797\n",
            "Epoch [130/250], Loss: 71.8170\n",
            "Epoch [140/250], Loss: 227.6206\n",
            "Epoch [150/250], Loss: 230.8858\n",
            "Epoch [160/250], Loss: 144.5672\n",
            "Epoch [170/250], Loss: 107.4260\n",
            "Epoch [180/250], Loss: 150.2338\n",
            "Epoch [190/250], Loss: 137.1404\n",
            "Epoch [200/250], Loss: 198.5640\n",
            "Epoch [210/250], Loss: 115.2197\n",
            "Epoch [220/250], Loss: 151.8829\n",
            "Epoch [230/250], Loss: 349.2935\n",
            "Epoch [240/250], Loss: 148.7637\n",
            "Epoch [250/250], Loss: 271.2626\n",
            "Test Loss: 171.2019\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=32\n",
            "Epoch [10/250], Loss: 128.5210\n",
            "Epoch [20/250], Loss: 261.3781\n",
            "Epoch [30/250], Loss: 224.7843\n",
            "Epoch [40/250], Loss: 113.2877\n",
            "Epoch [50/250], Loss: 264.8977\n",
            "Epoch [60/250], Loss: 201.1407\n",
            "Epoch [70/250], Loss: 268.5692\n",
            "Epoch [80/250], Loss: 359.8370\n",
            "Epoch [90/250], Loss: 176.4210\n",
            "Epoch [100/250], Loss: 201.5641\n",
            "Epoch [110/250], Loss: 107.4942\n",
            "Epoch [120/250], Loss: 165.4819\n",
            "Epoch [130/250], Loss: 183.4930\n",
            "Epoch [140/250], Loss: 108.6337\n",
            "Epoch [150/250], Loss: 149.2614\n",
            "Epoch [160/250], Loss: 328.8401\n",
            "Epoch [170/250], Loss: 174.9112\n",
            "Epoch [180/250], Loss: 139.7782\n",
            "Epoch [190/250], Loss: 133.9934\n",
            "Epoch [200/250], Loss: 160.7306\n",
            "Epoch [210/250], Loss: 196.7556\n",
            "Epoch [220/250], Loss: 127.3842\n",
            "Epoch [230/250], Loss: 149.1303\n",
            "Epoch [240/250], Loss: 250.8185\n",
            "Epoch [250/250], Loss: 93.0765\n",
            "Test Loss: 172.8017\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=64\n",
            "Epoch [10/250], Loss: 166.6432\n",
            "Epoch [20/250], Loss: 154.9730\n",
            "Epoch [30/250], Loss: 151.1914\n",
            "Epoch [40/250], Loss: 152.7988\n",
            "Epoch [50/250], Loss: 194.7506\n",
            "Epoch [60/250], Loss: 210.2846\n",
            "Epoch [70/250], Loss: 116.8047\n",
            "Epoch [80/250], Loss: 349.0200\n",
            "Epoch [90/250], Loss: 223.7564\n",
            "Epoch [100/250], Loss: 165.4807\n",
            "Epoch [110/250], Loss: 397.7232\n",
            "Epoch [120/250], Loss: 126.4033\n",
            "Epoch [130/250], Loss: 109.9782\n",
            "Epoch [140/250], Loss: 119.5552\n",
            "Epoch [150/250], Loss: 170.8687\n",
            "Epoch [160/250], Loss: 719.2756\n",
            "Epoch [170/250], Loss: 116.1416\n",
            "Epoch [180/250], Loss: 236.7459\n",
            "Epoch [190/250], Loss: 202.2963\n",
            "Epoch [200/250], Loss: 176.1396\n",
            "Epoch [210/250], Loss: 233.9478\n",
            "Epoch [220/250], Loss: 173.6913\n",
            "Epoch [230/250], Loss: 201.8929\n",
            "Epoch [240/250], Loss: 50.3511\n",
            "Epoch [250/250], Loss: 275.6465\n",
            "Test Loss: 171.1863\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=128\n",
            "Epoch [10/250], Loss: 177.2924\n",
            "Epoch [20/250], Loss: 200.8255\n",
            "Epoch [30/250], Loss: 135.3143\n",
            "Epoch [40/250], Loss: 155.5137\n",
            "Epoch [50/250], Loss: 158.6071\n",
            "Epoch [60/250], Loss: 170.9697\n",
            "Epoch [70/250], Loss: 195.5967\n",
            "Epoch [80/250], Loss: 214.5441\n",
            "Epoch [90/250], Loss: 270.9294\n",
            "Epoch [100/250], Loss: 140.7669\n",
            "Epoch [110/250], Loss: 198.3537\n",
            "Epoch [120/250], Loss: 255.0869\n",
            "Epoch [130/250], Loss: 206.1340\n",
            "Epoch [140/250], Loss: 205.1664\n",
            "Epoch [150/250], Loss: 217.0818\n",
            "Epoch [160/250], Loss: 214.0322\n",
            "Epoch [170/250], Loss: 246.3399\n",
            "Epoch [180/250], Loss: 176.6348\n",
            "Epoch [190/250], Loss: 160.2221\n",
            "Epoch [200/250], Loss: 151.9200\n",
            "Epoch [210/250], Loss: 170.8532\n",
            "Epoch [220/250], Loss: 203.0966\n",
            "Epoch [230/250], Loss: 176.6800\n",
            "Epoch [240/250], Loss: 232.3852\n",
            "Epoch [250/250], Loss: 222.0979\n",
            "Test Loss: 171.8893\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=256\n",
            "Epoch [10/250], Loss: 174.3990\n",
            "Epoch [20/250], Loss: 181.7809\n",
            "Epoch [30/250], Loss: 143.6360\n",
            "Epoch [40/250], Loss: 160.5580\n",
            "Epoch [50/250], Loss: 247.2424\n",
            "Epoch [60/250], Loss: 225.5059\n",
            "Epoch [70/250], Loss: 181.9592\n",
            "Epoch [80/250], Loss: 285.2671\n",
            "Epoch [90/250], Loss: 144.8342\n",
            "Epoch [100/250], Loss: 198.2210\n",
            "Epoch [110/250], Loss: 146.5287\n",
            "Epoch [120/250], Loss: 154.2449\n",
            "Epoch [130/250], Loss: 173.8414\n",
            "Epoch [140/250], Loss: 193.6491\n",
            "Epoch [150/250], Loss: 140.4563\n",
            "Epoch [160/250], Loss: 164.0763\n",
            "Epoch [170/250], Loss: 114.1600\n",
            "Epoch [180/250], Loss: 148.8982\n",
            "Epoch [190/250], Loss: 179.3540\n",
            "Epoch [200/250], Loss: 156.9439\n",
            "Epoch [210/250], Loss: 194.5150\n",
            "Epoch [220/250], Loss: 184.8171\n",
            "Epoch [230/250], Loss: 177.2043\n",
            "Epoch [240/250], Loss: 200.0187\n",
            "Epoch [250/250], Loss: 186.5656\n",
            "Test Loss: 169.7382\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=1, Batch Size=512\n",
            "Epoch [10/250], Loss: 621.7571\n",
            "Epoch [20/250], Loss: 193.8900\n",
            "Epoch [30/250], Loss: 225.9363\n",
            "Epoch [40/250], Loss: 192.0755\n",
            "Epoch [50/250], Loss: 192.1883\n",
            "Epoch [60/250], Loss: 188.6760\n",
            "Epoch [70/250], Loss: 188.7055\n",
            "Epoch [80/250], Loss: 188.1146\n",
            "Epoch [90/250], Loss: 188.1889\n",
            "Epoch [100/250], Loss: 188.0919\n",
            "Epoch [110/250], Loss: 188.0999\n",
            "Epoch [120/250], Loss: 188.0933\n",
            "Epoch [130/250], Loss: 188.0906\n",
            "Epoch [140/250], Loss: 188.0910\n",
            "Epoch [150/250], Loss: 188.0906\n",
            "Epoch [160/250], Loss: 188.0905\n",
            "Epoch [170/250], Loss: 188.0905\n",
            "Epoch [180/250], Loss: 188.0905\n",
            "Epoch [190/250], Loss: 188.0905\n",
            "Epoch [200/250], Loss: 188.0905\n",
            "Epoch [210/250], Loss: 188.0905\n",
            "Epoch [220/250], Loss: 188.0905\n",
            "Epoch [230/250], Loss: 188.0905\n",
            "Epoch [240/250], Loss: 188.0905\n",
            "Epoch [250/250], Loss: 188.0905\n",
            "Test Loss: 171.9698\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=16\n",
            "Epoch [10/250], Loss: 269.0085\n",
            "Epoch [20/250], Loss: 107.4709\n",
            "Epoch [30/250], Loss: 71.7727\n",
            "Epoch [40/250], Loss: 12.5350\n",
            "Epoch [50/250], Loss: 172.4869\n",
            "Epoch [60/250], Loss: 119.5508\n",
            "Epoch [70/250], Loss: 38.3352\n",
            "Epoch [80/250], Loss: 96.2696\n",
            "Epoch [90/250], Loss: 36.4185\n",
            "Epoch [100/250], Loss: 66.2823\n",
            "Epoch [110/250], Loss: 61.6122\n",
            "Epoch [120/250], Loss: 152.6858\n",
            "Epoch [130/250], Loss: 53.4997\n",
            "Epoch [140/250], Loss: 106.8484\n",
            "Epoch [150/250], Loss: 30.8163\n",
            "Epoch [160/250], Loss: 209.2675\n",
            "Epoch [170/250], Loss: 70.5526\n",
            "Epoch [180/250], Loss: 188.4770\n",
            "Epoch [190/250], Loss: 32.2471\n",
            "Epoch [200/250], Loss: 109.5177\n",
            "Epoch [210/250], Loss: 78.9545\n",
            "Epoch [220/250], Loss: 58.3585\n",
            "Epoch [230/250], Loss: 77.4130\n",
            "Epoch [240/250], Loss: 75.8650\n",
            "Epoch [250/250], Loss: 38.1608\n",
            "Test Loss: 79.9436\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=32\n",
            "Epoch [10/250], Loss: 417.9377\n",
            "Epoch [20/250], Loss: 124.8384\n",
            "Epoch [30/250], Loss: 343.2221\n",
            "Epoch [40/250], Loss: 136.8116\n",
            "Epoch [50/250], Loss: 155.6321\n",
            "Epoch [60/250], Loss: 196.0299\n",
            "Epoch [70/250], Loss: 108.4353\n",
            "Epoch [80/250], Loss: 286.5659\n",
            "Epoch [90/250], Loss: 283.6632\n",
            "Epoch [100/250], Loss: 137.6335\n",
            "Epoch [110/250], Loss: 122.5963\n",
            "Epoch [120/250], Loss: 74.4430\n",
            "Epoch [130/250], Loss: 101.8075\n",
            "Epoch [140/250], Loss: 94.5857\n",
            "Epoch [150/250], Loss: 183.2685\n",
            "Epoch [160/250], Loss: 93.3847\n",
            "Epoch [170/250], Loss: 264.2976\n",
            "Epoch [180/250], Loss: 146.3669\n",
            "Epoch [190/250], Loss: 221.6489\n",
            "Epoch [200/250], Loss: 642.2764\n",
            "Epoch [210/250], Loss: 124.1152\n",
            "Epoch [220/250], Loss: 134.5120\n",
            "Epoch [230/250], Loss: 209.4282\n",
            "Epoch [240/250], Loss: 159.1984\n",
            "Epoch [250/250], Loss: 729.4864\n",
            "Test Loss: 172.3897\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=64\n",
            "Epoch [10/250], Loss: 965.8013\n",
            "Epoch [20/250], Loss: 382.2080\n",
            "Epoch [30/250], Loss: 173.8517\n",
            "Epoch [40/250], Loss: 83.2528\n",
            "Epoch [50/250], Loss: 98.6812\n",
            "Epoch [60/250], Loss: 241.3566\n",
            "Epoch [70/250], Loss: 139.0045\n",
            "Epoch [80/250], Loss: 84.6471\n",
            "Epoch [90/250], Loss: 56.1172\n",
            "Epoch [100/250], Loss: 154.8507\n",
            "Epoch [110/250], Loss: 159.5926\n",
            "Epoch [120/250], Loss: 100.7445\n",
            "Epoch [130/250], Loss: 40.7228\n",
            "Epoch [140/250], Loss: 65.3665\n",
            "Epoch [150/250], Loss: 138.4084\n",
            "Epoch [160/250], Loss: 64.0211\n",
            "Epoch [170/250], Loss: 167.9973\n",
            "Epoch [180/250], Loss: 78.8918\n",
            "Epoch [190/250], Loss: 53.2155\n",
            "Epoch [200/250], Loss: 253.8728\n",
            "Epoch [210/250], Loss: 58.5515\n",
            "Epoch [220/250], Loss: 114.3333\n",
            "Epoch [230/250], Loss: 72.8941\n",
            "Epoch [240/250], Loss: 113.8929\n",
            "Epoch [250/250], Loss: 114.3409\n",
            "Test Loss: 128.8248\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=128\n",
            "Epoch [10/250], Loss: 1260.7040\n",
            "Epoch [20/250], Loss: 958.1274\n",
            "Epoch [30/250], Loss: 551.1248\n",
            "Epoch [40/250], Loss: 364.4013\n",
            "Epoch [50/250], Loss: 227.1809\n",
            "Epoch [60/250], Loss: 229.5324\n",
            "Epoch [70/250], Loss: 167.4329\n",
            "Epoch [80/250], Loss: 162.6569\n",
            "Epoch [90/250], Loss: 161.6994\n",
            "Epoch [100/250], Loss: 180.4432\n",
            "Epoch [110/250], Loss: 178.2530\n",
            "Epoch [120/250], Loss: 203.0511\n",
            "Epoch [130/250], Loss: 170.4098\n",
            "Epoch [140/250], Loss: 170.1176\n",
            "Epoch [150/250], Loss: 161.5606\n",
            "Epoch [160/250], Loss: 243.0756\n",
            "Epoch [170/250], Loss: 166.7055\n",
            "Epoch [180/250], Loss: 89.4087\n",
            "Epoch [190/250], Loss: 74.1904\n",
            "Epoch [200/250], Loss: 64.3675\n",
            "Epoch [210/250], Loss: 63.1980\n",
            "Epoch [220/250], Loss: 57.6758\n",
            "Epoch [230/250], Loss: 72.3836\n",
            "Epoch [240/250], Loss: 68.3787\n",
            "Epoch [250/250], Loss: 66.4456\n",
            "Test Loss: 86.8789\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=256\n",
            "Epoch [10/250], Loss: 1522.9479\n",
            "Epoch [20/250], Loss: 1227.8361\n",
            "Epoch [30/250], Loss: 890.6884\n",
            "Epoch [40/250], Loss: 618.2798\n",
            "Epoch [50/250], Loss: 496.7750\n",
            "Epoch [60/250], Loss: 473.7137\n",
            "Epoch [70/250], Loss: 266.2261\n",
            "Epoch [80/250], Loss: 265.5699\n",
            "Epoch [90/250], Loss: 245.1330\n",
            "Epoch [100/250], Loss: 225.9647\n",
            "Epoch [110/250], Loss: 305.4063\n",
            "Epoch [120/250], Loss: 269.1312\n",
            "Epoch [130/250], Loss: 163.6489\n",
            "Epoch [140/250], Loss: 255.2186\n",
            "Epoch [150/250], Loss: 155.1810\n",
            "Epoch [160/250], Loss: 279.1112\n",
            "Epoch [170/250], Loss: 166.3109\n",
            "Epoch [180/250], Loss: 175.5751\n",
            "Epoch [190/250], Loss: 264.8078\n",
            "Epoch [200/250], Loss: 180.8436\n",
            "Epoch [210/250], Loss: 132.5644\n",
            "Epoch [220/250], Loss: 252.9164\n",
            "Epoch [230/250], Loss: 179.2107\n",
            "Epoch [240/250], Loss: 172.6699\n",
            "Epoch [250/250], Loss: 156.2105\n",
            "Test Loss: 171.7737\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.1, Batch Size=512\n",
            "Epoch [10/250], Loss: 1519.0405\n",
            "Epoch [20/250], Loss: 1370.7161\n",
            "Epoch [30/250], Loss: 1224.8838\n",
            "Epoch [40/250], Loss: 1082.7413\n",
            "Epoch [50/250], Loss: 943.2899\n",
            "Epoch [60/250], Loss: 811.7685\n",
            "Epoch [70/250], Loss: 698.5754\n",
            "Epoch [80/250], Loss: 604.5300\n",
            "Epoch [90/250], Loss: 527.0144\n",
            "Epoch [100/250], Loss: 463.1222\n",
            "Epoch [110/250], Loss: 410.4180\n",
            "Epoch [120/250], Loss: 366.9659\n",
            "Epoch [130/250], Loss: 331.2210\n",
            "Epoch [140/250], Loss: 301.9253\n",
            "Epoch [150/250], Loss: 278.0336\n",
            "Epoch [160/250], Loss: 258.6638\n",
            "Epoch [170/250], Loss: 243.0642\n",
            "Epoch [180/250], Loss: 230.5914\n",
            "Epoch [190/250], Loss: 220.6949\n",
            "Epoch [200/250], Loss: 212.9056\n",
            "Epoch [210/250], Loss: 206.8254\n",
            "Epoch [220/250], Loss: 202.1198\n",
            "Epoch [230/250], Loss: 198.5094\n",
            "Epoch [240/250], Loss: 195.7637\n",
            "Epoch [250/250], Loss: 193.6942\n",
            "Test Loss: 167.8369\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=16\n",
            "Epoch [10/250], Loss: 1199.3579\n",
            "Epoch [20/250], Loss: 536.3017\n",
            "Epoch [30/250], Loss: 754.6905\n",
            "Epoch [40/250], Loss: 787.6440\n",
            "Epoch [50/250], Loss: 249.6907\n",
            "Epoch [60/250], Loss: 784.7061\n",
            "Epoch [70/250], Loss: 438.2249\n",
            "Epoch [80/250], Loss: 222.3692\n",
            "Epoch [90/250], Loss: 292.7313\n",
            "Epoch [100/250], Loss: 432.3114\n",
            "Epoch [110/250], Loss: 230.2444\n",
            "Epoch [120/250], Loss: 218.8320\n",
            "Epoch [130/250], Loss: 111.9119\n",
            "Epoch [140/250], Loss: 147.9244\n",
            "Epoch [150/250], Loss: 112.0394\n",
            "Epoch [160/250], Loss: 119.3039\n",
            "Epoch [170/250], Loss: 195.6974\n",
            "Epoch [180/250], Loss: 216.4407\n",
            "Epoch [190/250], Loss: 258.6765\n",
            "Epoch [200/250], Loss: 174.3747\n",
            "Epoch [210/250], Loss: 59.0663\n",
            "Epoch [220/250], Loss: 37.0037\n",
            "Epoch [230/250], Loss: 21.9785\n",
            "Epoch [240/250], Loss: 113.5886\n",
            "Epoch [250/250], Loss: 31.2475\n",
            "Test Loss: 65.2853\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=32\n",
            "Epoch [10/250], Loss: 1590.6360\n",
            "Epoch [20/250], Loss: 1811.7415\n",
            "Epoch [30/250], Loss: 744.3676\n",
            "Epoch [40/250], Loss: 970.9265\n",
            "Epoch [50/250], Loss: 798.8465\n",
            "Epoch [60/250], Loss: 707.6606\n",
            "Epoch [70/250], Loss: 748.2451\n",
            "Epoch [80/250], Loss: 442.3269\n",
            "Epoch [90/250], Loss: 669.5610\n",
            "Epoch [100/250], Loss: 467.2966\n",
            "Epoch [110/250], Loss: 467.7068\n",
            "Epoch [120/250], Loss: 550.7383\n",
            "Epoch [130/250], Loss: 158.8137\n",
            "Epoch [140/250], Loss: 168.9644\n",
            "Epoch [150/250], Loss: 1003.8286\n",
            "Epoch [160/250], Loss: 207.1308\n",
            "Epoch [170/250], Loss: 248.0421\n",
            "Epoch [180/250], Loss: 174.6489\n",
            "Epoch [190/250], Loss: 140.3443\n",
            "Epoch [200/250], Loss: 155.4962\n",
            "Epoch [210/250], Loss: 324.7012\n",
            "Epoch [220/250], Loss: 198.5534\n",
            "Epoch [230/250], Loss: 119.4069\n",
            "Epoch [240/250], Loss: 96.3160\n",
            "Epoch [250/250], Loss: 211.1003\n",
            "Test Loss: 167.8275\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=64\n",
            "Epoch [10/250], Loss: 956.1803\n",
            "Epoch [20/250], Loss: 1393.7803\n",
            "Epoch [30/250], Loss: 1031.2808\n",
            "Epoch [40/250], Loss: 1498.6257\n",
            "Epoch [50/250], Loss: 881.3033\n",
            "Epoch [60/250], Loss: 927.9927\n",
            "Epoch [70/250], Loss: 654.5387\n",
            "Epoch [80/250], Loss: 1177.7557\n",
            "Epoch [90/250], Loss: 1489.6355\n",
            "Epoch [100/250], Loss: 1005.7993\n",
            "Epoch [110/250], Loss: 859.2947\n",
            "Epoch [120/250], Loss: 575.0643\n",
            "Epoch [130/250], Loss: 707.2343\n",
            "Epoch [140/250], Loss: 619.2972\n",
            "Epoch [150/250], Loss: 535.8755\n",
            "Epoch [160/250], Loss: 437.2299\n",
            "Epoch [170/250], Loss: 298.2183\n",
            "Epoch [180/250], Loss: 365.9894\n",
            "Epoch [190/250], Loss: 626.0268\n",
            "Epoch [200/250], Loss: 434.3404\n",
            "Epoch [210/250], Loss: 261.5679\n",
            "Epoch [220/250], Loss: 513.4667\n",
            "Epoch [230/250], Loss: 92.4312\n",
            "Epoch [240/250], Loss: 248.5769\n",
            "Epoch [250/250], Loss: 287.9904\n",
            "Test Loss: 244.5025\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=128\n",
            "Epoch [10/250], Loss: 1517.8013\n",
            "Epoch [20/250], Loss: 1335.5212\n",
            "Epoch [30/250], Loss: 1423.6473\n",
            "Epoch [40/250], Loss: 1656.5929\n",
            "Epoch [50/250], Loss: 1487.0906\n",
            "Epoch [60/250], Loss: 1355.9868\n",
            "Epoch [70/250], Loss: 1171.1609\n",
            "Epoch [80/250], Loss: 1304.0758\n",
            "Epoch [90/250], Loss: 1070.3135\n",
            "Epoch [100/250], Loss: 1049.7699\n",
            "Epoch [110/250], Loss: 985.0591\n",
            "Epoch [120/250], Loss: 948.0964\n",
            "Epoch [130/250], Loss: 915.2696\n",
            "Epoch [140/250], Loss: 919.0244\n",
            "Epoch [150/250], Loss: 1000.4246\n",
            "Epoch [160/250], Loss: 884.5256\n",
            "Epoch [170/250], Loss: 813.5396\n",
            "Epoch [180/250], Loss: 959.1729\n",
            "Epoch [190/250], Loss: 730.9065\n",
            "Epoch [200/250], Loss: 777.9780\n",
            "Epoch [210/250], Loss: 788.8657\n",
            "Epoch [220/250], Loss: 871.7534\n",
            "Epoch [230/250], Loss: 682.5565\n",
            "Epoch [240/250], Loss: 692.7452\n",
            "Epoch [250/250], Loss: 606.1404\n",
            "Test Loss: 526.5157\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=256\n",
            "Epoch [10/250], Loss: 1668.0883\n",
            "Epoch [20/250], Loss: 1447.5707\n",
            "Epoch [30/250], Loss: 1353.8291\n",
            "Epoch [40/250], Loss: 1387.1183\n",
            "Epoch [50/250], Loss: 1396.9852\n",
            "Epoch [60/250], Loss: 1297.0621\n",
            "Epoch [70/250], Loss: 1449.2085\n",
            "Epoch [80/250], Loss: 1390.4989\n",
            "Epoch [90/250], Loss: 1181.4856\n",
            "Epoch [100/250], Loss: 1315.3458\n",
            "Epoch [110/250], Loss: 1453.8945\n",
            "Epoch [120/250], Loss: 1269.0316\n",
            "Epoch [130/250], Loss: 1209.7800\n",
            "Epoch [140/250], Loss: 1126.2067\n",
            "Epoch [150/250], Loss: 1249.8600\n",
            "Epoch [160/250], Loss: 1130.9557\n",
            "Epoch [170/250], Loss: 969.0359\n",
            "Epoch [180/250], Loss: 1218.5464\n",
            "Epoch [190/250], Loss: 821.0400\n",
            "Epoch [200/250], Loss: 1073.5153\n",
            "Epoch [210/250], Loss: 1096.4075\n",
            "Epoch [220/250], Loss: 843.7227\n",
            "Epoch [230/250], Loss: 795.0477\n",
            "Epoch [240/250], Loss: 725.0571\n",
            "Epoch [250/250], Loss: 1000.8541\n",
            "Test Loss: 730.4310\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.01, Batch Size=512\n",
            "Epoch [10/250], Loss: 1654.3035\n",
            "Epoch [20/250], Loss: 1638.0446\n",
            "Epoch [30/250], Loss: 1621.7953\n",
            "Epoch [40/250], Loss: 1605.4065\n",
            "Epoch [50/250], Loss: 1588.7324\n",
            "Epoch [60/250], Loss: 1571.6749\n",
            "Epoch [70/250], Loss: 1554.2134\n",
            "Epoch [80/250], Loss: 1536.3917\n",
            "Epoch [90/250], Loss: 1518.3357\n",
            "Epoch [100/250], Loss: 1500.2134\n",
            "Epoch [110/250], Loss: 1482.0862\n",
            "Epoch [120/250], Loss: 1463.8351\n",
            "Epoch [130/250], Loss: 1445.2432\n",
            "Epoch [140/250], Loss: 1426.2495\n",
            "Epoch [150/250], Loss: 1406.9928\n",
            "Epoch [160/250], Loss: 1387.5237\n",
            "Epoch [170/250], Loss: 1367.8864\n",
            "Epoch [180/250], Loss: 1348.2831\n",
            "Epoch [190/250], Loss: 1328.7800\n",
            "Epoch [200/250], Loss: 1308.9902\n",
            "Epoch [210/250], Loss: 1288.6498\n",
            "Epoch [220/250], Loss: 1267.4828\n",
            "Epoch [230/250], Loss: 1244.5627\n",
            "Epoch [240/250], Loss: 1220.6924\n",
            "Epoch [250/250], Loss: 1197.7740\n",
            "Test Loss: 1049.1992\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=16\n",
            "Epoch [10/250], Loss: 1771.9167\n",
            "Epoch [20/250], Loss: 1804.1273\n",
            "Epoch [30/250], Loss: 1463.8376\n",
            "Epoch [40/250], Loss: 1368.8505\n",
            "Epoch [50/250], Loss: 1618.6456\n",
            "Epoch [60/250], Loss: 1742.3647\n",
            "Epoch [70/250], Loss: 1205.2496\n",
            "Epoch [80/250], Loss: 1725.7859\n",
            "Epoch [90/250], Loss: 807.9387\n",
            "Epoch [100/250], Loss: 510.1976\n",
            "Epoch [110/250], Loss: 934.6082\n",
            "Epoch [120/250], Loss: 1652.1969\n",
            "Epoch [130/250], Loss: 1576.8098\n",
            "Epoch [140/250], Loss: 1170.6759\n",
            "Epoch [150/250], Loss: 1098.6450\n",
            "Epoch [160/250], Loss: 1134.5494\n",
            "Epoch [170/250], Loss: 1046.5958\n",
            "Epoch [180/250], Loss: 858.9904\n",
            "Epoch [190/250], Loss: 832.4478\n",
            "Epoch [200/250], Loss: 745.9176\n",
            "Epoch [210/250], Loss: 1098.9303\n",
            "Epoch [220/250], Loss: 1240.6433\n",
            "Epoch [230/250], Loss: 587.1163\n",
            "Epoch [240/250], Loss: 785.3727\n",
            "Epoch [250/250], Loss: 904.6309\n",
            "Test Loss: 754.6665\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=32\n",
            "Epoch [10/250], Loss: 1972.1476\n",
            "Epoch [20/250], Loss: 2265.9961\n",
            "Epoch [30/250], Loss: 1431.8083\n",
            "Epoch [40/250], Loss: 1578.9821\n",
            "Epoch [50/250], Loss: 1104.3507\n",
            "Epoch [60/250], Loss: 1635.8029\n",
            "Epoch [70/250], Loss: 3068.7407\n",
            "Epoch [80/250], Loss: 2509.2688\n",
            "Epoch [90/250], Loss: 1418.5579\n",
            "Epoch [100/250], Loss: 1660.9171\n",
            "Epoch [110/250], Loss: 2209.8721\n",
            "Epoch [120/250], Loss: 1390.7958\n",
            "Epoch [130/250], Loss: 1368.9108\n",
            "Epoch [140/250], Loss: 1642.0602\n",
            "Epoch [150/250], Loss: 779.8709\n",
            "Epoch [160/250], Loss: 1136.6504\n",
            "Epoch [170/250], Loss: 1172.1681\n",
            "Epoch [180/250], Loss: 1049.3335\n",
            "Epoch [190/250], Loss: 1645.3485\n",
            "Epoch [200/250], Loss: 1242.5175\n",
            "Epoch [210/250], Loss: 1467.5850\n",
            "Epoch [220/250], Loss: 1219.1149\n",
            "Epoch [230/250], Loss: 1534.7147\n",
            "Epoch [240/250], Loss: 1323.6805\n",
            "Epoch [250/250], Loss: 997.4430\n",
            "Test Loss: 1024.9921\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=64\n",
            "Epoch [10/250], Loss: 1402.5500\n",
            "Epoch [20/250], Loss: 1198.6981\n",
            "Epoch [30/250], Loss: 1205.5472\n",
            "Epoch [40/250], Loss: 1631.1761\n",
            "Epoch [50/250], Loss: 1937.4988\n",
            "Epoch [60/250], Loss: 1061.6444\n",
            "Epoch [70/250], Loss: 1223.0587\n",
            "Epoch [80/250], Loss: 1705.4083\n",
            "Epoch [90/250], Loss: 1304.2289\n",
            "Epoch [100/250], Loss: 1212.7985\n",
            "Epoch [110/250], Loss: 1753.8301\n",
            "Epoch [120/250], Loss: 1705.2657\n",
            "Epoch [130/250], Loss: 1407.3322\n",
            "Epoch [140/250], Loss: 1522.3143\n",
            "Epoch [150/250], Loss: 1507.9371\n",
            "Epoch [160/250], Loss: 1887.9012\n",
            "Epoch [170/250], Loss: 1473.3718\n",
            "Epoch [180/250], Loss: 1415.8934\n",
            "Epoch [190/250], Loss: 1252.3589\n",
            "Epoch [200/250], Loss: 1088.7124\n",
            "Epoch [210/250], Loss: 1980.4178\n",
            "Epoch [220/250], Loss: 1155.0897\n",
            "Epoch [230/250], Loss: 1379.5082\n",
            "Epoch [240/250], Loss: 1185.2772\n",
            "Epoch [250/250], Loss: 1932.6062\n",
            "Test Loss: 1217.2822\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1725.6394\n",
            "Epoch [20/250], Loss: 1772.7373\n",
            "Epoch [30/250], Loss: 1732.1038\n",
            "Epoch [40/250], Loss: 1543.5392\n",
            "Epoch [50/250], Loss: 1607.3760\n",
            "Epoch [60/250], Loss: 1546.0913\n",
            "Epoch [70/250], Loss: 1639.7269\n",
            "Epoch [80/250], Loss: 1587.0442\n",
            "Epoch [90/250], Loss: 1702.9868\n",
            "Epoch [100/250], Loss: 1521.3591\n",
            "Epoch [110/250], Loss: 1661.1198\n",
            "Epoch [120/250], Loss: 1501.3582\n",
            "Epoch [130/250], Loss: 1547.1152\n",
            "Epoch [140/250], Loss: 1449.7965\n",
            "Epoch [150/250], Loss: 1357.4364\n",
            "Epoch [160/250], Loss: 1883.5457\n",
            "Epoch [170/250], Loss: 1432.1654\n",
            "Epoch [180/250], Loss: 1535.7054\n",
            "Epoch [190/250], Loss: 1550.1843\n",
            "Epoch [200/250], Loss: 1403.8800\n",
            "Epoch [210/250], Loss: 1435.7219\n",
            "Epoch [220/250], Loss: 1641.3126\n",
            "Epoch [230/250], Loss: 1463.0465\n",
            "Epoch [240/250], Loss: 1436.2080\n",
            "Epoch [250/250], Loss: 1540.2794\n",
            "Test Loss: 1364.2561\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1504.8252\n",
            "Epoch [20/250], Loss: 1646.6304\n",
            "Epoch [30/250], Loss: 1713.0541\n",
            "Epoch [40/250], Loss: 1725.0253\n",
            "Epoch [50/250], Loss: 1547.7518\n",
            "Epoch [60/250], Loss: 1502.8367\n",
            "Epoch [70/250], Loss: 1585.8234\n",
            "Epoch [80/250], Loss: 1726.9746\n",
            "Epoch [90/250], Loss: 1781.6503\n",
            "Epoch [100/250], Loss: 1720.4851\n",
            "Epoch [110/250], Loss: 1476.3253\n",
            "Epoch [120/250], Loss: 1666.9688\n",
            "Epoch [130/250], Loss: 1642.6064\n",
            "Epoch [140/250], Loss: 1536.8290\n",
            "Epoch [150/250], Loss: 1648.8181\n",
            "Epoch [160/250], Loss: 1666.1891\n",
            "Epoch [170/250], Loss: 1606.3523\n",
            "Epoch [180/250], Loss: 1448.2860\n",
            "Epoch [190/250], Loss: 1398.7280\n",
            "Epoch [200/250], Loss: 1566.5161\n",
            "Epoch [210/250], Loss: 1811.0743\n",
            "Epoch [220/250], Loss: 1537.7710\n",
            "Epoch [230/250], Loss: 1428.3447\n",
            "Epoch [240/250], Loss: 1569.0394\n",
            "Epoch [250/250], Loss: 1397.7927\n",
            "Test Loss: 1407.7206\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1663.5328\n",
            "Epoch [20/250], Loss: 1661.9016\n",
            "Epoch [30/250], Loss: 1660.2710\n",
            "Epoch [40/250], Loss: 1658.6410\n",
            "Epoch [50/250], Loss: 1657.0114\n",
            "Epoch [60/250], Loss: 1655.3816\n",
            "Epoch [70/250], Loss: 1653.7515\n",
            "Epoch [80/250], Loss: 1652.1206\n",
            "Epoch [90/250], Loss: 1650.4886\n",
            "Epoch [100/250], Loss: 1648.8553\n",
            "Epoch [110/250], Loss: 1647.2203\n",
            "Epoch [120/250], Loss: 1645.5831\n",
            "Epoch [130/250], Loss: 1643.9437\n",
            "Epoch [140/250], Loss: 1642.3014\n",
            "Epoch [150/250], Loss: 1640.6561\n",
            "Epoch [160/250], Loss: 1639.0079\n",
            "Epoch [170/250], Loss: 1637.3563\n",
            "Epoch [180/250], Loss: 1635.7007\n",
            "Epoch [190/250], Loss: 1634.0414\n",
            "Epoch [200/250], Loss: 1632.3778\n",
            "Epoch [210/250], Loss: 1630.7100\n",
            "Epoch [220/250], Loss: 1629.0377\n",
            "Epoch [230/250], Loss: 1627.3606\n",
            "Epoch [240/250], Loss: 1625.6790\n",
            "Epoch [250/250], Loss: 1623.9924\n",
            "Test Loss: 1452.2148\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=16\n",
            "Epoch [10/250], Loss: 1062.1461\n",
            "Epoch [20/250], Loss: 1970.8680\n",
            "Epoch [30/250], Loss: 2119.4607\n",
            "Epoch [40/250], Loss: 1551.0156\n",
            "Epoch [50/250], Loss: 1586.9500\n",
            "Epoch [60/250], Loss: 1602.6747\n",
            "Epoch [70/250], Loss: 1824.7456\n",
            "Epoch [80/250], Loss: 1560.6646\n",
            "Epoch [90/250], Loss: 1794.9938\n",
            "Epoch [100/250], Loss: 1989.0684\n",
            "Epoch [110/250], Loss: 2750.8242\n",
            "Epoch [120/250], Loss: 1487.7179\n",
            "Epoch [130/250], Loss: 1567.8419\n",
            "Epoch [140/250], Loss: 1336.6345\n",
            "Epoch [150/250], Loss: 1241.9103\n",
            "Epoch [160/250], Loss: 1584.8466\n",
            "Epoch [170/250], Loss: 1992.9999\n",
            "Epoch [180/250], Loss: 2270.7849\n",
            "Epoch [190/250], Loss: 1563.3441\n",
            "Epoch [200/250], Loss: 2264.6770\n",
            "Epoch [210/250], Loss: 1474.3318\n",
            "Epoch [220/250], Loss: 1568.7958\n",
            "Epoch [230/250], Loss: 1663.2250\n",
            "Epoch [240/250], Loss: 1602.2781\n",
            "Epoch [250/250], Loss: 1508.7031\n",
            "Test Loss: 1415.2679\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=32\n",
            "Epoch [10/250], Loss: 1973.2163\n",
            "Epoch [20/250], Loss: 1274.0740\n",
            "Epoch [30/250], Loss: 2122.8508\n",
            "Epoch [40/250], Loss: 1670.3253\n",
            "Epoch [50/250], Loss: 2025.9825\n",
            "Epoch [60/250], Loss: 1823.7141\n",
            "Epoch [70/250], Loss: 1389.8778\n",
            "Epoch [80/250], Loss: 1442.5311\n",
            "Epoch [90/250], Loss: 1280.4943\n",
            "Epoch [100/250], Loss: 1950.5712\n",
            "Epoch [110/250], Loss: 1265.8418\n",
            "Epoch [120/250], Loss: 1790.7622\n",
            "Epoch [130/250], Loss: 1932.4786\n",
            "Epoch [140/250], Loss: 1536.7358\n",
            "Epoch [150/250], Loss: 1333.9604\n",
            "Epoch [160/250], Loss: 1611.5431\n",
            "Epoch [170/250], Loss: 2000.9808\n",
            "Epoch [180/250], Loss: 1598.4507\n",
            "Epoch [190/250], Loss: 1343.9215\n",
            "Epoch [200/250], Loss: 1271.2523\n",
            "Epoch [210/250], Loss: 993.2621\n",
            "Epoch [220/250], Loss: 1067.5170\n",
            "Epoch [230/250], Loss: 1751.3324\n",
            "Epoch [240/250], Loss: 1344.4274\n",
            "Epoch [250/250], Loss: 1547.1426\n",
            "Test Loss: 1443.1667\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=64\n",
            "Epoch [10/250], Loss: 1590.9009\n",
            "Epoch [20/250], Loss: 1513.9491\n",
            "Epoch [30/250], Loss: 1512.7755\n",
            "Epoch [40/250], Loss: 1217.2173\n",
            "Epoch [50/250], Loss: 2727.4912\n",
            "Epoch [60/250], Loss: 2150.5364\n",
            "Epoch [70/250], Loss: 1680.6078\n",
            "Epoch [80/250], Loss: 1436.5962\n",
            "Epoch [90/250], Loss: 1793.8615\n",
            "Epoch [100/250], Loss: 1526.0469\n",
            "Epoch [110/250], Loss: 2250.1003\n",
            "Epoch [120/250], Loss: 2334.8530\n",
            "Epoch [130/250], Loss: 1354.5909\n",
            "Epoch [140/250], Loss: 998.2895\n",
            "Epoch [150/250], Loss: 1780.9583\n",
            "Epoch [160/250], Loss: 1716.8260\n",
            "Epoch [170/250], Loss: 1458.6685\n",
            "Epoch [180/250], Loss: 1589.2465\n",
            "Epoch [190/250], Loss: 1549.2075\n",
            "Epoch [200/250], Loss: 1315.1193\n",
            "Epoch [210/250], Loss: 1685.7374\n",
            "Epoch [220/250], Loss: 1829.0051\n",
            "Epoch [230/250], Loss: 1782.5073\n",
            "Epoch [240/250], Loss: 1621.4624\n",
            "Epoch [250/250], Loss: 1367.4869\n",
            "Test Loss: 1470.6899\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1805.2644\n",
            "Epoch [20/250], Loss: 1614.8931\n",
            "Epoch [30/250], Loss: 1692.7502\n",
            "Epoch [40/250], Loss: 1844.9637\n",
            "Epoch [50/250], Loss: 1659.0580\n",
            "Epoch [60/250], Loss: 1715.3577\n",
            "Epoch [70/250], Loss: 1739.0757\n",
            "Epoch [80/250], Loss: 1831.5281\n",
            "Epoch [90/250], Loss: 1593.4630\n",
            "Epoch [100/250], Loss: 1488.0640\n",
            "Epoch [110/250], Loss: 1432.4230\n",
            "Epoch [120/250], Loss: 1724.3113\n",
            "Epoch [130/250], Loss: 1782.5259\n",
            "Epoch [140/250], Loss: 1595.1158\n",
            "Epoch [150/250], Loss: 1751.1123\n",
            "Epoch [160/250], Loss: 1632.0621\n",
            "Epoch [170/250], Loss: 1679.5798\n",
            "Epoch [180/250], Loss: 1596.9030\n",
            "Epoch [190/250], Loss: 1506.9604\n",
            "Epoch [200/250], Loss: 1743.8600\n",
            "Epoch [210/250], Loss: 1681.5640\n",
            "Epoch [220/250], Loss: 1819.6969\n",
            "Epoch [230/250], Loss: 1700.3954\n",
            "Epoch [240/250], Loss: 1867.7432\n",
            "Epoch [250/250], Loss: 1613.8059\n",
            "Test Loss: 1475.7888\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1648.9470\n",
            "Epoch [20/250], Loss: 1610.8250\n",
            "Epoch [30/250], Loss: 1547.6737\n",
            "Epoch [40/250], Loss: 1559.4397\n",
            "Epoch [50/250], Loss: 1692.6936\n",
            "Epoch [60/250], Loss: 1377.4429\n",
            "Epoch [70/250], Loss: 1634.0044\n",
            "Epoch [80/250], Loss: 1743.4531\n",
            "Epoch [90/250], Loss: 1562.4399\n",
            "Epoch [100/250], Loss: 1763.8419\n",
            "Epoch [110/250], Loss: 1411.6646\n",
            "Epoch [120/250], Loss: 1768.5565\n",
            "Epoch [130/250], Loss: 1407.7076\n",
            "Epoch [140/250], Loss: 1827.9175\n",
            "Epoch [150/250], Loss: 1684.2297\n",
            "Epoch [160/250], Loss: 1707.2926\n",
            "Epoch [170/250], Loss: 1671.3699\n",
            "Epoch [180/250], Loss: 1435.0504\n",
            "Epoch [190/250], Loss: 1544.3469\n",
            "Epoch [200/250], Loss: 1569.3143\n",
            "Epoch [210/250], Loss: 1576.3617\n",
            "Epoch [220/250], Loss: 1527.5294\n",
            "Epoch [230/250], Loss: 1577.8002\n",
            "Epoch [240/250], Loss: 1527.4945\n",
            "Epoch [250/250], Loss: 1400.5969\n",
            "Test Loss: 1473.0995\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Softmax, Epochs=250, LR=0.0001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1665.3104\n",
            "Epoch [20/250], Loss: 1665.1469\n",
            "Epoch [30/250], Loss: 1664.9834\n",
            "Epoch [40/250], Loss: 1664.8195\n",
            "Epoch [50/250], Loss: 1664.6558\n",
            "Epoch [60/250], Loss: 1664.4923\n",
            "Epoch [70/250], Loss: 1664.3285\n",
            "Epoch [80/250], Loss: 1664.1650\n",
            "Epoch [90/250], Loss: 1664.0015\n",
            "Epoch [100/250], Loss: 1663.8378\n",
            "Epoch [110/250], Loss: 1663.6743\n",
            "Epoch [120/250], Loss: 1663.5107\n",
            "Epoch [130/250], Loss: 1663.3469\n",
            "Epoch [140/250], Loss: 1663.1833\n",
            "Epoch [150/250], Loss: 1663.0198\n",
            "Epoch [160/250], Loss: 1662.8563\n",
            "Epoch [170/250], Loss: 1662.6927\n",
            "Epoch [180/250], Loss: 1662.5293\n",
            "Epoch [190/250], Loss: 1662.3656\n",
            "Epoch [200/250], Loss: 1662.2023\n",
            "Epoch [210/250], Loss: 1662.0386\n",
            "Epoch [220/250], Loss: 1661.8752\n",
            "Epoch [230/250], Loss: 1661.7119\n",
            "Epoch [240/250], Loss: 1661.5481\n",
            "Epoch [250/250], Loss: 1661.3846\n",
            "Test Loss: 1487.7391\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=16\n",
            "Test Loss: 1189.6570\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=32\n",
            "Test Loss: 157.7666\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=64\n",
            "Test Loss: 23975.0195\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=128\n",
            "Test Loss: 19065.9629\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=256\n",
            "Test Loss: 181655.2031\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=10, Batch Size=512\n",
            "Test Loss: 5810.2920\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=16\n",
            "Test Loss: 206.2243\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=32\n",
            "Test Loss: 867.6967\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=64\n",
            "Test Loss: 1559.0222\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=128\n",
            "Test Loss: 2870.8293\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=256\n",
            "Test Loss: 6114.5176\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=1, Batch Size=512\n",
            "Test Loss: 1337.6532\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=16\n",
            "Test Loss: 160.8634\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=32\n",
            "Test Loss: 327.0077\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=64\n",
            "Test Loss: 237.7912\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=128\n",
            "Test Loss: 840.1748\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=256\n",
            "Test Loss: 927.7496\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.1, Batch Size=512\n",
            "Test Loss: 1268.6376\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=16\n",
            "Test Loss: 602.4315\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=32\n",
            "Test Loss: 876.6627\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=64\n",
            "Test Loss: 1112.8000\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=128\n",
            "Test Loss: 1360.5081\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=256\n",
            "Test Loss: 1386.1586\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.01, Batch Size=512\n",
            "Test Loss: 1434.8518\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=16\n",
            "Test Loss: 1351.9612\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=32\n",
            "Test Loss: 1446.0779\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=64\n",
            "Test Loss: 1444.4252\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=128\n",
            "Test Loss: 1468.3295\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=256\n",
            "Test Loss: 1476.3370\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.001, Batch Size=512\n",
            "Test Loss: 1483.7083\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=16\n",
            "Test Loss: 1491.9666\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=32\n",
            "Test Loss: 1485.6569\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=64\n",
            "Test Loss: 1491.5892\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=128\n",
            "Test Loss: 1496.0579\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=256\n",
            "Test Loss: 1496.2886\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=1, LR=0.0001, Batch Size=512\n",
            "Test Loss: 1494.7822\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=16\n",
            "Epoch [10/10], Loss: 72.4839\n",
            "Test Loss: 112.2601\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=32\n",
            "Epoch [10/10], Loss: 259.1934\n",
            "Test Loss: 220.8667\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=64\n",
            "Epoch [10/10], Loss: 111.4791\n",
            "Test Loss: 129.3194\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=128\n",
            "Epoch [10/10], Loss: 7337.1343\n",
            "Test Loss: 1615.9924\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=256\n",
            "Epoch [10/10], Loss: 8805.5918\n",
            "Test Loss: 16690.4551\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=10, Batch Size=512\n",
            "Epoch [10/10], Loss: 59515.9805\n",
            "Test Loss: 24575.2207\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=16\n",
            "Epoch [10/10], Loss: 102.1399\n",
            "Test Loss: 95.0321\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=32\n",
            "Epoch [10/10], Loss: 40.5502\n",
            "Test Loss: 89.5833\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=64\n",
            "Epoch [10/10], Loss: 85.0112\n",
            "Test Loss: 90.2766\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=128\n",
            "Epoch [10/10], Loss: 75.8585\n",
            "Test Loss: 70.8576\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=256\n",
            "Epoch [10/10], Loss: 192.1558\n",
            "Test Loss: 92.3081\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=1, Batch Size=512\n",
            "Epoch [10/10], Loss: 823.5432\n",
            "Test Loss: 966.4769\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=16\n",
            "Epoch [10/10], Loss: 18.3833\n",
            "Test Loss: 62.4139\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=32\n",
            "Epoch [10/10], Loss: 36.9997\n",
            "Test Loss: 54.9892\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=64\n",
            "Epoch [10/10], Loss: 64.6452\n",
            "Test Loss: 104.8544\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=128\n",
            "Epoch [10/10], Loss: 102.1809\n",
            "Test Loss: 103.9804\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=256\n",
            "Epoch [10/10], Loss: 275.3334\n",
            "Test Loss: 187.5915\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.1, Batch Size=512\n",
            "Epoch [10/10], Loss: 192.7707\n",
            "Test Loss: 215.7773\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=16\n",
            "Epoch [10/10], Loss: 141.8694\n",
            "Test Loss: 171.6283\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=32\n",
            "Epoch [10/10], Loss: 135.1123\n",
            "Test Loss: 172.6381\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=64\n",
            "Epoch [10/10], Loss: 69.3963\n",
            "Test Loss: 171.9010\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=128\n",
            "Epoch [10/10], Loss: 505.3810\n",
            "Test Loss: 387.6959\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=256\n",
            "Epoch [10/10], Loss: 850.5692\n",
            "Test Loss: 606.8447\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.01, Batch Size=512\n",
            "Epoch [10/10], Loss: 1147.2120\n",
            "Test Loss: 966.4559\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=16\n",
            "Epoch [10/10], Loss: 696.9939\n",
            "Test Loss: 499.6928\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=32\n",
            "Epoch [10/10], Loss: 692.5473\n",
            "Test Loss: 777.3934\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=64\n",
            "Epoch [10/10], Loss: 1146.5488\n",
            "Test Loss: 1025.8939\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=128\n",
            "Epoch [10/10], Loss: 1625.5690\n",
            "Test Loss: 1251.4916\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=256\n",
            "Epoch [10/10], Loss: 1384.7263\n",
            "Test Loss: 1331.3451\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.001, Batch Size=512\n",
            "Epoch [10/10], Loss: 1618.6594\n",
            "Test Loss: 1443.0486\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=16\n",
            "Epoch [10/10], Loss: 1300.6659\n",
            "Test Loss: 1285.5471\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=32\n",
            "Epoch [10/10], Loss: 1710.9888\n",
            "Test Loss: 1429.4183\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=64\n",
            "Epoch [10/10], Loss: 1686.5742\n",
            "Test Loss: 1454.8521\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=128\n",
            "Epoch [10/10], Loss: 1647.0679\n",
            "Test Loss: 1463.6992\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=256\n",
            "Epoch [10/10], Loss: 1577.1786\n",
            "Test Loss: 1486.5769\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=10, LR=0.0001, Batch Size=512\n",
            "Epoch [10/10], Loss: 1654.5812\n",
            "Test Loss: 1481.4656\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=16\n",
            "Epoch [10/25], Loss: 202.4109\n",
            "Epoch [20/25], Loss: 129.8423\n",
            "Test Loss: 392.3571\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=32\n",
            "Epoch [10/25], Loss: 56.1391\n",
            "Epoch [20/25], Loss: 251.8479\n",
            "Test Loss: 153.0697\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=64\n",
            "Epoch [10/25], Loss: 242.9715\n",
            "Epoch [20/25], Loss: 69.6527\n",
            "Test Loss: 228.8483\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=128\n",
            "Epoch [10/25], Loss: 6725.4810\n",
            "Epoch [20/25], Loss: 845.0828\n",
            "Test Loss: 233.6161\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=256\n",
            "Epoch [10/25], Loss: 2707.4900\n",
            "Epoch [20/25], Loss: 1134.4320\n",
            "Test Loss: 3160.7812\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=10, Batch Size=512\n",
            "Epoch [10/25], Loss: 48973.4336\n",
            "Epoch [20/25], Loss: 13909.7070\n",
            "Test Loss: 18333.2910\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=16\n",
            "Epoch [10/25], Loss: 97.2630\n",
            "Epoch [20/25], Loss: 280.7290\n",
            "Test Loss: 149.6089\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=32\n",
            "Epoch [10/25], Loss: 164.7363\n",
            "Epoch [20/25], Loss: 92.4682\n",
            "Test Loss: 99.3908\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=64\n",
            "Epoch [10/25], Loss: 157.1551\n",
            "Epoch [20/25], Loss: 87.5433\n",
            "Test Loss: 92.8090\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=128\n",
            "Epoch [10/25], Loss: 162.1121\n",
            "Epoch [20/25], Loss: 125.4947\n",
            "Test Loss: 100.8330\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=256\n",
            "Epoch [10/25], Loss: 186.9681\n",
            "Epoch [20/25], Loss: 97.5712\n",
            "Test Loss: 100.3124\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=1, Batch Size=512\n",
            "Epoch [10/25], Loss: 666.0001\n",
            "Epoch [20/25], Loss: 188.6683\n",
            "Test Loss: 109.1757\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=16\n",
            "Epoch [10/25], Loss: 84.7614\n",
            "Epoch [20/25], Loss: 43.7664\n",
            "Test Loss: 74.9335\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=32\n",
            "Epoch [10/25], Loss: 50.8444\n",
            "Epoch [20/25], Loss: 47.8261\n",
            "Test Loss: 61.6110\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=64\n",
            "Epoch [10/25], Loss: 58.5866\n",
            "Epoch [20/25], Loss: 13.2749\n",
            "Test Loss: 54.8054\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=128\n",
            "Epoch [10/25], Loss: 280.3424\n",
            "Epoch [20/25], Loss: 156.0019\n",
            "Test Loss: 55.9488\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=256\n",
            "Epoch [10/25], Loss: 87.9091\n",
            "Epoch [20/25], Loss: 79.0088\n",
            "Test Loss: 52.6643\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.1, Batch Size=512\n",
            "Epoch [10/25], Loss: 226.2332\n",
            "Epoch [20/25], Loss: 121.7613\n",
            "Test Loss: 73.3171\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=16\n",
            "Epoch [10/25], Loss: 199.7518\n",
            "Epoch [20/25], Loss: 130.6706\n",
            "Test Loss: 170.2808\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=32\n",
            "Epoch [10/25], Loss: 755.4630\n",
            "Epoch [20/25], Loss: 129.6534\n",
            "Test Loss: 171.3082\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=64\n",
            "Epoch [10/25], Loss: 105.4033\n",
            "Epoch [20/25], Loss: 100.8495\n",
            "Test Loss: 171.9088\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=128\n",
            "Epoch [10/25], Loss: 478.1974\n",
            "Epoch [20/25], Loss: 286.1252\n",
            "Test Loss: 168.5708\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=256\n",
            "Epoch [10/25], Loss: 750.6962\n",
            "Epoch [20/25], Loss: 378.9912\n",
            "Test Loss: 192.9159\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.01, Batch Size=512\n",
            "Epoch [10/25], Loss: 1072.6874\n",
            "Epoch [20/25], Loss: 723.2507\n",
            "Test Loss: 465.5221\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=16\n",
            "Epoch [10/25], Loss: 745.1262\n",
            "Epoch [20/25], Loss: 577.1775\n",
            "Test Loss: 166.2953\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=32\n",
            "Epoch [10/25], Loss: 716.3530\n",
            "Epoch [20/25], Loss: 551.5239\n",
            "Test Loss: 396.8459\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=64\n",
            "Epoch [10/25], Loss: 673.2742\n",
            "Epoch [20/25], Loss: 878.6166\n",
            "Test Loss: 651.2627\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=128\n",
            "Epoch [10/25], Loss: 1518.4452\n",
            "Epoch [20/25], Loss: 1240.4862\n",
            "Test Loss: 907.5069\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=256\n",
            "Epoch [10/25], Loss: 1434.0607\n",
            "Epoch [20/25], Loss: 1319.1704\n",
            "Test Loss: 1035.3551\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.001, Batch Size=512\n",
            "Epoch [10/25], Loss: 1614.5176\n",
            "Epoch [20/25], Loss: 1520.2827\n",
            "Test Loss: 1290.1139\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=16\n",
            "Epoch [10/25], Loss: 1373.4673\n",
            "Epoch [20/25], Loss: 1143.5477\n",
            "Test Loss: 971.4940\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=32\n",
            "Epoch [10/25], Loss: 1557.3569\n",
            "Epoch [20/25], Loss: 1709.4418\n",
            "Test Loss: 1213.1350\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=64\n",
            "Epoch [10/25], Loss: 1892.3492\n",
            "Epoch [20/25], Loss: 1439.2249\n",
            "Test Loss: 1342.5048\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=128\n",
            "Epoch [10/25], Loss: 1801.0446\n",
            "Epoch [20/25], Loss: 1561.3967\n",
            "Test Loss: 1446.2819\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=256\n",
            "Epoch [10/25], Loss: 1482.8876\n",
            "Epoch [20/25], Loss: 1844.3729\n",
            "Test Loss: 1466.0209\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=25, LR=0.0001, Batch Size=512\n",
            "Epoch [10/25], Loss: 1641.1699\n",
            "Epoch [20/25], Loss: 1636.2598\n",
            "Test Loss: 1462.0081\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=16\n",
            "Epoch [10/50], Loss: 120.1926\n",
            "Epoch [20/50], Loss: 232.1589\n",
            "Epoch [30/50], Loss: 625.7675\n",
            "Epoch [40/50], Loss: 499.6111\n",
            "Epoch [50/50], Loss: 404.7117\n",
            "Test Loss: 2187.9592\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=32\n",
            "Epoch [10/50], Loss: 113.6449\n",
            "Epoch [20/50], Loss: 74.6623\n",
            "Epoch [30/50], Loss: 124.9369\n",
            "Epoch [40/50], Loss: 55.7813\n",
            "Epoch [50/50], Loss: 143.4848\n",
            "Test Loss: 132.8868\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=64\n",
            "Epoch [10/50], Loss: 399.5363\n",
            "Epoch [20/50], Loss: 183.3439\n",
            "Epoch [30/50], Loss: 132.3130\n",
            "Epoch [40/50], Loss: 770.1133\n",
            "Epoch [50/50], Loss: 144.1075\n",
            "Test Loss: 221.4530\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=128\n",
            "Epoch [10/50], Loss: 2355.0537\n",
            "Epoch [20/50], Loss: 168.2320\n",
            "Epoch [30/50], Loss: 148.2527\n",
            "Epoch [40/50], Loss: 114.0782\n",
            "Epoch [50/50], Loss: 129.8194\n",
            "Test Loss: 103.3649\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=256\n",
            "Epoch [10/50], Loss: 24695.8867\n",
            "Epoch [20/50], Loss: 1139.3971\n",
            "Epoch [30/50], Loss: 2677.4666\n",
            "Epoch [40/50], Loss: 644.2857\n",
            "Epoch [50/50], Loss: 149.2321\n",
            "Test Loss: 175.6133\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=10, Batch Size=512\n",
            "Epoch [10/50], Loss: 39436.0938\n",
            "Epoch [20/50], Loss: 10039.3037\n",
            "Epoch [30/50], Loss: 1944.8678\n",
            "Epoch [40/50], Loss: 1528.4480\n",
            "Epoch [50/50], Loss: 860.2679\n",
            "Test Loss: 854.5365\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=16\n",
            "Epoch [10/50], Loss: 178.7053\n",
            "Epoch [20/50], Loss: 241.5847\n",
            "Epoch [30/50], Loss: 183.2844\n",
            "Epoch [40/50], Loss: 183.6077\n",
            "Epoch [50/50], Loss: 136.5912\n",
            "Test Loss: 97.3591\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=32\n",
            "Epoch [10/50], Loss: 21.5719\n",
            "Epoch [20/50], Loss: 31.3751\n",
            "Epoch [30/50], Loss: 77.6396\n",
            "Epoch [40/50], Loss: 131.5531\n",
            "Epoch [50/50], Loss: 84.3654\n",
            "Test Loss: 101.5962\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=64\n",
            "Epoch [10/50], Loss: 52.1195\n",
            "Epoch [20/50], Loss: 87.1880\n",
            "Epoch [30/50], Loss: 659.6237\n",
            "Epoch [40/50], Loss: 79.3605\n",
            "Epoch [50/50], Loss: 97.7649\n",
            "Test Loss: 126.5619\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=128\n",
            "Epoch [10/50], Loss: 143.6779\n",
            "Epoch [20/50], Loss: 91.2807\n",
            "Epoch [30/50], Loss: 70.8990\n",
            "Epoch [40/50], Loss: 87.4635\n",
            "Epoch [50/50], Loss: 84.8039\n",
            "Test Loss: 75.0745\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=256\n",
            "Epoch [10/50], Loss: 191.4843\n",
            "Epoch [20/50], Loss: 68.7451\n",
            "Epoch [30/50], Loss: 77.6209\n",
            "Epoch [40/50], Loss: 86.4311\n",
            "Epoch [50/50], Loss: 107.6022\n",
            "Test Loss: 80.3896\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=1, Batch Size=512\n",
            "Epoch [10/50], Loss: 1018.0704\n",
            "Epoch [20/50], Loss: 314.0995\n",
            "Epoch [30/50], Loss: 217.1795\n",
            "Epoch [40/50], Loss: 144.3309\n",
            "Epoch [50/50], Loss: 124.2564\n",
            "Test Loss: 185.2467\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=16\n",
            "Epoch [10/50], Loss: 39.0335\n",
            "Epoch [20/50], Loss: 84.5887\n",
            "Epoch [30/50], Loss: 44.5366\n",
            "Epoch [40/50], Loss: 74.8035\n",
            "Epoch [50/50], Loss: 42.3279\n",
            "Test Loss: 74.4692\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=32\n",
            "Epoch [10/50], Loss: 21.9406\n",
            "Epoch [20/50], Loss: 49.6871\n",
            "Epoch [30/50], Loss: 81.8907\n",
            "Epoch [40/50], Loss: 64.7073\n",
            "Epoch [50/50], Loss: 54.3562\n",
            "Test Loss: 68.2689\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=64\n",
            "Epoch [10/50], Loss: 132.0727\n",
            "Epoch [20/50], Loss: 99.3351\n",
            "Epoch [30/50], Loss: 594.3754\n",
            "Epoch [40/50], Loss: 23.9271\n",
            "Epoch [50/50], Loss: 54.6122\n",
            "Test Loss: 62.4206\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=128\n",
            "Epoch [10/50], Loss: 63.9678\n",
            "Epoch [20/50], Loss: 67.2701\n",
            "Epoch [30/50], Loss: 115.3430\n",
            "Epoch [40/50], Loss: 117.1752\n",
            "Epoch [50/50], Loss: 52.6937\n",
            "Test Loss: 47.7034\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=256\n",
            "Epoch [10/50], Loss: 174.3025\n",
            "Epoch [20/50], Loss: 171.5119\n",
            "Epoch [30/50], Loss: 174.1399\n",
            "Epoch [40/50], Loss: 167.8526\n",
            "Epoch [50/50], Loss: 171.0323\n",
            "Test Loss: 170.1294\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.1, Batch Size=512\n",
            "Epoch [10/50], Loss: 268.1980\n",
            "Epoch [20/50], Loss: 135.5721\n",
            "Epoch [30/50], Loss: 110.5495\n",
            "Epoch [40/50], Loss: 93.0983\n",
            "Epoch [50/50], Loss: 80.1709\n",
            "Test Loss: 84.8765\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=16\n",
            "Epoch [10/50], Loss: 84.5356\n",
            "Epoch [20/50], Loss: 184.4632\n",
            "Epoch [30/50], Loss: 264.9117\n",
            "Epoch [40/50], Loss: 155.3210\n",
            "Epoch [50/50], Loss: 235.5607\n",
            "Test Loss: 173.2202\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=32\n",
            "Epoch [10/50], Loss: 338.6588\n",
            "Epoch [20/50], Loss: 93.7308\n",
            "Epoch [30/50], Loss: 150.0278\n",
            "Epoch [40/50], Loss: 138.5007\n",
            "Epoch [50/50], Loss: 54.1438\n",
            "Test Loss: 172.6985\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=64\n",
            "Epoch [10/50], Loss: 187.6596\n",
            "Epoch [20/50], Loss: 139.0342\n",
            "Epoch [30/50], Loss: 158.6975\n",
            "Epoch [40/50], Loss: 107.7317\n",
            "Epoch [50/50], Loss: 150.5205\n",
            "Test Loss: 172.1245\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=128\n",
            "Epoch [10/50], Loss: 506.0392\n",
            "Epoch [20/50], Loss: 249.3259\n",
            "Epoch [30/50], Loss: 165.0106\n",
            "Epoch [40/50], Loss: 177.5735\n",
            "Epoch [50/50], Loss: 194.9039\n",
            "Test Loss: 172.1497\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=256\n",
            "Epoch [10/50], Loss: 754.6602\n",
            "Epoch [20/50], Loss: 282.3132\n",
            "Epoch [30/50], Loss: 219.6710\n",
            "Epoch [40/50], Loss: 171.4757\n",
            "Epoch [50/50], Loss: 154.6371\n",
            "Test Loss: 173.0828\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.01, Batch Size=512\n",
            "Epoch [10/50], Loss: 1111.7897\n",
            "Epoch [20/50], Loss: 753.5366\n",
            "Epoch [30/50], Loss: 494.3862\n",
            "Epoch [40/50], Loss: 332.8488\n",
            "Epoch [50/50], Loss: 245.9886\n",
            "Test Loss: 194.5602\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=16\n",
            "Epoch [10/50], Loss: 657.4188\n",
            "Epoch [20/50], Loss: 273.3564\n",
            "Epoch [30/50], Loss: 176.0205\n",
            "Epoch [40/50], Loss: 85.3166\n",
            "Epoch [50/50], Loss: 61.1712\n",
            "Test Loss: 61.1542\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=32\n",
            "Epoch [10/50], Loss: 742.7838\n",
            "Epoch [20/50], Loss: 640.6897\n",
            "Epoch [30/50], Loss: 300.7604\n",
            "Epoch [40/50], Loss: 138.2467\n",
            "Epoch [50/50], Loss: 429.7986\n",
            "Test Loss: 156.8025\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=64\n",
            "Epoch [10/50], Loss: 915.0307\n",
            "Epoch [20/50], Loss: 1090.5966\n",
            "Epoch [30/50], Loss: 440.2102\n",
            "Epoch [40/50], Loss: 415.9890\n",
            "Epoch [50/50], Loss: 516.9930\n",
            "Test Loss: 337.1319\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=128\n",
            "Epoch [10/50], Loss: 1194.8895\n",
            "Epoch [20/50], Loss: 1066.5356\n",
            "Epoch [30/50], Loss: 900.3603\n",
            "Epoch [40/50], Loss: 726.8324\n",
            "Epoch [50/50], Loss: 835.5595\n",
            "Test Loss: 661.4989\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=256\n",
            "Epoch [10/50], Loss: 1420.7483\n",
            "Epoch [20/50], Loss: 1212.3594\n",
            "Epoch [30/50], Loss: 1038.0824\n",
            "Epoch [40/50], Loss: 932.1158\n",
            "Epoch [50/50], Loss: 866.0248\n",
            "Test Loss: 785.5610\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.001, Batch Size=512\n",
            "Epoch [10/50], Loss: 1610.8356\n",
            "Epoch [20/50], Loss: 1528.8319\n",
            "Epoch [30/50], Loss: 1412.8971\n",
            "Epoch [40/50], Loss: 1293.4276\n",
            "Epoch [50/50], Loss: 1188.3425\n",
            "Test Loss: 1035.7765\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=16\n",
            "Epoch [10/50], Loss: 1610.4985\n",
            "Epoch [20/50], Loss: 1539.2020\n",
            "Epoch [30/50], Loss: 892.2688\n",
            "Epoch [40/50], Loss: 1119.3156\n",
            "Epoch [50/50], Loss: 475.8806\n",
            "Test Loss: 738.4549\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=32\n",
            "Epoch [10/50], Loss: 1160.2173\n",
            "Epoch [20/50], Loss: 2002.7172\n",
            "Epoch [30/50], Loss: 1205.4937\n",
            "Epoch [40/50], Loss: 1061.7798\n",
            "Epoch [50/50], Loss: 2003.5059\n",
            "Test Loss: 923.1726\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=64\n",
            "Epoch [10/50], Loss: 1951.0015\n",
            "Epoch [20/50], Loss: 1989.9871\n",
            "Epoch [30/50], Loss: 1318.5038\n",
            "Epoch [40/50], Loss: 1569.3363\n",
            "Epoch [50/50], Loss: 989.6377\n",
            "Test Loss: 1115.2701\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=128\n",
            "Epoch [10/50], Loss: 1642.0088\n",
            "Epoch [20/50], Loss: 1433.7543\n",
            "Epoch [30/50], Loss: 1685.2732\n",
            "Epoch [40/50], Loss: 1656.1447\n",
            "Epoch [50/50], Loss: 1797.8131\n",
            "Test Loss: 1363.1984\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=256\n",
            "Epoch [10/50], Loss: 1620.7551\n",
            "Epoch [20/50], Loss: 1747.9398\n",
            "Epoch [30/50], Loss: 1735.8389\n",
            "Epoch [40/50], Loss: 1924.7662\n",
            "Epoch [50/50], Loss: 1445.7465\n",
            "Test Loss: 1419.2662\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=50, LR=0.0001, Batch Size=512\n",
            "Epoch [10/50], Loss: 1653.0315\n",
            "Epoch [20/50], Loss: 1648.5707\n",
            "Epoch [30/50], Loss: 1643.9840\n",
            "Epoch [40/50], Loss: 1639.1271\n",
            "Epoch [50/50], Loss: 1633.8495\n",
            "Test Loss: 1461.9028\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=16\n",
            "Epoch [10/100], Loss: 241.3953\n",
            "Epoch [20/100], Loss: 1686.5918\n",
            "Epoch [30/100], Loss: 183.5212\n",
            "Epoch [40/100], Loss: 211.7578\n",
            "Epoch [50/100], Loss: 341.2508\n",
            "Epoch [60/100], Loss: 112.7352\n",
            "Epoch [70/100], Loss: 544.5984\n",
            "Epoch [80/100], Loss: 247.0060\n",
            "Epoch [90/100], Loss: 230.1315\n",
            "Epoch [100/100], Loss: 147.8948\n",
            "Test Loss: 121.5424\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=32\n",
            "Epoch [10/100], Loss: 59.9461\n",
            "Epoch [20/100], Loss: 65.2658\n",
            "Epoch [30/100], Loss: 103.6776\n",
            "Epoch [40/100], Loss: 89.8359\n",
            "Epoch [50/100], Loss: 407.1965\n",
            "Epoch [60/100], Loss: 65.3367\n",
            "Epoch [70/100], Loss: 153.1046\n",
            "Epoch [80/100], Loss: 210.1809\n",
            "Epoch [90/100], Loss: 510.7254\n",
            "Epoch [100/100], Loss: 181.0016\n",
            "Test Loss: 351.4408\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=64\n",
            "Epoch [10/100], Loss: 442.8200\n",
            "Epoch [20/100], Loss: 234.6588\n",
            "Epoch [30/100], Loss: 50.1450\n",
            "Epoch [40/100], Loss: 50.7304\n",
            "Epoch [50/100], Loss: 419.8915\n",
            "Epoch [60/100], Loss: 159.4003\n",
            "Epoch [70/100], Loss: 162.0170\n",
            "Epoch [80/100], Loss: 180.6382\n",
            "Epoch [90/100], Loss: 189.5897\n",
            "Epoch [100/100], Loss: 63.0173\n",
            "Test Loss: 86.3541\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=128\n",
            "Epoch [10/100], Loss: 360.2711\n",
            "Epoch [20/100], Loss: 176.2464\n",
            "Epoch [30/100], Loss: 106.6453\n",
            "Epoch [40/100], Loss: 101.9338\n",
            "Epoch [50/100], Loss: 123.9076\n",
            "Epoch [60/100], Loss: 192.0254\n",
            "Epoch [70/100], Loss: 157.4835\n",
            "Epoch [80/100], Loss: 358.7802\n",
            "Epoch [90/100], Loss: 113.3831\n",
            "Epoch [100/100], Loss: 112.3596\n",
            "Test Loss: 91.8410\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=256\n",
            "Epoch [10/100], Loss: 14532.0430\n",
            "Epoch [20/100], Loss: 2994.8701\n",
            "Epoch [30/100], Loss: 143.5621\n",
            "Epoch [40/100], Loss: 238.4229\n",
            "Epoch [50/100], Loss: 134.6639\n",
            "Epoch [60/100], Loss: 107.2436\n",
            "Epoch [70/100], Loss: 92.7784\n",
            "Epoch [80/100], Loss: 150.2408\n",
            "Epoch [90/100], Loss: 132.3029\n",
            "Epoch [100/100], Loss: 102.7932\n",
            "Test Loss: 112.3776\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=10, Batch Size=512\n",
            "Epoch [10/100], Loss: 60294.9258\n",
            "Epoch [20/100], Loss: 17899.3594\n",
            "Epoch [30/100], Loss: 9160.3018\n",
            "Epoch [40/100], Loss: 1332.2294\n",
            "Epoch [50/100], Loss: 798.0753\n",
            "Epoch [60/100], Loss: 573.2478\n",
            "Epoch [70/100], Loss: 433.4384\n",
            "Epoch [80/100], Loss: 355.9192\n",
            "Epoch [90/100], Loss: 504.9865\n",
            "Epoch [100/100], Loss: 599.7969\n",
            "Test Loss: 773.8804\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=16\n",
            "Epoch [10/100], Loss: 70.4534\n",
            "Epoch [20/100], Loss: 93.5163\n",
            "Epoch [30/100], Loss: 80.8324\n",
            "Epoch [40/100], Loss: 228.4173\n",
            "Epoch [50/100], Loss: 244.7600\n",
            "Epoch [60/100], Loss: 131.1169\n",
            "Epoch [70/100], Loss: 64.3711\n",
            "Epoch [80/100], Loss: 53.8477\n",
            "Epoch [90/100], Loss: 73.6605\n",
            "Epoch [100/100], Loss: 83.4967\n",
            "Test Loss: 101.7809\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=32\n",
            "Epoch [10/100], Loss: 58.8479\n",
            "Epoch [20/100], Loss: 166.5795\n",
            "Epoch [30/100], Loss: 238.9843\n",
            "Epoch [40/100], Loss: 117.8628\n",
            "Epoch [50/100], Loss: 61.7776\n",
            "Epoch [60/100], Loss: 90.0300\n",
            "Epoch [70/100], Loss: 97.6097\n",
            "Epoch [80/100], Loss: 110.2762\n",
            "Epoch [90/100], Loss: 176.0707\n",
            "Epoch [100/100], Loss: 72.3879\n",
            "Test Loss: 90.7413\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=64\n",
            "Epoch [10/100], Loss: 100.6718\n",
            "Epoch [20/100], Loss: 166.6893\n",
            "Epoch [30/100], Loss: 107.9305\n",
            "Epoch [40/100], Loss: 143.4914\n",
            "Epoch [50/100], Loss: 38.3879\n",
            "Epoch [60/100], Loss: 131.1247\n",
            "Epoch [70/100], Loss: 66.4872\n",
            "Epoch [80/100], Loss: 702.7236\n",
            "Epoch [90/100], Loss: 28.9524\n",
            "Epoch [100/100], Loss: 46.8905\n",
            "Test Loss: 149.0037\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=128\n",
            "Epoch [10/100], Loss: 162.3241\n",
            "Epoch [20/100], Loss: 59.3895\n",
            "Epoch [30/100], Loss: 164.9015\n",
            "Epoch [40/100], Loss: 169.3108\n",
            "Epoch [50/100], Loss: 178.4233\n",
            "Epoch [60/100], Loss: 111.0896\n",
            "Epoch [70/100], Loss: 83.9181\n",
            "Epoch [80/100], Loss: 160.9214\n",
            "Epoch [90/100], Loss: 95.3124\n",
            "Epoch [100/100], Loss: 85.1258\n",
            "Test Loss: 79.1502\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=256\n",
            "Epoch [10/100], Loss: 115.7285\n",
            "Epoch [20/100], Loss: 129.3520\n",
            "Epoch [30/100], Loss: 77.6384\n",
            "Epoch [40/100], Loss: 88.2410\n",
            "Epoch [50/100], Loss: 81.8567\n",
            "Epoch [60/100], Loss: 52.1182\n",
            "Epoch [70/100], Loss: 59.2653\n",
            "Epoch [80/100], Loss: 89.1811\n",
            "Epoch [90/100], Loss: 149.9797\n",
            "Epoch [100/100], Loss: 74.9843\n",
            "Test Loss: 57.2144\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=1, Batch Size=512\n",
            "Epoch [10/100], Loss: 982.0390\n",
            "Epoch [20/100], Loss: 209.3769\n",
            "Epoch [30/100], Loss: 120.2850\n",
            "Epoch [40/100], Loss: 102.3128\n",
            "Epoch [50/100], Loss: 109.7374\n",
            "Epoch [60/100], Loss: 109.4457\n",
            "Epoch [70/100], Loss: 106.2295\n",
            "Epoch [80/100], Loss: 96.3315\n",
            "Epoch [90/100], Loss: 95.8529\n",
            "Epoch [100/100], Loss: 105.9959\n",
            "Test Loss: 74.6216\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=16\n",
            "Epoch [10/100], Loss: 61.5334\n",
            "Epoch [20/100], Loss: 45.9815\n",
            "Epoch [30/100], Loss: 116.7759\n",
            "Epoch [40/100], Loss: 70.5097\n",
            "Epoch [50/100], Loss: 170.1764\n",
            "Epoch [60/100], Loss: 172.5697\n",
            "Epoch [70/100], Loss: 32.5056\n",
            "Epoch [80/100], Loss: 127.2787\n",
            "Epoch [90/100], Loss: 39.0325\n",
            "Epoch [100/100], Loss: 93.9584\n",
            "Test Loss: 65.7354\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=32\n",
            "Epoch [10/100], Loss: 26.2632\n",
            "Epoch [20/100], Loss: 49.0698\n",
            "Epoch [30/100], Loss: 54.5156\n",
            "Epoch [40/100], Loss: 45.2710\n",
            "Epoch [50/100], Loss: 817.5451\n",
            "Epoch [60/100], Loss: 31.6882\n",
            "Epoch [70/100], Loss: 183.5980\n",
            "Epoch [80/100], Loss: 67.0440\n",
            "Epoch [90/100], Loss: 68.1981\n",
            "Epoch [100/100], Loss: 81.0600\n",
            "Test Loss: 62.2744\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=64\n",
            "Epoch [10/100], Loss: 154.2960\n",
            "Epoch [20/100], Loss: 50.5922\n",
            "Epoch [30/100], Loss: 192.2264\n",
            "Epoch [40/100], Loss: 49.3921\n",
            "Epoch [50/100], Loss: 65.5334\n",
            "Epoch [60/100], Loss: 43.5512\n",
            "Epoch [70/100], Loss: 247.3931\n",
            "Epoch [80/100], Loss: 76.6311\n",
            "Epoch [90/100], Loss: 50.4709\n",
            "Epoch [100/100], Loss: 66.1198\n",
            "Test Loss: 61.6149\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=128\n",
            "Epoch [10/100], Loss: 170.7124\n",
            "Epoch [20/100], Loss: 104.0819\n",
            "Epoch [30/100], Loss: 62.5124\n",
            "Epoch [40/100], Loss: 65.5517\n",
            "Epoch [50/100], Loss: 124.1492\n",
            "Epoch [60/100], Loss: 49.3947\n",
            "Epoch [70/100], Loss: 85.0027\n",
            "Epoch [80/100], Loss: 58.0054\n",
            "Epoch [90/100], Loss: 120.4059\n",
            "Epoch [100/100], Loss: 68.0020\n",
            "Test Loss: 47.4791\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=256\n",
            "Epoch [10/100], Loss: 104.9608\n",
            "Epoch [20/100], Loss: 90.2777\n",
            "Epoch [30/100], Loss: 59.3208\n",
            "Epoch [40/100], Loss: 116.1675\n",
            "Epoch [50/100], Loss: 138.3717\n",
            "Epoch [60/100], Loss: 59.1864\n",
            "Epoch [70/100], Loss: 43.4959\n",
            "Epoch [80/100], Loss: 44.7931\n",
            "Epoch [90/100], Loss: 26.1262\n",
            "Epoch [100/100], Loss: 72.7006\n",
            "Test Loss: 53.0589\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.1, Batch Size=512\n",
            "Epoch [10/100], Loss: 190.2371\n",
            "Epoch [20/100], Loss: 207.1853\n",
            "Epoch [30/100], Loss: 213.0278\n",
            "Epoch [40/100], Loss: 199.3058\n",
            "Epoch [50/100], Loss: 191.5601\n",
            "Epoch [60/100], Loss: 189.0787\n",
            "Epoch [70/100], Loss: 188.4024\n",
            "Epoch [80/100], Loss: 188.2082\n",
            "Epoch [90/100], Loss: 188.0979\n",
            "Epoch [100/100], Loss: 181.4454\n",
            "Test Loss: 165.8731\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=16\n",
            "Epoch [10/100], Loss: 109.7974\n",
            "Epoch [20/100], Loss: 233.6000\n",
            "Epoch [30/100], Loss: 178.5684\n",
            "Epoch [40/100], Loss: 166.5129\n",
            "Epoch [50/100], Loss: 61.0656\n",
            "Epoch [60/100], Loss: 189.4607\n",
            "Epoch [70/100], Loss: 229.2668\n",
            "Epoch [80/100], Loss: 103.5315\n",
            "Epoch [90/100], Loss: 146.3972\n",
            "Epoch [100/100], Loss: 110.5017\n",
            "Test Loss: 173.6546\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=32\n",
            "Epoch [10/100], Loss: 139.0141\n",
            "Epoch [20/100], Loss: 164.2867\n",
            "Epoch [30/100], Loss: 205.9075\n",
            "Epoch [40/100], Loss: 201.7980\n",
            "Epoch [50/100], Loss: 142.7810\n",
            "Epoch [60/100], Loss: 172.1442\n",
            "Epoch [70/100], Loss: 94.5522\n",
            "Epoch [80/100], Loss: 84.1216\n",
            "Epoch [90/100], Loss: 193.0722\n",
            "Epoch [100/100], Loss: 220.3528\n",
            "Test Loss: 169.9914\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=64\n",
            "Epoch [10/100], Loss: 168.5864\n",
            "Epoch [20/100], Loss: 136.8754\n",
            "Epoch [30/100], Loss: 121.9292\n",
            "Epoch [40/100], Loss: 135.9720\n",
            "Epoch [50/100], Loss: 152.3089\n",
            "Epoch [60/100], Loss: 336.4778\n",
            "Epoch [70/100], Loss: 127.5033\n",
            "Epoch [80/100], Loss: 214.5123\n",
            "Epoch [90/100], Loss: 74.4432\n",
            "Epoch [100/100], Loss: 154.7252\n",
            "Test Loss: 173.1972\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=128\n",
            "Epoch [10/100], Loss: 559.5295\n",
            "Epoch [20/100], Loss: 200.7981\n",
            "Epoch [30/100], Loss: 173.6916\n",
            "Epoch [40/100], Loss: 244.3265\n",
            "Epoch [50/100], Loss: 216.2761\n",
            "Epoch [60/100], Loss: 152.9378\n",
            "Epoch [70/100], Loss: 179.0397\n",
            "Epoch [80/100], Loss: 188.2559\n",
            "Epoch [90/100], Loss: 262.3133\n",
            "Epoch [100/100], Loss: 125.3831\n",
            "Test Loss: 171.9544\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=256\n",
            "Epoch [10/100], Loss: 767.0384\n",
            "Epoch [20/100], Loss: 354.5786\n",
            "Epoch [30/100], Loss: 109.7796\n",
            "Epoch [40/100], Loss: 151.5905\n",
            "Epoch [50/100], Loss: 227.7715\n",
            "Epoch [60/100], Loss: 225.9762\n",
            "Epoch [70/100], Loss: 162.5965\n",
            "Epoch [80/100], Loss: 191.6741\n",
            "Epoch [90/100], Loss: 263.3531\n",
            "Epoch [100/100], Loss: 166.8663\n",
            "Test Loss: 171.6647\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.01, Batch Size=512\n",
            "Epoch [10/100], Loss: 1110.1830\n",
            "Epoch [20/100], Loss: 752.7859\n",
            "Epoch [30/100], Loss: 493.4301\n",
            "Epoch [40/100], Loss: 331.8688\n",
            "Epoch [50/100], Loss: 245.2481\n",
            "Epoch [60/100], Loss: 206.2536\n",
            "Epoch [70/100], Loss: 192.2118\n",
            "Epoch [80/100], Loss: 188.5668\n",
            "Epoch [90/100], Loss: 188.0910\n",
            "Epoch [100/100], Loss: 188.1501\n",
            "Test Loss: 173.0729\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=16\n",
            "Epoch [10/100], Loss: 927.2576\n",
            "Epoch [20/100], Loss: 272.8531\n",
            "Epoch [30/100], Loss: 145.0402\n",
            "Epoch [40/100], Loss: 86.3108\n",
            "Epoch [50/100], Loss: 79.3589\n",
            "Epoch [60/100], Loss: 71.5378\n",
            "Epoch [70/100], Loss: 60.7282\n",
            "Epoch [80/100], Loss: 36.5401\n",
            "Epoch [90/100], Loss: 19.6001\n",
            "Epoch [100/100], Loss: 84.1999\n",
            "Test Loss: 48.3901\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=32\n",
            "Epoch [10/100], Loss: 646.6619\n",
            "Epoch [20/100], Loss: 433.3718\n",
            "Epoch [30/100], Loss: 276.0896\n",
            "Epoch [40/100], Loss: 399.7326\n",
            "Epoch [50/100], Loss: 171.9979\n",
            "Epoch [60/100], Loss: 145.3021\n",
            "Epoch [70/100], Loss: 106.1643\n",
            "Epoch [80/100], Loss: 52.4326\n",
            "Epoch [90/100], Loss: 122.5288\n",
            "Epoch [100/100], Loss: 24.7556\n",
            "Test Loss: 59.0650\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=64\n",
            "Epoch [10/100], Loss: 1042.1759\n",
            "Epoch [20/100], Loss: 911.5893\n",
            "Epoch [30/100], Loss: 531.5064\n",
            "Epoch [40/100], Loss: 398.3615\n",
            "Epoch [50/100], Loss: 495.4078\n",
            "Epoch [60/100], Loss: 294.9383\n",
            "Epoch [70/100], Loss: 303.2588\n",
            "Epoch [80/100], Loss: 123.2794\n",
            "Epoch [90/100], Loss: 238.0119\n",
            "Epoch [100/100], Loss: 110.6299\n",
            "Test Loss: 145.7873\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=128\n",
            "Epoch [10/100], Loss: 1498.3063\n",
            "Epoch [20/100], Loss: 1224.1924\n",
            "Epoch [30/100], Loss: 1101.1506\n",
            "Epoch [40/100], Loss: 890.4577\n",
            "Epoch [50/100], Loss: 814.4710\n",
            "Epoch [60/100], Loss: 824.1283\n",
            "Epoch [70/100], Loss: 536.0912\n",
            "Epoch [80/100], Loss: 468.9592\n",
            "Epoch [90/100], Loss: 588.3801\n",
            "Epoch [100/100], Loss: 371.5337\n",
            "Test Loss: 353.0067\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=256\n",
            "Epoch [10/100], Loss: 1501.7554\n",
            "Epoch [20/100], Loss: 1248.0084\n",
            "Epoch [30/100], Loss: 1166.5944\n",
            "Epoch [40/100], Loss: 1041.6337\n",
            "Epoch [50/100], Loss: 1035.0320\n",
            "Epoch [60/100], Loss: 852.2285\n",
            "Epoch [70/100], Loss: 863.8242\n",
            "Epoch [80/100], Loss: 717.5975\n",
            "Epoch [90/100], Loss: 639.3070\n",
            "Epoch [100/100], Loss: 756.2086\n",
            "Test Loss: 502.9715\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.001, Batch Size=512\n",
            "Epoch [10/100], Loss: 1617.9385\n",
            "Epoch [20/100], Loss: 1524.3456\n",
            "Epoch [30/100], Loss: 1404.1573\n",
            "Epoch [40/100], Loss: 1287.7362\n",
            "Epoch [50/100], Loss: 1194.4197\n",
            "Epoch [60/100], Loss: 1127.8837\n",
            "Epoch [70/100], Loss: 1072.8356\n",
            "Epoch [80/100], Loss: 1024.0312\n",
            "Epoch [90/100], Loss: 979.4255\n",
            "Epoch [100/100], Loss: 938.0063\n",
            "Test Loss: 805.8094\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=16\n",
            "Epoch [10/100], Loss: 1582.5894\n",
            "Epoch [20/100], Loss: 870.0684\n",
            "Epoch [30/100], Loss: 871.7488\n",
            "Epoch [40/100], Loss: 815.6896\n",
            "Epoch [50/100], Loss: 485.1745\n",
            "Epoch [60/100], Loss: 710.6097\n",
            "Epoch [70/100], Loss: 714.7022\n",
            "Epoch [80/100], Loss: 498.6320\n",
            "Epoch [90/100], Loss: 768.9109\n",
            "Epoch [100/100], Loss: 658.7023\n",
            "Test Loss: 478.4440\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=32\n",
            "Epoch [10/100], Loss: 1208.8075\n",
            "Epoch [20/100], Loss: 1206.5470\n",
            "Epoch [30/100], Loss: 1013.8664\n",
            "Epoch [40/100], Loss: 1449.9667\n",
            "Epoch [50/100], Loss: 1310.8938\n",
            "Epoch [60/100], Loss: 1072.5038\n",
            "Epoch [70/100], Loss: 969.0607\n",
            "Epoch [80/100], Loss: 643.4184\n",
            "Epoch [90/100], Loss: 693.1340\n",
            "Epoch [100/100], Loss: 1022.9203\n",
            "Test Loss: 744.5906\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=64\n",
            "Epoch [10/100], Loss: 1740.3785\n",
            "Epoch [20/100], Loss: 1367.5341\n",
            "Epoch [30/100], Loss: 1303.7886\n",
            "Epoch [40/100], Loss: 1300.7666\n",
            "Epoch [50/100], Loss: 1515.4487\n",
            "Epoch [60/100], Loss: 1014.4703\n",
            "Epoch [70/100], Loss: 1025.8062\n",
            "Epoch [80/100], Loss: 1277.4320\n",
            "Epoch [90/100], Loss: 1362.4027\n",
            "Epoch [100/100], Loss: 861.7582\n",
            "Test Loss: 952.1849\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=128\n",
            "Epoch [10/100], Loss: 1658.4929\n",
            "Epoch [20/100], Loss: 1636.8202\n",
            "Epoch [30/100], Loss: 1606.9214\n",
            "Epoch [40/100], Loss: 1751.4484\n",
            "Epoch [50/100], Loss: 1817.8538\n",
            "Epoch [60/100], Loss: 1292.3512\n",
            "Epoch [70/100], Loss: 1470.0820\n",
            "Epoch [80/100], Loss: 1279.4077\n",
            "Epoch [90/100], Loss: 1272.0543\n",
            "Epoch [100/100], Loss: 1311.8512\n",
            "Test Loss: 1178.1223\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=256\n",
            "Epoch [10/100], Loss: 1691.2023\n",
            "Epoch [20/100], Loss: 1673.6373\n",
            "Epoch [30/100], Loss: 1532.7957\n",
            "Epoch [40/100], Loss: 1642.2524\n",
            "Epoch [50/100], Loss: 1450.9879\n",
            "Epoch [60/100], Loss: 1623.3364\n",
            "Epoch [70/100], Loss: 1450.4393\n",
            "Epoch [80/100], Loss: 1383.5988\n",
            "Epoch [90/100], Loss: 1554.7007\n",
            "Epoch [100/100], Loss: 1414.5007\n",
            "Test Loss: 1296.7424\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=100, LR=0.0001, Batch Size=512\n",
            "Epoch [10/100], Loss: 1658.5857\n",
            "Epoch [20/100], Loss: 1653.9689\n",
            "Epoch [30/100], Loss: 1649.1998\n",
            "Epoch [40/100], Loss: 1644.1257\n",
            "Epoch [50/100], Loss: 1638.5822\n",
            "Epoch [60/100], Loss: 1632.4147\n",
            "Epoch [70/100], Loss: 1625.4869\n",
            "Epoch [80/100], Loss: 1617.6847\n",
            "Epoch [90/100], Loss: 1608.9224\n",
            "Epoch [100/100], Loss: 1599.1471\n",
            "Test Loss: 1431.9091\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=16\n",
            "Epoch [10/250], Loss: 120.2438\n",
            "Epoch [20/250], Loss: 185.1568\n",
            "Epoch [30/250], Loss: 86.3470\n",
            "Epoch [40/250], Loss: 108.7239\n",
            "Epoch [50/250], Loss: 174.1531\n",
            "Epoch [60/250], Loss: 998.8467\n",
            "Epoch [70/250], Loss: 290.5087\n",
            "Epoch [80/250], Loss: 434.6318\n",
            "Epoch [90/250], Loss: 144.3329\n",
            "Epoch [100/250], Loss: 377.6679\n",
            "Epoch [110/250], Loss: 588.5388\n",
            "Epoch [120/250], Loss: 761.7231\n",
            "Epoch [130/250], Loss: 150.4222\n",
            "Epoch [140/250], Loss: 133.6917\n",
            "Epoch [150/250], Loss: 149.6187\n",
            "Epoch [160/250], Loss: 1434.9628\n",
            "Epoch [170/250], Loss: 1324.5769\n",
            "Epoch [180/250], Loss: 917.8258\n",
            "Epoch [190/250], Loss: 3847.0312\n",
            "Epoch [200/250], Loss: 100.4408\n",
            "Epoch [210/250], Loss: 191.2946\n",
            "Epoch [220/250], Loss: 230.7341\n",
            "Epoch [230/250], Loss: 513.1019\n",
            "Epoch [240/250], Loss: 40.9586\n",
            "Epoch [250/250], Loss: 1204.7990\n",
            "Test Loss: 102.8115\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=32\n",
            "Epoch [10/250], Loss: 44.4186\n",
            "Epoch [20/250], Loss: 258.3897\n",
            "Epoch [30/250], Loss: 490.8390\n",
            "Epoch [40/250], Loss: 590.4268\n",
            "Epoch [50/250], Loss: 1356.3774\n",
            "Epoch [60/250], Loss: 145.1908\n",
            "Epoch [70/250], Loss: 188.7021\n",
            "Epoch [80/250], Loss: 337.8163\n",
            "Epoch [90/250], Loss: 629.6345\n",
            "Epoch [100/250], Loss: 107.3102\n",
            "Epoch [110/250], Loss: 3452.2991\n",
            "Epoch [120/250], Loss: 205.4851\n",
            "Epoch [130/250], Loss: 709.3080\n",
            "Epoch [140/250], Loss: 644.4194\n",
            "Epoch [150/250], Loss: 342.7368\n",
            "Epoch [160/250], Loss: 170.2945\n",
            "Epoch [170/250], Loss: 1167.8862\n",
            "Epoch [180/250], Loss: 159.2137\n",
            "Epoch [190/250], Loss: 1194.4283\n",
            "Epoch [200/250], Loss: 1324.8522\n",
            "Epoch [210/250], Loss: 544.2565\n",
            "Epoch [220/250], Loss: 1762.9038\n",
            "Epoch [230/250], Loss: 172.0552\n",
            "Epoch [240/250], Loss: 278.5522\n",
            "Epoch [250/250], Loss: 320.1798\n",
            "Test Loss: 231.1452\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=64\n",
            "Epoch [10/250], Loss: 163.2943\n",
            "Epoch [20/250], Loss: 238.4610\n",
            "Epoch [30/250], Loss: 648.7299\n",
            "Epoch [40/250], Loss: 144.2515\n",
            "Epoch [50/250], Loss: 91.2042\n",
            "Epoch [60/250], Loss: 129.2906\n",
            "Epoch [70/250], Loss: 219.6545\n",
            "Epoch [80/250], Loss: 286.5705\n",
            "Epoch [90/250], Loss: 91.5989\n",
            "Epoch [100/250], Loss: 1706.8403\n",
            "Epoch [110/250], Loss: 713.5363\n",
            "Epoch [120/250], Loss: 95.8077\n",
            "Epoch [130/250], Loss: 218.7713\n",
            "Epoch [140/250], Loss: 207.5244\n",
            "Epoch [150/250], Loss: 94.9007\n",
            "Epoch [160/250], Loss: 577.8900\n",
            "Epoch [170/250], Loss: 579.5078\n",
            "Epoch [180/250], Loss: 213.1236\n",
            "Epoch [190/250], Loss: 506.6366\n",
            "Epoch [200/250], Loss: 239.7388\n",
            "Epoch [210/250], Loss: 168.6694\n",
            "Epoch [220/250], Loss: 85.7902\n",
            "Epoch [230/250], Loss: 148.6715\n",
            "Epoch [240/250], Loss: 2033.9122\n",
            "Epoch [250/250], Loss: 723.0291\n",
            "Test Loss: 620.8756\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=128\n",
            "Epoch [10/250], Loss: 827.9064\n",
            "Epoch [20/250], Loss: 279.4571\n",
            "Epoch [30/250], Loss: 231.6325\n",
            "Epoch [40/250], Loss: 219.6904\n",
            "Epoch [50/250], Loss: 239.6750\n",
            "Epoch [60/250], Loss: 127.5005\n",
            "Epoch [70/250], Loss: 158.6158\n",
            "Epoch [80/250], Loss: 169.6247\n",
            "Epoch [90/250], Loss: 125.5123\n",
            "Epoch [100/250], Loss: 122.7883\n",
            "Epoch [110/250], Loss: 199.6341\n",
            "Epoch [120/250], Loss: 261.2613\n",
            "Epoch [130/250], Loss: 155.0534\n",
            "Epoch [140/250], Loss: 141.2671\n",
            "Epoch [150/250], Loss: 171.3446\n",
            "Epoch [160/250], Loss: 114.9101\n",
            "Epoch [170/250], Loss: 215.1523\n",
            "Epoch [180/250], Loss: 139.5864\n",
            "Epoch [190/250], Loss: 155.7985\n",
            "Epoch [200/250], Loss: 275.9348\n",
            "Epoch [210/250], Loss: 234.4651\n",
            "Epoch [220/250], Loss: 133.2101\n",
            "Epoch [230/250], Loss: 164.0349\n",
            "Epoch [240/250], Loss: 157.5697\n",
            "Epoch [250/250], Loss: 257.8487\n",
            "Test Loss: 250.6387\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=256\n",
            "Epoch [10/250], Loss: 15603.6934\n",
            "Epoch [20/250], Loss: 228.4183\n",
            "Epoch [30/250], Loss: 322.9633\n",
            "Epoch [40/250], Loss: 214.2178\n",
            "Epoch [50/250], Loss: 244.3758\n",
            "Epoch [60/250], Loss: 144.7528\n",
            "Epoch [70/250], Loss: 127.8664\n",
            "Epoch [80/250], Loss: 102.9229\n",
            "Epoch [90/250], Loss: 103.1359\n",
            "Epoch [100/250], Loss: 166.9895\n",
            "Epoch [110/250], Loss: 115.4295\n",
            "Epoch [120/250], Loss: 129.3094\n",
            "Epoch [130/250], Loss: 97.0361\n",
            "Epoch [140/250], Loss: 202.9308\n",
            "Epoch [150/250], Loss: 99.8447\n",
            "Epoch [160/250], Loss: 108.4151\n",
            "Epoch [170/250], Loss: 130.8669\n",
            "Epoch [180/250], Loss: 134.6197\n",
            "Epoch [190/250], Loss: 152.1720\n",
            "Epoch [200/250], Loss: 128.7736\n",
            "Epoch [210/250], Loss: 165.1655\n",
            "Epoch [220/250], Loss: 109.9321\n",
            "Epoch [230/250], Loss: 235.9194\n",
            "Epoch [240/250], Loss: 164.6948\n",
            "Epoch [250/250], Loss: 143.0474\n",
            "Test Loss: 143.2992\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=10, Batch Size=512\n",
            "Epoch [10/250], Loss: 49658.6875\n",
            "Epoch [20/250], Loss: 6144.4385\n",
            "Epoch [30/250], Loss: 245.6494\n",
            "Epoch [40/250], Loss: 2259.1162\n",
            "Epoch [50/250], Loss: 238.3367\n",
            "Epoch [60/250], Loss: 439.1521\n",
            "Epoch [70/250], Loss: 160.4074\n",
            "Epoch [80/250], Loss: 115.3366\n",
            "Epoch [90/250], Loss: 114.1882\n",
            "Epoch [100/250], Loss: 114.2004\n",
            "Epoch [110/250], Loss: 114.5417\n",
            "Epoch [120/250], Loss: 114.7392\n",
            "Epoch [130/250], Loss: 114.3882\n",
            "Epoch [140/250], Loss: 114.1662\n",
            "Epoch [150/250], Loss: 114.1974\n",
            "Epoch [160/250], Loss: 114.1667\n",
            "Epoch [170/250], Loss: 114.1702\n",
            "Epoch [180/250], Loss: 114.1661\n",
            "Epoch [190/250], Loss: 114.1661\n",
            "Epoch [200/250], Loss: 114.1661\n",
            "Epoch [210/250], Loss: 114.1660\n",
            "Epoch [220/250], Loss: 114.1659\n",
            "Epoch [230/250], Loss: 114.1659\n",
            "Epoch [240/250], Loss: 114.1659\n",
            "Epoch [250/250], Loss: 114.1659\n",
            "Test Loss: 91.5806\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=16\n",
            "Epoch [10/250], Loss: 40.1237\n",
            "Epoch [20/250], Loss: 119.2446\n",
            "Epoch [30/250], Loss: 136.9974\n",
            "Epoch [40/250], Loss: 149.4766\n",
            "Epoch [50/250], Loss: 84.1333\n",
            "Epoch [60/250], Loss: 40.0464\n",
            "Epoch [70/250], Loss: 108.1937\n",
            "Epoch [80/250], Loss: 91.3522\n",
            "Epoch [90/250], Loss: 183.7479\n",
            "Epoch [100/250], Loss: 51.1174\n",
            "Epoch [110/250], Loss: 99.8321\n",
            "Epoch [120/250], Loss: 237.5013\n",
            "Epoch [130/250], Loss: 146.2048\n",
            "Epoch [140/250], Loss: 65.1203\n",
            "Epoch [150/250], Loss: 303.4695\n",
            "Epoch [160/250], Loss: 44.4050\n",
            "Epoch [170/250], Loss: 83.5082\n",
            "Epoch [180/250], Loss: 125.8219\n",
            "Epoch [190/250], Loss: 210.3214\n",
            "Epoch [200/250], Loss: 44.6961\n",
            "Epoch [210/250], Loss: 115.2549\n",
            "Epoch [220/250], Loss: 70.3131\n",
            "Epoch [230/250], Loss: 34.7397\n",
            "Epoch [240/250], Loss: 216.3793\n",
            "Epoch [250/250], Loss: 64.0218\n",
            "Test Loss: 89.3822\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=32\n",
            "Epoch [10/250], Loss: 147.6683\n",
            "Epoch [20/250], Loss: 71.6044\n",
            "Epoch [30/250], Loss: 36.6016\n",
            "Epoch [40/250], Loss: 116.3245\n",
            "Epoch [50/250], Loss: 60.1941\n",
            "Epoch [60/250], Loss: 216.7374\n",
            "Epoch [70/250], Loss: 35.8684\n",
            "Epoch [80/250], Loss: 95.9305\n",
            "Epoch [90/250], Loss: 61.3576\n",
            "Epoch [100/250], Loss: 95.2786\n",
            "Epoch [110/250], Loss: 118.2443\n",
            "Epoch [120/250], Loss: 250.7254\n",
            "Epoch [130/250], Loss: 64.4019\n",
            "Epoch [140/250], Loss: 105.5477\n",
            "Epoch [150/250], Loss: 716.2680\n",
            "Epoch [160/250], Loss: 31.6515\n",
            "Epoch [170/250], Loss: 69.2740\n",
            "Epoch [180/250], Loss: 119.6903\n",
            "Epoch [190/250], Loss: 81.4335\n",
            "Epoch [200/250], Loss: 129.2953\n",
            "Epoch [210/250], Loss: 55.1820\n",
            "Epoch [220/250], Loss: 84.9218\n",
            "Epoch [230/250], Loss: 75.3056\n",
            "Epoch [240/250], Loss: 54.0796\n",
            "Epoch [250/250], Loss: 34.2771\n",
            "Test Loss: 76.8484\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=64\n",
            "Epoch [10/250], Loss: 94.6668\n",
            "Epoch [20/250], Loss: 95.2651\n",
            "Epoch [30/250], Loss: 44.4844\n",
            "Epoch [40/250], Loss: 43.4622\n",
            "Epoch [50/250], Loss: 66.6534\n",
            "Epoch [60/250], Loss: 123.8221\n",
            "Epoch [70/250], Loss: 44.2302\n",
            "Epoch [80/250], Loss: 168.1354\n",
            "Epoch [90/250], Loss: 109.3139\n",
            "Epoch [100/250], Loss: 243.5504\n",
            "Epoch [110/250], Loss: 72.5659\n",
            "Epoch [120/250], Loss: 47.5350\n",
            "Epoch [130/250], Loss: 93.0276\n",
            "Epoch [140/250], Loss: 144.7381\n",
            "Epoch [150/250], Loss: 50.9670\n",
            "Epoch [160/250], Loss: 84.6208\n",
            "Epoch [170/250], Loss: 729.4608\n",
            "Epoch [180/250], Loss: 152.1196\n",
            "Epoch [190/250], Loss: 98.2043\n",
            "Epoch [200/250], Loss: 97.4413\n",
            "Epoch [210/250], Loss: 58.4510\n",
            "Epoch [220/250], Loss: 41.7631\n",
            "Epoch [230/250], Loss: 67.2304\n",
            "Epoch [240/250], Loss: 16.1671\n",
            "Epoch [250/250], Loss: 82.4586\n",
            "Test Loss: 130.3032\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=128\n",
            "Epoch [10/250], Loss: 186.6541\n",
            "Epoch [20/250], Loss: 172.6764\n",
            "Epoch [30/250], Loss: 96.1835\n",
            "Epoch [40/250], Loss: 79.4207\n",
            "Epoch [50/250], Loss: 69.1951\n",
            "Epoch [60/250], Loss: 74.4527\n",
            "Epoch [70/250], Loss: 68.5599\n",
            "Epoch [80/250], Loss: 91.3481\n",
            "Epoch [90/250], Loss: 74.8761\n",
            "Epoch [100/250], Loss: 147.7835\n",
            "Epoch [110/250], Loss: 52.9204\n",
            "Epoch [120/250], Loss: 75.8485\n",
            "Epoch [130/250], Loss: 75.5352\n",
            "Epoch [140/250], Loss: 73.8301\n",
            "Epoch [150/250], Loss: 71.4961\n",
            "Epoch [160/250], Loss: 59.4230\n",
            "Epoch [170/250], Loss: 159.5388\n",
            "Epoch [180/250], Loss: 143.9332\n",
            "Epoch [190/250], Loss: 55.6480\n",
            "Epoch [200/250], Loss: 164.2766\n",
            "Epoch [210/250], Loss: 106.2941\n",
            "Epoch [220/250], Loss: 75.3565\n",
            "Epoch [230/250], Loss: 65.9629\n",
            "Epoch [240/250], Loss: 58.1670\n",
            "Epoch [250/250], Loss: 105.4238\n",
            "Test Loss: 70.3996\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=256\n",
            "Epoch [10/250], Loss: 775.7715\n",
            "Epoch [20/250], Loss: 247.6644\n",
            "Epoch [30/250], Loss: 74.7610\n",
            "Epoch [40/250], Loss: 99.7009\n",
            "Epoch [50/250], Loss: 62.9944\n",
            "Epoch [60/250], Loss: 110.8895\n",
            "Epoch [70/250], Loss: 53.1961\n",
            "Epoch [80/250], Loss: 50.8432\n",
            "Epoch [90/250], Loss: 76.3460\n",
            "Epoch [100/250], Loss: 76.3070\n",
            "Epoch [110/250], Loss: 123.4623\n",
            "Epoch [120/250], Loss: 72.0853\n",
            "Epoch [130/250], Loss: 85.5007\n",
            "Epoch [140/250], Loss: 89.7512\n",
            "Epoch [150/250], Loss: 135.2350\n",
            "Epoch [160/250], Loss: 59.9559\n",
            "Epoch [170/250], Loss: 68.2323\n",
            "Epoch [180/250], Loss: 83.3921\n",
            "Epoch [190/250], Loss: 54.9464\n",
            "Epoch [200/250], Loss: 81.4111\n",
            "Epoch [210/250], Loss: 113.4287\n",
            "Epoch [220/250], Loss: 57.2215\n",
            "Epoch [230/250], Loss: 52.7758\n",
            "Epoch [240/250], Loss: 81.6114\n",
            "Epoch [250/250], Loss: 58.2440\n",
            "Test Loss: 69.0690\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=1, Batch Size=512\n",
            "Epoch [10/250], Loss: 1207.6180\n",
            "Epoch [20/250], Loss: 378.2987\n",
            "Epoch [30/250], Loss: 260.2139\n",
            "Epoch [40/250], Loss: 139.6125\n",
            "Epoch [50/250], Loss: 112.0520\n",
            "Epoch [60/250], Loss: 101.7723\n",
            "Epoch [70/250], Loss: 97.6607\n",
            "Epoch [80/250], Loss: 90.9315\n",
            "Epoch [90/250], Loss: 89.1833\n",
            "Epoch [100/250], Loss: 94.7574\n",
            "Epoch [110/250], Loss: 90.2950\n",
            "Epoch [120/250], Loss: 93.0278\n",
            "Epoch [130/250], Loss: 91.1779\n",
            "Epoch [140/250], Loss: 88.7624\n",
            "Epoch [150/250], Loss: 87.2729\n",
            "Epoch [160/250], Loss: 86.8101\n",
            "Epoch [170/250], Loss: 86.5691\n",
            "Epoch [180/250], Loss: 89.8116\n",
            "Epoch [190/250], Loss: 91.8967\n",
            "Epoch [200/250], Loss: 88.4302\n",
            "Epoch [210/250], Loss: 88.5156\n",
            "Epoch [220/250], Loss: 89.8875\n",
            "Epoch [230/250], Loss: 85.5232\n",
            "Epoch [240/250], Loss: 83.5762\n",
            "Epoch [250/250], Loss: 78.9069\n",
            "Test Loss: 61.0934\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=16\n",
            "Epoch [10/250], Loss: 137.2130\n",
            "Epoch [20/250], Loss: 76.6167\n",
            "Epoch [30/250], Loss: 61.1985\n",
            "Epoch [40/250], Loss: 145.7979\n",
            "Epoch [50/250], Loss: 39.1530\n",
            "Epoch [60/250], Loss: 63.0972\n",
            "Epoch [70/250], Loss: 145.5351\n",
            "Epoch [80/250], Loss: 28.8098\n",
            "Epoch [90/250], Loss: 53.1058\n",
            "Epoch [100/250], Loss: 77.2783\n",
            "Epoch [110/250], Loss: 134.2620\n",
            "Epoch [120/250], Loss: 131.5905\n",
            "Epoch [130/250], Loss: 85.3227\n",
            "Epoch [140/250], Loss: 195.6980\n",
            "Epoch [150/250], Loss: 75.2262\n",
            "Epoch [160/250], Loss: 31.2281\n",
            "Epoch [170/250], Loss: 24.1412\n",
            "Epoch [180/250], Loss: 93.0978\n",
            "Epoch [190/250], Loss: 73.2736\n",
            "Epoch [200/250], Loss: 87.3845\n",
            "Epoch [210/250], Loss: 42.5191\n",
            "Epoch [220/250], Loss: 171.9027\n",
            "Epoch [230/250], Loss: 236.3443\n",
            "Epoch [240/250], Loss: 43.1971\n",
            "Epoch [250/250], Loss: 275.1713\n",
            "Test Loss: 71.4248\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=32\n",
            "Epoch [10/250], Loss: 60.1732\n",
            "Epoch [20/250], Loss: 86.7479\n",
            "Epoch [30/250], Loss: 36.9794\n",
            "Epoch [40/250], Loss: 47.2062\n",
            "Epoch [50/250], Loss: 50.6849\n",
            "Epoch [60/250], Loss: 80.3655\n",
            "Epoch [70/250], Loss: 135.7359\n",
            "Epoch [80/250], Loss: 76.7181\n",
            "Epoch [90/250], Loss: 23.5989\n",
            "Epoch [100/250], Loss: 158.5226\n",
            "Epoch [110/250], Loss: 47.4823\n",
            "Epoch [120/250], Loss: 41.4284\n",
            "Epoch [130/250], Loss: 27.1698\n",
            "Epoch [140/250], Loss: 39.7543\n",
            "Epoch [150/250], Loss: 23.7725\n",
            "Epoch [160/250], Loss: 154.9208\n",
            "Epoch [170/250], Loss: 25.8284\n",
            "Epoch [180/250], Loss: 44.7160\n",
            "Epoch [190/250], Loss: 99.2065\n",
            "Epoch [200/250], Loss: 12.1484\n",
            "Epoch [210/250], Loss: 85.4844\n",
            "Epoch [220/250], Loss: 90.5295\n",
            "Epoch [230/250], Loss: 208.5993\n",
            "Epoch [240/250], Loss: 40.1020\n",
            "Epoch [250/250], Loss: 57.8904\n",
            "Test Loss: 59.1172\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=64\n",
            "Epoch [10/250], Loss: 64.3566\n",
            "Epoch [20/250], Loss: 78.8326\n",
            "Epoch [30/250], Loss: 41.4461\n",
            "Epoch [40/250], Loss: 41.5891\n",
            "Epoch [50/250], Loss: 57.4033\n",
            "Epoch [60/250], Loss: 31.6193\n",
            "Epoch [70/250], Loss: 33.5686\n",
            "Epoch [80/250], Loss: 37.5592\n",
            "Epoch [90/250], Loss: 23.1731\n",
            "Epoch [100/250], Loss: 57.1655\n",
            "Epoch [110/250], Loss: 36.5277\n",
            "Epoch [120/250], Loss: 48.5017\n",
            "Epoch [130/250], Loss: 12.9768\n",
            "Epoch [140/250], Loss: 63.6436\n",
            "Epoch [150/250], Loss: 61.0794\n",
            "Epoch [160/250], Loss: 55.5182\n",
            "Epoch [170/250], Loss: 209.0960\n",
            "Epoch [180/250], Loss: 171.0042\n",
            "Epoch [190/250], Loss: 91.7563\n",
            "Epoch [200/250], Loss: 162.2011\n",
            "Epoch [210/250], Loss: 157.4835\n",
            "Epoch [220/250], Loss: 57.6763\n",
            "Epoch [230/250], Loss: 161.8036\n",
            "Epoch [240/250], Loss: 18.1762\n",
            "Epoch [250/250], Loss: 30.6445\n",
            "Test Loss: 57.2769\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=128\n",
            "Epoch [10/250], Loss: 76.3176\n",
            "Epoch [20/250], Loss: 56.9763\n",
            "Epoch [30/250], Loss: 40.2739\n",
            "Epoch [40/250], Loss: 55.3695\n",
            "Epoch [50/250], Loss: 46.2544\n",
            "Epoch [60/250], Loss: 116.7136\n",
            "Epoch [70/250], Loss: 37.4730\n",
            "Epoch [80/250], Loss: 52.1742\n",
            "Epoch [90/250], Loss: 54.2816\n",
            "Epoch [100/250], Loss: 44.7977\n",
            "Epoch [110/250], Loss: 63.1867\n",
            "Epoch [120/250], Loss: 37.3220\n",
            "Epoch [130/250], Loss: 120.8473\n",
            "Epoch [140/250], Loss: 41.0763\n",
            "Epoch [150/250], Loss: 44.1552\n",
            "Epoch [160/250], Loss: 51.0777\n",
            "Epoch [170/250], Loss: 59.9841\n",
            "Epoch [180/250], Loss: 43.7319\n",
            "Epoch [190/250], Loss: 76.7425\n",
            "Epoch [200/250], Loss: 119.3485\n",
            "Epoch [210/250], Loss: 58.4702\n",
            "Epoch [220/250], Loss: 45.1575\n",
            "Epoch [230/250], Loss: 79.8165\n",
            "Epoch [240/250], Loss: 57.1608\n",
            "Epoch [250/250], Loss: 70.1874\n",
            "Test Loss: 55.2899\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=256\n",
            "Epoch [10/250], Loss: 67.8686\n",
            "Epoch [20/250], Loss: 129.5758\n",
            "Epoch [30/250], Loss: 78.3115\n",
            "Epoch [40/250], Loss: 54.1497\n",
            "Epoch [50/250], Loss: 39.7362\n",
            "Epoch [60/250], Loss: 68.1390\n",
            "Epoch [70/250], Loss: 45.1512\n",
            "Epoch [80/250], Loss: 95.8462\n",
            "Epoch [90/250], Loss: 31.9812\n",
            "Epoch [100/250], Loss: 59.2150\n",
            "Epoch [110/250], Loss: 31.6431\n",
            "Epoch [120/250], Loss: 36.6020\n",
            "Epoch [130/250], Loss: 118.9384\n",
            "Epoch [140/250], Loss: 35.1476\n",
            "Epoch [150/250], Loss: 46.5855\n",
            "Epoch [160/250], Loss: 114.3631\n",
            "Epoch [170/250], Loss: 55.5553\n",
            "Epoch [180/250], Loss: 114.9961\n",
            "Epoch [190/250], Loss: 143.3992\n",
            "Epoch [200/250], Loss: 51.9615\n",
            "Epoch [210/250], Loss: 29.1746\n",
            "Epoch [220/250], Loss: 43.4891\n",
            "Epoch [230/250], Loss: 59.5064\n",
            "Epoch [240/250], Loss: 100.4491\n",
            "Epoch [250/250], Loss: 120.0492\n",
            "Test Loss: 47.6974\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.1, Batch Size=512\n",
            "Epoch [10/250], Loss: 166.0400\n",
            "Epoch [20/250], Loss: 131.9248\n",
            "Epoch [30/250], Loss: 98.2044\n",
            "Epoch [40/250], Loss: 80.6351\n",
            "Epoch [50/250], Loss: 72.7036\n",
            "Epoch [60/250], Loss: 69.7419\n",
            "Epoch [70/250], Loss: 67.1368\n",
            "Epoch [80/250], Loss: 65.9094\n",
            "Epoch [90/250], Loss: 64.7462\n",
            "Epoch [100/250], Loss: 64.1475\n",
            "Epoch [110/250], Loss: 63.8271\n",
            "Epoch [120/250], Loss: 62.8561\n",
            "Epoch [130/250], Loss: 62.1167\n",
            "Epoch [140/250], Loss: 62.3403\n",
            "Epoch [150/250], Loss: 61.4009\n",
            "Epoch [160/250], Loss: 61.1184\n",
            "Epoch [170/250], Loss: 60.0478\n",
            "Epoch [180/250], Loss: 59.7356\n",
            "Epoch [190/250], Loss: 63.2189\n",
            "Epoch [200/250], Loss: 63.1500\n",
            "Epoch [210/250], Loss: 63.5726\n",
            "Epoch [220/250], Loss: 60.4303\n",
            "Epoch [230/250], Loss: 59.1087\n",
            "Epoch [240/250], Loss: 58.7862\n",
            "Epoch [250/250], Loss: 55.9722\n",
            "Test Loss: 47.2998\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=16\n",
            "Epoch [10/250], Loss: 96.0516\n",
            "Epoch [20/250], Loss: 404.2613\n",
            "Epoch [30/250], Loss: 281.4802\n",
            "Epoch [40/250], Loss: 119.5208\n",
            "Epoch [50/250], Loss: 173.2284\n",
            "Epoch [60/250], Loss: 121.5161\n",
            "Epoch [70/250], Loss: 209.1043\n",
            "Epoch [80/250], Loss: 143.3104\n",
            "Epoch [90/250], Loss: 115.0915\n",
            "Epoch [100/250], Loss: 146.2921\n",
            "Epoch [110/250], Loss: 173.7237\n",
            "Epoch [120/250], Loss: 178.4995\n",
            "Epoch [130/250], Loss: 166.1980\n",
            "Epoch [140/250], Loss: 40.1792\n",
            "Epoch [150/250], Loss: 16.7440\n",
            "Epoch [160/250], Loss: 49.2475\n",
            "Epoch [170/250], Loss: 78.6113\n",
            "Epoch [180/250], Loss: 24.5081\n",
            "Epoch [190/250], Loss: 134.0872\n",
            "Epoch [200/250], Loss: 116.5186\n",
            "Epoch [210/250], Loss: 68.3214\n",
            "Epoch [220/250], Loss: 102.4709\n",
            "Epoch [230/250], Loss: 17.9491\n",
            "Epoch [240/250], Loss: 80.5181\n",
            "Epoch [250/250], Loss: 26.9053\n",
            "Test Loss: 40.2456\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=32\n",
            "Epoch [10/250], Loss: 153.1374\n",
            "Epoch [20/250], Loss: 56.0565\n",
            "Epoch [30/250], Loss: 205.8493\n",
            "Epoch [40/250], Loss: 669.4921\n",
            "Epoch [50/250], Loss: 168.1805\n",
            "Epoch [60/250], Loss: 195.0083\n",
            "Epoch [70/250], Loss: 225.0498\n",
            "Epoch [80/250], Loss: 86.9559\n",
            "Epoch [90/250], Loss: 110.5764\n",
            "Epoch [100/250], Loss: 91.2652\n",
            "Epoch [110/250], Loss: 678.7955\n",
            "Epoch [120/250], Loss: 171.3282\n",
            "Epoch [130/250], Loss: 103.7941\n",
            "Epoch [140/250], Loss: 165.6124\n",
            "Epoch [150/250], Loss: 257.7839\n",
            "Epoch [160/250], Loss: 204.1186\n",
            "Epoch [170/250], Loss: 154.2014\n",
            "Epoch [180/250], Loss: 185.7982\n",
            "Epoch [190/250], Loss: 127.9120\n",
            "Epoch [200/250], Loss: 136.5307\n",
            "Epoch [210/250], Loss: 114.8647\n",
            "Epoch [220/250], Loss: 55.4676\n",
            "Epoch [230/250], Loss: 284.2354\n",
            "Epoch [240/250], Loss: 154.5106\n",
            "Epoch [250/250], Loss: 71.3148\n",
            "Test Loss: 171.8143\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=64\n",
            "Epoch [10/250], Loss: 106.4034\n",
            "Epoch [20/250], Loss: 105.4689\n",
            "Epoch [30/250], Loss: 36.7827\n",
            "Epoch [40/250], Loss: 176.0386\n",
            "Epoch [50/250], Loss: 75.4646\n",
            "Epoch [60/250], Loss: 201.4139\n",
            "Epoch [70/250], Loss: 85.3178\n",
            "Epoch [80/250], Loss: 307.7271\n",
            "Epoch [90/250], Loss: 57.5370\n",
            "Epoch [100/250], Loss: 100.5791\n",
            "Epoch [110/250], Loss: 294.7412\n",
            "Epoch [120/250], Loss: 116.6239\n",
            "Epoch [130/250], Loss: 185.3635\n",
            "Epoch [140/250], Loss: 148.4961\n",
            "Epoch [150/250], Loss: 179.8129\n",
            "Epoch [160/250], Loss: 171.5365\n",
            "Epoch [170/250], Loss: 139.3146\n",
            "Epoch [180/250], Loss: 188.0447\n",
            "Epoch [190/250], Loss: 151.5932\n",
            "Epoch [200/250], Loss: 213.4510\n",
            "Epoch [210/250], Loss: 27.2654\n",
            "Epoch [220/250], Loss: 181.3726\n",
            "Epoch [230/250], Loss: 295.8593\n",
            "Epoch [240/250], Loss: 204.5130\n",
            "Epoch [250/250], Loss: 209.4653\n",
            "Test Loss: 170.2371\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=128\n",
            "Epoch [10/250], Loss: 496.2780\n",
            "Epoch [20/250], Loss: 168.2619\n",
            "Epoch [30/250], Loss: 258.0038\n",
            "Epoch [40/250], Loss: 158.6806\n",
            "Epoch [50/250], Loss: 164.6222\n",
            "Epoch [60/250], Loss: 164.7204\n",
            "Epoch [70/250], Loss: 183.8589\n",
            "Epoch [80/250], Loss: 151.3376\n",
            "Epoch [90/250], Loss: 179.3912\n",
            "Epoch [100/250], Loss: 126.3842\n",
            "Epoch [110/250], Loss: 205.7067\n",
            "Epoch [120/250], Loss: 180.6682\n",
            "Epoch [130/250], Loss: 196.2092\n",
            "Epoch [140/250], Loss: 177.1395\n",
            "Epoch [150/250], Loss: 228.1089\n",
            "Epoch [160/250], Loss: 265.0296\n",
            "Epoch [170/250], Loss: 204.0816\n",
            "Epoch [180/250], Loss: 165.8124\n",
            "Epoch [190/250], Loss: 193.0996\n",
            "Epoch [200/250], Loss: 146.3259\n",
            "Epoch [210/250], Loss: 168.0845\n",
            "Epoch [220/250], Loss: 144.5154\n",
            "Epoch [230/250], Loss: 206.5583\n",
            "Epoch [240/250], Loss: 308.1852\n",
            "Epoch [250/250], Loss: 172.1622\n",
            "Test Loss: 171.9269\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=256\n",
            "Epoch [10/250], Loss: 683.5087\n",
            "Epoch [20/250], Loss: 243.2545\n",
            "Epoch [30/250], Loss: 168.0239\n",
            "Epoch [40/250], Loss: 255.2952\n",
            "Epoch [50/250], Loss: 234.2263\n",
            "Epoch [60/250], Loss: 147.8988\n",
            "Epoch [70/250], Loss: 130.5676\n",
            "Epoch [80/250], Loss: 178.1068\n",
            "Epoch [90/250], Loss: 171.0538\n",
            "Epoch [100/250], Loss: 135.8274\n",
            "Epoch [110/250], Loss: 226.4367\n",
            "Epoch [120/250], Loss: 254.2307\n",
            "Epoch [130/250], Loss: 156.8671\n",
            "Epoch [140/250], Loss: 159.9649\n",
            "Epoch [150/250], Loss: 155.4635\n",
            "Epoch [160/250], Loss: 160.5138\n",
            "Epoch [170/250], Loss: 237.3957\n",
            "Epoch [180/250], Loss: 240.5901\n",
            "Epoch [190/250], Loss: 282.6279\n",
            "Epoch [200/250], Loss: 245.8900\n",
            "Epoch [210/250], Loss: 157.6119\n",
            "Epoch [220/250], Loss: 170.4365\n",
            "Epoch [230/250], Loss: 145.0979\n",
            "Epoch [240/250], Loss: 148.6069\n",
            "Epoch [250/250], Loss: 212.7936\n",
            "Test Loss: 172.6944\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.01, Batch Size=512\n",
            "Epoch [10/250], Loss: 1104.2528\n",
            "Epoch [20/250], Loss: 747.1755\n",
            "Epoch [30/250], Loss: 488.7513\n",
            "Epoch [40/250], Loss: 328.5043\n",
            "Epoch [50/250], Loss: 243.2174\n",
            "Epoch [60/250], Loss: 205.2599\n",
            "Epoch [70/250], Loss: 191.8442\n",
            "Epoch [80/250], Loss: 188.4828\n",
            "Epoch [90/250], Loss: 188.0925\n",
            "Epoch [100/250], Loss: 188.1598\n",
            "Epoch [110/250], Loss: 188.1636\n",
            "Epoch [120/250], Loss: 188.1265\n",
            "Epoch [130/250], Loss: 188.1007\n",
            "Epoch [140/250], Loss: 188.0923\n",
            "Epoch [150/250], Loss: 188.0913\n",
            "Epoch [160/250], Loss: 188.0915\n",
            "Epoch [170/250], Loss: 188.0914\n",
            "Epoch [180/250], Loss: 188.0913\n",
            "Epoch [190/250], Loss: 188.0912\n",
            "Epoch [200/250], Loss: 188.0912\n",
            "Epoch [210/250], Loss: 188.0911\n",
            "Epoch [220/250], Loss: 188.0911\n",
            "Epoch [230/250], Loss: 188.0911\n",
            "Epoch [240/250], Loss: 188.0911\n",
            "Epoch [250/250], Loss: 188.0911\n",
            "Test Loss: 171.9695\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=16\n",
            "Epoch [10/250], Loss: 435.7357\n",
            "Epoch [20/250], Loss: 85.9036\n",
            "Epoch [30/250], Loss: 134.4049\n",
            "Epoch [40/250], Loss: 108.8068\n",
            "Epoch [50/250], Loss: 42.3865\n",
            "Epoch [60/250], Loss: 137.8786\n",
            "Epoch [70/250], Loss: 54.6391\n",
            "Epoch [80/250], Loss: 94.8662\n",
            "Epoch [90/250], Loss: 43.4640\n",
            "Epoch [100/250], Loss: 19.6382\n",
            "Epoch [110/250], Loss: 93.9926\n",
            "Epoch [120/250], Loss: 53.6720\n",
            "Epoch [130/250], Loss: 11.2848\n",
            "Epoch [140/250], Loss: 86.5328\n",
            "Epoch [150/250], Loss: 30.7396\n",
            "Epoch [160/250], Loss: 24.8994\n",
            "Epoch [170/250], Loss: 26.6896\n",
            "Epoch [180/250], Loss: 67.5728\n",
            "Epoch [190/250], Loss: 17.5276\n",
            "Epoch [200/250], Loss: 45.2689\n",
            "Epoch [210/250], Loss: 15.2562\n",
            "Epoch [220/250], Loss: 18.9191\n",
            "Epoch [230/250], Loss: 20.1927\n",
            "Epoch [240/250], Loss: 40.2599\n",
            "Epoch [250/250], Loss: 40.6540\n",
            "Test Loss: 35.5571\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=32\n",
            "Epoch [10/250], Loss: 1228.9177\n",
            "Epoch [20/250], Loss: 526.1160\n",
            "Epoch [30/250], Loss: 364.6181\n",
            "Epoch [40/250], Loss: 164.1478\n",
            "Epoch [50/250], Loss: 95.1852\n",
            "Epoch [60/250], Loss: 102.3203\n",
            "Epoch [70/250], Loss: 405.4674\n",
            "Epoch [80/250], Loss: 33.7142\n",
            "Epoch [90/250], Loss: 58.4916\n",
            "Epoch [100/250], Loss: 22.7021\n",
            "Epoch [110/250], Loss: 32.6569\n",
            "Epoch [120/250], Loss: 45.3674\n",
            "Epoch [130/250], Loss: 156.3245\n",
            "Epoch [140/250], Loss: 128.8579\n",
            "Epoch [150/250], Loss: 23.5306\n",
            "Epoch [160/250], Loss: 15.5953\n",
            "Epoch [170/250], Loss: 47.3520\n",
            "Epoch [180/250], Loss: 446.0969\n",
            "Epoch [190/250], Loss: 26.3731\n",
            "Epoch [200/250], Loss: 22.5828\n",
            "Epoch [210/250], Loss: 14.2586\n",
            "Epoch [220/250], Loss: 6.3416\n",
            "Epoch [230/250], Loss: 13.1403\n",
            "Epoch [240/250], Loss: 7.8331\n",
            "Epoch [250/250], Loss: 15.2633\n",
            "Test Loss: 35.0809\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=64\n",
            "Epoch [10/250], Loss: 2064.6699\n",
            "Epoch [20/250], Loss: 513.9250\n",
            "Epoch [30/250], Loss: 735.3414\n",
            "Epoch [40/250], Loss: 663.1828\n",
            "Epoch [50/250], Loss: 553.4599\n",
            "Epoch [60/250], Loss: 360.7873\n",
            "Epoch [70/250], Loss: 194.4796\n",
            "Epoch [80/250], Loss: 139.5804\n",
            "Epoch [90/250], Loss: 761.2042\n",
            "Epoch [100/250], Loss: 215.7195\n",
            "Epoch [110/250], Loss: 114.4330\n",
            "Epoch [120/250], Loss: 201.2163\n",
            "Epoch [130/250], Loss: 317.2600\n",
            "Epoch [140/250], Loss: 62.5971\n",
            "Epoch [150/250], Loss: 67.8462\n",
            "Epoch [160/250], Loss: 59.0499\n",
            "Epoch [170/250], Loss: 32.5159\n",
            "Epoch [180/250], Loss: 33.7237\n",
            "Epoch [190/250], Loss: 30.8797\n",
            "Epoch [200/250], Loss: 17.0610\n",
            "Epoch [210/250], Loss: 24.3923\n",
            "Epoch [220/250], Loss: 56.3975\n",
            "Epoch [230/250], Loss: 58.9371\n",
            "Epoch [240/250], Loss: 26.0686\n",
            "Epoch [250/250], Loss: 78.9005\n",
            "Test Loss: 46.8369\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1269.5746\n",
            "Epoch [20/250], Loss: 1199.9846\n",
            "Epoch [30/250], Loss: 1005.8367\n",
            "Epoch [40/250], Loss: 706.7886\n",
            "Epoch [50/250], Loss: 772.0707\n",
            "Epoch [60/250], Loss: 757.3304\n",
            "Epoch [70/250], Loss: 574.2352\n",
            "Epoch [80/250], Loss: 618.4327\n",
            "Epoch [90/250], Loss: 399.8725\n",
            "Epoch [100/250], Loss: 457.5026\n",
            "Epoch [110/250], Loss: 494.2597\n",
            "Epoch [120/250], Loss: 445.3973\n",
            "Epoch [130/250], Loss: 368.1596\n",
            "Epoch [140/250], Loss: 285.8768\n",
            "Epoch [150/250], Loss: 227.2972\n",
            "Epoch [160/250], Loss: 264.4535\n",
            "Epoch [170/250], Loss: 178.4497\n",
            "Epoch [180/250], Loss: 349.6732\n",
            "Epoch [190/250], Loss: 181.7246\n",
            "Epoch [200/250], Loss: 288.2454\n",
            "Epoch [210/250], Loss: 224.5445\n",
            "Epoch [220/250], Loss: 164.6605\n",
            "Epoch [230/250], Loss: 146.7870\n",
            "Epoch [240/250], Loss: 159.9518\n",
            "Epoch [250/250], Loss: 114.9258\n",
            "Test Loss: 97.2968\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1564.3759\n",
            "Epoch [20/250], Loss: 1279.6353\n",
            "Epoch [30/250], Loss: 1023.2559\n",
            "Epoch [40/250], Loss: 1023.8987\n",
            "Epoch [50/250], Loss: 927.7683\n",
            "Epoch [60/250], Loss: 797.6287\n",
            "Epoch [70/250], Loss: 976.0314\n",
            "Epoch [80/250], Loss: 569.1729\n",
            "Epoch [90/250], Loss: 718.3741\n",
            "Epoch [100/250], Loss: 578.0364\n",
            "Epoch [110/250], Loss: 589.1454\n",
            "Epoch [120/250], Loss: 527.5479\n",
            "Epoch [130/250], Loss: 453.1893\n",
            "Epoch [140/250], Loss: 574.5002\n",
            "Epoch [150/250], Loss: 401.7529\n",
            "Epoch [160/250], Loss: 415.4309\n",
            "Epoch [170/250], Loss: 304.7946\n",
            "Epoch [180/250], Loss: 379.8211\n",
            "Epoch [190/250], Loss: 377.9126\n",
            "Epoch [200/250], Loss: 363.6906\n",
            "Epoch [210/250], Loss: 242.1613\n",
            "Epoch [220/250], Loss: 256.1337\n",
            "Epoch [230/250], Loss: 209.6253\n",
            "Epoch [240/250], Loss: 272.0239\n",
            "Epoch [250/250], Loss: 225.0259\n",
            "Test Loss: 172.7814\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1619.0421\n",
            "Epoch [20/250], Loss: 1532.6768\n",
            "Epoch [30/250], Loss: 1400.5698\n",
            "Epoch [40/250], Loss: 1273.3137\n",
            "Epoch [50/250], Loss: 1180.0946\n",
            "Epoch [60/250], Loss: 1114.2056\n",
            "Epoch [70/250], Loss: 1060.0605\n",
            "Epoch [80/250], Loss: 1011.9183\n",
            "Epoch [90/250], Loss: 967.8129\n",
            "Epoch [100/250], Loss: 926.8177\n",
            "Epoch [110/250], Loss: 888.3898\n",
            "Epoch [120/250], Loss: 852.1709\n",
            "Epoch [130/250], Loss: 817.9066\n",
            "Epoch [140/250], Loss: 785.4061\n",
            "Epoch [150/250], Loss: 754.5204\n",
            "Epoch [160/250], Loss: 725.1298\n",
            "Epoch [170/250], Loss: 697.1348\n",
            "Epoch [180/250], Loss: 670.4512\n",
            "Epoch [190/250], Loss: 645.0059\n",
            "Epoch [200/250], Loss: 620.7347\n",
            "Epoch [210/250], Loss: 597.5803\n",
            "Epoch [220/250], Loss: 575.4904\n",
            "Epoch [230/250], Loss: 554.4172\n",
            "Epoch [240/250], Loss: 534.3165\n",
            "Epoch [250/250], Loss: 515.1462\n",
            "Test Loss: 423.0833\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=16\n",
            "Epoch [10/250], Loss: 1615.2640\n",
            "Epoch [20/250], Loss: 794.5770\n",
            "Epoch [30/250], Loss: 689.2637\n",
            "Epoch [40/250], Loss: 921.6060\n",
            "Epoch [50/250], Loss: 668.3251\n",
            "Epoch [60/250], Loss: 758.3033\n",
            "Epoch [70/250], Loss: 737.3193\n",
            "Epoch [80/250], Loss: 837.8641\n",
            "Epoch [90/250], Loss: 1109.6584\n",
            "Epoch [100/250], Loss: 736.5972\n",
            "Epoch [110/250], Loss: 331.6903\n",
            "Epoch [120/250], Loss: 423.3697\n",
            "Epoch [130/250], Loss: 414.3075\n",
            "Epoch [140/250], Loss: 436.0932\n",
            "Epoch [150/250], Loss: 213.1127\n",
            "Epoch [160/250], Loss: 371.9511\n",
            "Epoch [170/250], Loss: 438.6521\n",
            "Epoch [180/250], Loss: 220.6240\n",
            "Epoch [190/250], Loss: 410.2422\n",
            "Epoch [200/250], Loss: 292.0470\n",
            "Epoch [210/250], Loss: 212.7281\n",
            "Epoch [220/250], Loss: 119.9273\n",
            "Epoch [230/250], Loss: 123.9900\n",
            "Epoch [240/250], Loss: 185.4160\n",
            "Epoch [250/250], Loss: 164.1240\n",
            "Test Loss: 145.3017\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=32\n",
            "Epoch [10/250], Loss: 1241.0684\n",
            "Epoch [20/250], Loss: 1026.3529\n",
            "Epoch [30/250], Loss: 2040.5898\n",
            "Epoch [40/250], Loss: 932.6063\n",
            "Epoch [50/250], Loss: 1210.3060\n",
            "Epoch [60/250], Loss: 2345.9875\n",
            "Epoch [70/250], Loss: 972.2612\n",
            "Epoch [80/250], Loss: 1111.6387\n",
            "Epoch [90/250], Loss: 697.6295\n",
            "Epoch [100/250], Loss: 939.9664\n",
            "Epoch [110/250], Loss: 548.3559\n",
            "Epoch [120/250], Loss: 614.1896\n",
            "Epoch [130/250], Loss: 593.1213\n",
            "Epoch [140/250], Loss: 870.1464\n",
            "Epoch [150/250], Loss: 541.8410\n",
            "Epoch [160/250], Loss: 655.0112\n",
            "Epoch [170/250], Loss: 529.7490\n",
            "Epoch [180/250], Loss: 542.4274\n",
            "Epoch [190/250], Loss: 592.2774\n",
            "Epoch [200/250], Loss: 663.8686\n",
            "Epoch [210/250], Loss: 506.3820\n",
            "Epoch [220/250], Loss: 354.9899\n",
            "Epoch [230/250], Loss: 217.3444\n",
            "Epoch [240/250], Loss: 475.7532\n",
            "Epoch [250/250], Loss: 206.3225\n",
            "Test Loss: 364.8672\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=64\n",
            "Epoch [10/250], Loss: 2035.3241\n",
            "Epoch [20/250], Loss: 1799.0419\n",
            "Epoch [30/250], Loss: 1747.8398\n",
            "Epoch [40/250], Loss: 1217.0385\n",
            "Epoch [50/250], Loss: 1370.2190\n",
            "Epoch [60/250], Loss: 1455.6276\n",
            "Epoch [70/250], Loss: 1230.6255\n",
            "Epoch [80/250], Loss: 1375.7101\n",
            "Epoch [90/250], Loss: 565.0469\n",
            "Epoch [100/250], Loss: 804.9277\n",
            "Epoch [110/250], Loss: 1336.0714\n",
            "Epoch [120/250], Loss: 1353.0831\n",
            "Epoch [130/250], Loss: 771.3063\n",
            "Epoch [140/250], Loss: 789.5970\n",
            "Epoch [150/250], Loss: 952.7597\n",
            "Epoch [160/250], Loss: 919.3183\n",
            "Epoch [170/250], Loss: 1513.1973\n",
            "Epoch [180/250], Loss: 652.1354\n",
            "Epoch [190/250], Loss: 725.5958\n",
            "Epoch [200/250], Loss: 1175.1215\n",
            "Epoch [210/250], Loss: 689.3758\n",
            "Epoch [220/250], Loss: 391.4313\n",
            "Epoch [230/250], Loss: 571.2465\n",
            "Epoch [240/250], Loss: 774.5980\n",
            "Epoch [250/250], Loss: 529.8582\n",
            "Test Loss: 615.1743\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=128\n",
            "Epoch [10/250], Loss: 1674.3496\n",
            "Epoch [20/250], Loss: 1903.6545\n",
            "Epoch [30/250], Loss: 1471.4629\n",
            "Epoch [40/250], Loss: 1559.9147\n",
            "Epoch [50/250], Loss: 1476.3756\n",
            "Epoch [60/250], Loss: 1449.3073\n",
            "Epoch [70/250], Loss: 1649.2340\n",
            "Epoch [80/250], Loss: 1245.3761\n",
            "Epoch [90/250], Loss: 1424.1482\n",
            "Epoch [100/250], Loss: 1261.9751\n",
            "Epoch [110/250], Loss: 1342.8466\n",
            "Epoch [120/250], Loss: 1136.2723\n",
            "Epoch [130/250], Loss: 1232.7217\n",
            "Epoch [140/250], Loss: 1171.0088\n",
            "Epoch [150/250], Loss: 1072.4994\n",
            "Epoch [160/250], Loss: 1349.4105\n",
            "Epoch [170/250], Loss: 1355.7651\n",
            "Epoch [180/250], Loss: 1113.1979\n",
            "Epoch [190/250], Loss: 1179.5040\n",
            "Epoch [200/250], Loss: 1038.3175\n",
            "Epoch [210/250], Loss: 1118.0624\n",
            "Epoch [220/250], Loss: 1104.6512\n",
            "Epoch [230/250], Loss: 1076.6931\n",
            "Epoch [240/250], Loss: 984.2051\n",
            "Epoch [250/250], Loss: 1013.4096\n",
            "Test Loss: 879.2568\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=256\n",
            "Epoch [10/250], Loss: 1591.2161\n",
            "Epoch [20/250], Loss: 1591.6414\n",
            "Epoch [30/250], Loss: 1722.2841\n",
            "Epoch [40/250], Loss: 1669.0708\n",
            "Epoch [50/250], Loss: 1965.5621\n",
            "Epoch [60/250], Loss: 1702.1748\n",
            "Epoch [70/250], Loss: 1617.7550\n",
            "Epoch [80/250], Loss: 1609.2083\n",
            "Epoch [90/250], Loss: 1320.1271\n",
            "Epoch [100/250], Loss: 1469.8450\n",
            "Epoch [110/250], Loss: 1622.4125\n",
            "Epoch [120/250], Loss: 1398.8959\n",
            "Epoch [130/250], Loss: 1411.3214\n",
            "Epoch [140/250], Loss: 1265.0145\n",
            "Epoch [150/250], Loss: 1306.3596\n",
            "Epoch [160/250], Loss: 1325.8079\n",
            "Epoch [170/250], Loss: 1070.5496\n",
            "Epoch [180/250], Loss: 1083.6425\n",
            "Epoch [190/250], Loss: 1221.3992\n",
            "Epoch [200/250], Loss: 1196.9481\n",
            "Epoch [210/250], Loss: 1259.0919\n",
            "Epoch [220/250], Loss: 1297.5438\n",
            "Epoch [230/250], Loss: 1169.5931\n",
            "Epoch [240/250], Loss: 978.4428\n",
            "Epoch [250/250], Loss: 1038.4828\n",
            "Test Loss: 964.1462\n",
            "Testing: Hidden Layers=3, Neurons=64, Activation=Tanh, Epochs=250, LR=0.0001, Batch Size=512\n",
            "Epoch [10/250], Loss: 1666.5510\n",
            "Epoch [20/250], Loss: 1661.0836\n",
            "Epoch [30/250], Loss: 1655.3522\n",
            "Epoch [40/250], Loss: 1649.1731\n",
            "Epoch [50/250], Loss: 1642.3695\n",
            "Epoch [60/250], Loss: 1634.7863\n",
            "Epoch [70/250], Loss: 1626.3015\n",
            "Epoch [80/250], Loss: 1616.8246\n",
            "Epoch [90/250], Loss: 1606.2915\n",
            "Epoch [100/250], Loss: 1594.6630\n",
            "Epoch [110/250], Loss: 1581.9307\n",
            "Epoch [120/250], Loss: 1568.1259\n",
            "Epoch [130/250], Loss: 1553.3241\n",
            "Epoch [140/250], Loss: 1537.6445\n",
            "Epoch [150/250], Loss: 1521.2397\n",
            "Epoch [160/250], Loss: 1504.2838\n",
            "Epoch [170/250], Loss: 1486.9578\n",
            "Epoch [180/250], Loss: 1469.4353\n",
            "Epoch [190/250], Loss: 1451.8698\n",
            "Epoch [200/250], Loss: 1434.3877\n",
            "Epoch [210/250], Loss: 1417.0863\n",
            "Epoch [220/250], Loss: 1400.0367\n",
            "Epoch [230/250], Loss: 1383.2863\n",
            "Epoch [240/250], Loss: 1366.8663\n",
            "Epoch [250/250], Loss: 1350.7999\n",
            "Test Loss: 1204.0265\n",
            "       Hidden Layers  Neurons Per Layer Activation Function  Epochs  \\\n",
            "0                  1                  4              Linear       1   \n",
            "1                  1                  4              Linear       1   \n",
            "2                  1                  4              Linear       1   \n",
            "3                  1                  4              Linear       1   \n",
            "4                  1                  4              Linear       1   \n",
            "...              ...                ...                 ...     ...   \n",
            "16195              3                 64                Tanh     250   \n",
            "16196              3                 64                Tanh     250   \n",
            "16197              3                 64                Tanh     250   \n",
            "16198              3                 64                Tanh     250   \n",
            "16199              3                 64                Tanh     250   \n",
            "\n",
            "       Learning Rate  Batch Size     Test Loss  \n",
            "0            10.0000          16  8.189186e+05  \n",
            "1            10.0000          32  2.577625e+05  \n",
            "2            10.0000          64  2.916354e+05  \n",
            "3            10.0000         128  4.845428e+05  \n",
            "4            10.0000         256  1.145337e+06  \n",
            "...              ...         ...           ...  \n",
            "16195         0.0001          32  3.648672e+02  \n",
            "16196         0.0001          64  6.151743e+02  \n",
            "16197         0.0001         128  8.792568e+02  \n",
            "16198         0.0001         256  9.641462e+02  \n",
            "16199         0.0001         512  1.204026e+03  \n",
            "\n",
            "[16200 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan model dengan loss terkecil\n",
        "best_result = results_df.loc[results_df['Test Loss'].idxmin()]\n",
        "print(f\"Best Model: {best_result}\")\n",
        "\n",
        "# Visualisasi Loss vs Epochs untuk model terbaik\n",
        "plt.plot(results_df['Epochs'], results_df['Test Loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.title('Test Loss vs Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "mDnyxKzzg35Y",
        "outputId": "634006a9-ade4-4b86-e4ef-d3780f479cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: Hidden Layers                  2\n",
            "Neurons Per Layer             32\n",
            "Activation Function         Tanh\n",
            "Epochs                       100\n",
            "Learning Rate               0.01\n",
            "Batch Size                    32\n",
            "Test Loss              31.881548\n",
            "Name: 9667, dtype: object\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}