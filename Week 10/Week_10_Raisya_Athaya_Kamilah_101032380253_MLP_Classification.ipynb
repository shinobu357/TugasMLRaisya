{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMKs2OxSDKRAbf+hpwVwupd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinobu357/TugasMLRaisya/blob/main/Week%2010/Week_10_Raisya_Athaya_Kamilah_101032380253_MLP_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7o_0Ve1XOnG",
        "outputId": "2738f3a5-3f62-4dc3-fb6f-4850dff23925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Hubungkan ke Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pytorch (if not installed yet)\n",
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t9xo3CbYVLF",
        "outputId": "a1315605-1827-4681-e7a8-eb116ee8b2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "pA8c6GCxaT2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/UTS ML/car.data.csv', header=None)\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AEXtZnVLJKjL",
        "outputId": "270ccf36-c38e-4dd4-bb43-7dfcdc308d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0      1  2  3      4     5      6\n",
              "0  vhigh  vhigh  2  2  small   low  unacc\n",
              "1  vhigh  vhigh  2  2  small   med  unacc\n",
              "2  vhigh  vhigh  2  2  small  high  unacc\n",
              "3  vhigh  vhigh  2  2    med   low  unacc\n",
              "4  vhigh  vhigh  2  2    med   med  unacc"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec565467-8b95-4902-b8d0-68bdbaa1713a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>low</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>med</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>high</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>low</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>med</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec565467-8b95-4902-b8d0-68bdbaa1713a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec565467-8b95-4902-b8d0-68bdbaa1713a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec565467-8b95-4902-b8d0-68bdbaa1713a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b616a3e-cc6c-48ce-9914-b966b1386c62\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b616a3e-cc6c-48ce-9914-b966b1386c62')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b616a3e-cc6c-48ce-9914-b966b1386c62 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1728,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"high\",\n          \"low\",\n          \"vhigh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"high\",\n          \"low\",\n          \"vhigh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"3\",\n          \"5more\",\n          \"2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2\",\n          \"4\",\n          \"more\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"small\",\n          \"med\",\n          \"big\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"low\",\n          \"med\",\n          \"high\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"acc\",\n          \"good\",\n          \"unacc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the categorical data to numeric values using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode all features and target variable\n",
        "data_encoded = data.apply(label_encoder.fit_transform)\n",
        "\n",
        "# Split features and target variable\n",
        "X = data_encoded.iloc[:, :-1].values\n",
        "y = data_encoded.iloc[:, -1].values\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 512\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "AE176tL4JSEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, epochs):\n",
        "    train_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "    return train_losses\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "bISi1nzBJT0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RelU"
      ],
      "metadata": {
        "id": "CAiYvZ8SJdAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_ReLU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers):\n",
        "        super(MLP_ReLU, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.ReLU())  # ReLU activation\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, len(np.unique(y))))  # Output layer with number of classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Experiment with ReLU activation\n",
        "hidden_layers = [4]  # Example: hidden layer with 64 neurons\n",
        "epochs = 1\n",
        "lr = 10\n",
        "\n",
        "model = MLP_ReLU(input_dim=X_train.shape[1], hidden_layers=hidden_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "train_losses_relu = train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_relu = evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "X8Yi6Jb1JX2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid"
      ],
      "metadata": {
        "id": "h3Z5c_2ZJf_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Sigmoid(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers):\n",
        "        super(MLP_Sigmoid, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.Sigmoid())  # Sigmoid activation\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, len(np.unique(y))))  # Output layer with number of classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Experiment with Sigmoid activation\n",
        "hidden_layers = [16,32,64]  # Example: hidden layer with 64 neurons\n",
        "epochs = 250\n",
        "lr = 10\n",
        "\n",
        "model = MLP_Sigmoid(input_dim=X_train.shape[1], hidden_layers=hidden_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "train_losses_sigmoid = train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_sigmoid = evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "ghNOJeHVJlZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8f9bb3ba-3513-4d27-e7e2-48dbcbfac14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Loss: 7.4652\n",
            "Epoch [2/250], Loss: 1.7456\n",
            "Epoch [3/250], Loss: 1.3777\n",
            "Epoch [4/250], Loss: 1.3198\n",
            "Epoch [5/250], Loss: 0.8410\n",
            "Epoch [6/250], Loss: 1.8773\n",
            "Epoch [7/250], Loss: 1.2164\n",
            "Epoch [8/250], Loss: 1.3265\n",
            "Epoch [9/250], Loss: 1.2371\n",
            "Epoch [10/250], Loss: 1.3836\n",
            "Epoch [11/250], Loss: 0.8461\n",
            "Epoch [12/250], Loss: 0.9726\n",
            "Epoch [13/250], Loss: 1.3026\n",
            "Epoch [14/250], Loss: 1.1855\n",
            "Epoch [15/250], Loss: 1.4308\n",
            "Epoch [16/250], Loss: 0.9010\n",
            "Epoch [17/250], Loss: 1.2436\n",
            "Epoch [18/250], Loss: 1.1755\n",
            "Epoch [19/250], Loss: 1.0696\n",
            "Epoch [20/250], Loss: 0.8376\n",
            "Epoch [21/250], Loss: 1.6050\n",
            "Epoch [22/250], Loss: 1.3875\n",
            "Epoch [23/250], Loss: 1.1176\n",
            "Epoch [24/250], Loss: 1.4855\n",
            "Epoch [25/250], Loss: 1.4959\n",
            "Epoch [26/250], Loss: 0.8429\n",
            "Epoch [27/250], Loss: 1.1574\n",
            "Epoch [28/250], Loss: 1.1012\n",
            "Epoch [29/250], Loss: 1.0906\n",
            "Epoch [30/250], Loss: 1.4517\n",
            "Epoch [31/250], Loss: 1.3237\n",
            "Epoch [32/250], Loss: 0.9751\n",
            "Epoch [33/250], Loss: 1.4789\n",
            "Epoch [34/250], Loss: 1.4872\n",
            "Epoch [35/250], Loss: 1.2713\n",
            "Epoch [36/250], Loss: 0.8522\n",
            "Epoch [37/250], Loss: 1.2620\n",
            "Epoch [38/250], Loss: 1.0601\n",
            "Epoch [39/250], Loss: 1.1189\n",
            "Epoch [40/250], Loss: 1.3084\n",
            "Epoch [41/250], Loss: 1.3261\n",
            "Epoch [42/250], Loss: 1.1610\n",
            "Epoch [43/250], Loss: 1.1024\n",
            "Epoch [44/250], Loss: 1.4574\n",
            "Epoch [45/250], Loss: 0.8886\n",
            "Epoch [46/250], Loss: 1.5759\n",
            "Epoch [47/250], Loss: 1.3540\n",
            "Epoch [48/250], Loss: 0.8739\n",
            "Epoch [49/250], Loss: 1.4668\n",
            "Epoch [50/250], Loss: 1.4969\n",
            "Epoch [51/250], Loss: 1.3172\n",
            "Epoch [52/250], Loss: 1.1776\n",
            "Epoch [53/250], Loss: 1.6042\n",
            "Epoch [54/250], Loss: 1.1243\n",
            "Epoch [55/250], Loss: 1.0694\n",
            "Epoch [56/250], Loss: 1.4834\n",
            "Epoch [57/250], Loss: 1.4442\n",
            "Epoch [58/250], Loss: 1.0510\n",
            "Epoch [59/250], Loss: 1.3657\n",
            "Epoch [60/250], Loss: 1.2371\n",
            "Epoch [61/250], Loss: 1.1837\n",
            "Epoch [62/250], Loss: 0.8465\n",
            "Epoch [63/250], Loss: 1.4596\n",
            "Epoch [64/250], Loss: 0.9811\n",
            "Epoch [65/250], Loss: 1.5882\n",
            "Epoch [66/250], Loss: 1.3015\n",
            "Epoch [67/250], Loss: 1.1872\n",
            "Epoch [68/250], Loss: 1.2737\n",
            "Epoch [69/250], Loss: 1.1991\n",
            "Epoch [70/250], Loss: 0.9625\n",
            "Epoch [71/250], Loss: 1.4617\n",
            "Epoch [72/250], Loss: 1.3739\n",
            "Epoch [73/250], Loss: 0.8410\n",
            "Epoch [74/250], Loss: 1.4425\n",
            "Epoch [75/250], Loss: 1.3898\n",
            "Epoch [76/250], Loss: 0.8543\n",
            "Epoch [77/250], Loss: 1.5166\n",
            "Epoch [78/250], Loss: 0.8396\n",
            "Epoch [79/250], Loss: 1.0188\n",
            "Epoch [80/250], Loss: 1.4094\n",
            "Epoch [81/250], Loss: 1.1781\n",
            "Epoch [82/250], Loss: 1.5708\n",
            "Epoch [83/250], Loss: 1.2535\n",
            "Epoch [84/250], Loss: 1.1560\n",
            "Epoch [85/250], Loss: 1.5544\n",
            "Epoch [86/250], Loss: 1.0776\n",
            "Epoch [87/250], Loss: 1.4348\n",
            "Epoch [88/250], Loss: 1.3485\n",
            "Epoch [89/250], Loss: 0.8441\n",
            "Epoch [90/250], Loss: 1.1183\n",
            "Epoch [91/250], Loss: 1.1002\n",
            "Epoch [92/250], Loss: 1.6292\n",
            "Epoch [93/250], Loss: 1.4091\n",
            "Epoch [94/250], Loss: 1.4644\n",
            "Epoch [95/250], Loss: 1.2872\n",
            "Epoch [96/250], Loss: 1.3014\n",
            "Epoch [97/250], Loss: 1.0699\n",
            "Epoch [98/250], Loss: 1.1146\n",
            "Epoch [99/250], Loss: 1.1772\n",
            "Epoch [100/250], Loss: 1.2334\n",
            "Epoch [101/250], Loss: 1.3625\n",
            "Epoch [102/250], Loss: 1.0586\n",
            "Epoch [103/250], Loss: 1.1256\n",
            "Epoch [104/250], Loss: 0.8615\n",
            "Epoch [105/250], Loss: 1.2846\n",
            "Epoch [106/250], Loss: 0.9465\n",
            "Epoch [107/250], Loss: 1.2670\n",
            "Epoch [108/250], Loss: 1.4110\n",
            "Epoch [109/250], Loss: 1.3221\n",
            "Epoch [110/250], Loss: 1.2464\n",
            "Epoch [111/250], Loss: 0.8749\n",
            "Epoch [112/250], Loss: 1.3192\n",
            "Epoch [113/250], Loss: 1.3808\n",
            "Epoch [114/250], Loss: 1.1896\n",
            "Epoch [115/250], Loss: 1.6678\n",
            "Epoch [116/250], Loss: 1.2820\n",
            "Epoch [117/250], Loss: 1.3684\n",
            "Epoch [118/250], Loss: 1.2872\n",
            "Epoch [119/250], Loss: 1.2485\n",
            "Epoch [120/250], Loss: 1.0704\n",
            "Epoch [121/250], Loss: 1.1907\n",
            "Epoch [122/250], Loss: 1.3553\n",
            "Epoch [123/250], Loss: 1.0986\n",
            "Epoch [124/250], Loss: 1.5235\n",
            "Epoch [125/250], Loss: 1.2652\n",
            "Epoch [126/250], Loss: 1.3234\n",
            "Epoch [127/250], Loss: 1.2104\n",
            "Epoch [128/250], Loss: 1.3996\n",
            "Epoch [129/250], Loss: 1.0919\n",
            "Epoch [130/250], Loss: 1.4745\n",
            "Epoch [131/250], Loss: 1.3288\n",
            "Epoch [132/250], Loss: 1.3362\n",
            "Epoch [133/250], Loss: 1.4908\n",
            "Epoch [134/250], Loss: 1.2761\n",
            "Epoch [135/250], Loss: 1.2985\n",
            "Epoch [136/250], Loss: 0.8572\n",
            "Epoch [137/250], Loss: 1.0999\n",
            "Epoch [138/250], Loss: 1.1456\n",
            "Epoch [139/250], Loss: 1.0273\n",
            "Epoch [140/250], Loss: 1.2403\n",
            "Epoch [141/250], Loss: 1.2319\n",
            "Epoch [142/250], Loss: 1.0978\n",
            "Epoch [143/250], Loss: 1.2795\n",
            "Epoch [144/250], Loss: 1.1332\n",
            "Epoch [145/250], Loss: 1.3206\n",
            "Epoch [146/250], Loss: 1.2502\n",
            "Epoch [147/250], Loss: 1.3250\n",
            "Epoch [148/250], Loss: 0.8944\n",
            "Epoch [149/250], Loss: 1.2498\n",
            "Epoch [150/250], Loss: 1.1624\n",
            "Epoch [151/250], Loss: 0.9174\n",
            "Epoch [152/250], Loss: 1.3835\n",
            "Epoch [153/250], Loss: 1.2393\n",
            "Epoch [154/250], Loss: 0.9849\n",
            "Epoch [155/250], Loss: 1.4343\n",
            "Epoch [156/250], Loss: 1.1963\n",
            "Epoch [157/250], Loss: 1.0692\n",
            "Epoch [158/250], Loss: 1.4325\n",
            "Epoch [159/250], Loss: 1.2061\n",
            "Epoch [160/250], Loss: 1.5557\n",
            "Epoch [161/250], Loss: 1.4002\n",
            "Epoch [162/250], Loss: 0.9304\n",
            "Epoch [163/250], Loss: 1.1724\n",
            "Epoch [164/250], Loss: 1.1273\n",
            "Epoch [165/250], Loss: 1.5078\n",
            "Epoch [166/250], Loss: 1.4533\n",
            "Epoch [167/250], Loss: 1.1422\n",
            "Epoch [168/250], Loss: 1.4202\n",
            "Epoch [169/250], Loss: 1.5353\n",
            "Epoch [170/250], Loss: 1.4489\n",
            "Epoch [171/250], Loss: 1.2973\n",
            "Epoch [172/250], Loss: 0.9852\n",
            "Epoch [173/250], Loss: 1.2056\n",
            "Epoch [174/250], Loss: 0.9359\n",
            "Epoch [175/250], Loss: 1.2546\n",
            "Epoch [176/250], Loss: 1.3002\n",
            "Epoch [177/250], Loss: 0.8797\n",
            "Epoch [178/250], Loss: 1.3293\n",
            "Epoch [179/250], Loss: 0.8777\n",
            "Epoch [180/250], Loss: 1.3987\n",
            "Epoch [181/250], Loss: 0.9370\n",
            "Epoch [182/250], Loss: 1.1807\n",
            "Epoch [183/250], Loss: 1.7436\n",
            "Epoch [184/250], Loss: 1.2915\n",
            "Epoch [185/250], Loss: 1.0094\n",
            "Epoch [186/250], Loss: 1.7868\n",
            "Epoch [187/250], Loss: 1.2598\n",
            "Epoch [188/250], Loss: 1.3242\n",
            "Epoch [189/250], Loss: 1.1234\n",
            "Epoch [190/250], Loss: 1.1119\n",
            "Epoch [191/250], Loss: 1.4703\n",
            "Epoch [192/250], Loss: 1.3841\n",
            "Epoch [193/250], Loss: 1.3640\n",
            "Epoch [194/250], Loss: 1.2919\n",
            "Epoch [195/250], Loss: 1.1619\n",
            "Epoch [196/250], Loss: 0.8313\n",
            "Epoch [197/250], Loss: 1.4973\n",
            "Epoch [198/250], Loss: 1.2466\n",
            "Epoch [199/250], Loss: 0.9850\n",
            "Epoch [200/250], Loss: 1.2662\n",
            "Epoch [201/250], Loss: 1.1406\n",
            "Epoch [202/250], Loss: 1.2739\n",
            "Epoch [203/250], Loss: 1.1758\n",
            "Epoch [204/250], Loss: 1.0493\n",
            "Epoch [205/250], Loss: 1.2636\n",
            "Epoch [206/250], Loss: 1.2872\n",
            "Epoch [207/250], Loss: 1.4994\n",
            "Epoch [208/250], Loss: 1.2418\n",
            "Epoch [209/250], Loss: 1.0652\n",
            "Epoch [210/250], Loss: 1.3797\n",
            "Epoch [211/250], Loss: 1.2155\n",
            "Epoch [212/250], Loss: 1.3931\n",
            "Epoch [213/250], Loss: 1.0405\n",
            "Epoch [214/250], Loss: 1.1842\n",
            "Epoch [215/250], Loss: 1.3759\n",
            "Epoch [216/250], Loss: 0.9915\n",
            "Epoch [217/250], Loss: 1.1012\n",
            "Epoch [218/250], Loss: 1.1711\n",
            "Epoch [219/250], Loss: 0.8700\n",
            "Epoch [220/250], Loss: 1.2491\n",
            "Epoch [221/250], Loss: 1.2384\n",
            "Epoch [222/250], Loss: 0.8303\n",
            "Epoch [223/250], Loss: 0.8408\n",
            "Epoch [224/250], Loss: 1.2187\n",
            "Epoch [225/250], Loss: 0.8244\n",
            "Epoch [226/250], Loss: 1.1905\n",
            "Epoch [227/250], Loss: 0.8587\n",
            "Epoch [228/250], Loss: 1.1827\n",
            "Epoch [229/250], Loss: 0.8509\n",
            "Epoch [230/250], Loss: 1.4228\n",
            "Epoch [231/250], Loss: 1.3753\n",
            "Epoch [232/250], Loss: 1.0274\n",
            "Epoch [233/250], Loss: 1.1460\n",
            "Epoch [234/250], Loss: 1.1834\n",
            "Epoch [235/250], Loss: 1.0207\n",
            "Epoch [236/250], Loss: 1.5698\n",
            "Epoch [237/250], Loss: 1.2215\n",
            "Epoch [238/250], Loss: 1.0889\n",
            "Epoch [239/250], Loss: 1.5215\n",
            "Epoch [240/250], Loss: 1.3427\n",
            "Epoch [241/250], Loss: 1.3217\n",
            "Epoch [242/250], Loss: 0.9168\n",
            "Epoch [243/250], Loss: 1.1093\n",
            "Epoch [244/250], Loss: 0.8767\n",
            "Epoch [245/250], Loss: 1.3916\n",
            "Epoch [246/250], Loss: 1.1080\n",
            "Epoch [247/250], Loss: 1.4761\n",
            "Epoch [248/250], Loss: 1.1446\n",
            "Epoch [249/250], Loss: 1.3669\n",
            "Epoch [250/250], Loss: 0.9245\n",
            "Test Accuracy: 67.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tanh"
      ],
      "metadata": {
        "id": "8y25lf6JJpaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Tanh(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers):\n",
        "        super(MLP_Tanh, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.Tanh())  # Tanh activation\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, len(np.unique(y))))  # Output layer with number of classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Experiment with Tanh activation\n",
        "hidden_layers = [16,32,64]  # Example: hidden layer with 64 neurons\n",
        "epochs = 250\n",
        "lr = 10\n",
        "\n",
        "model = MLP_Tanh(input_dim=X_train.shape[1], hidden_layers=hidden_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "train_losses_tanh = train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_tanh = evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "x2fG8QySJsIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "acef54b8-e4f0-4675-cee3-3f21d3f7290d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Loss: 11.8059\n",
            "Epoch [2/250], Loss: 79.0421\n",
            "Epoch [3/250], Loss: 153.4555\n",
            "Epoch [4/250], Loss: 108.4655\n",
            "Epoch [5/250], Loss: 164.0088\n",
            "Epoch [6/250], Loss: 172.1033\n",
            "Epoch [7/250], Loss: 145.5839\n",
            "Epoch [8/250], Loss: 116.9109\n",
            "Epoch [9/250], Loss: 186.4050\n",
            "Epoch [10/250], Loss: 153.6872\n",
            "Epoch [11/250], Loss: 111.8042\n",
            "Epoch [12/250], Loss: 178.1324\n",
            "Epoch [13/250], Loss: 87.2985\n",
            "Epoch [14/250], Loss: 189.0258\n",
            "Epoch [15/250], Loss: 181.1360\n",
            "Epoch [16/250], Loss: 143.3685\n",
            "Epoch [17/250], Loss: 163.4900\n",
            "Epoch [18/250], Loss: 164.2583\n",
            "Epoch [19/250], Loss: 140.8490\n",
            "Epoch [20/250], Loss: 136.0762\n",
            "Epoch [21/250], Loss: 157.7492\n",
            "Epoch [22/250], Loss: 102.0767\n",
            "Epoch [23/250], Loss: 118.5884\n",
            "Epoch [24/250], Loss: 144.7599\n",
            "Epoch [25/250], Loss: 123.9152\n",
            "Epoch [26/250], Loss: 160.5291\n",
            "Epoch [27/250], Loss: 135.3695\n",
            "Epoch [28/250], Loss: 172.6226\n",
            "Epoch [29/250], Loss: 173.5915\n",
            "Epoch [30/250], Loss: 146.0839\n",
            "Epoch [31/250], Loss: 110.8574\n",
            "Epoch [32/250], Loss: 157.2813\n",
            "Epoch [33/250], Loss: 153.9761\n",
            "Epoch [34/250], Loss: 122.0949\n",
            "Epoch [35/250], Loss: 178.7017\n",
            "Epoch [36/250], Loss: 84.3520\n",
            "Epoch [37/250], Loss: 202.0086\n",
            "Epoch [38/250], Loss: 156.0836\n",
            "Epoch [39/250], Loss: 142.3181\n",
            "Epoch [40/250], Loss: 143.6307\n",
            "Epoch [41/250], Loss: 168.1350\n",
            "Epoch [42/250], Loss: 131.8473\n",
            "Epoch [43/250], Loss: 132.4339\n",
            "Epoch [44/250], Loss: 166.1566\n",
            "Epoch [45/250], Loss: 93.3333\n",
            "Epoch [46/250], Loss: 190.7841\n",
            "Epoch [47/250], Loss: 149.3723\n",
            "Epoch [48/250], Loss: 132.1507\n",
            "Epoch [49/250], Loss: 164.9709\n",
            "Epoch [50/250], Loss: 144.2600\n",
            "Epoch [51/250], Loss: 118.7122\n",
            "Epoch [52/250], Loss: 154.9853\n",
            "Epoch [53/250], Loss: 143.5668\n",
            "Epoch [54/250], Loss: 95.0192\n",
            "Epoch [55/250], Loss: 174.1129\n",
            "Epoch [56/250], Loss: 143.1519\n",
            "Epoch [57/250], Loss: 172.4488\n",
            "Epoch [58/250], Loss: 179.0832\n",
            "Epoch [59/250], Loss: 142.0663\n",
            "Epoch [60/250], Loss: 113.9113\n",
            "Epoch [61/250], Loss: 177.4847\n",
            "Epoch [62/250], Loss: 137.8937\n",
            "Epoch [63/250], Loss: 122.0208\n",
            "Epoch [64/250], Loss: 172.6343\n",
            "Epoch [65/250], Loss: 111.4435\n",
            "Epoch [66/250], Loss: 205.8795\n",
            "Epoch [67/250], Loss: 160.5818\n",
            "Epoch [68/250], Loss: 127.6793\n",
            "Epoch [69/250], Loss: 131.0513\n",
            "Epoch [70/250], Loss: 162.3783\n",
            "Epoch [71/250], Loss: 89.5543\n",
            "Epoch [72/250], Loss: 199.0653\n",
            "Epoch [73/250], Loss: 150.1978\n",
            "Epoch [74/250], Loss: 109.7560\n",
            "Epoch [75/250], Loss: 185.6400\n",
            "Epoch [76/250], Loss: 165.9976\n",
            "Epoch [77/250], Loss: 108.3476\n",
            "Epoch [78/250], Loss: 146.4712\n",
            "Epoch [79/250], Loss: 146.2203\n",
            "Epoch [80/250], Loss: 137.4351\n",
            "Epoch [81/250], Loss: 185.0085\n",
            "Epoch [82/250], Loss: 154.5928\n",
            "Epoch [83/250], Loss: 105.6744\n",
            "Epoch [84/250], Loss: 176.4480\n",
            "Epoch [85/250], Loss: 131.2501\n",
            "Epoch [86/250], Loss: 116.4691\n",
            "Epoch [87/250], Loss: 175.9267\n",
            "Epoch [88/250], Loss: 111.5518\n",
            "Epoch [89/250], Loss: 199.6916\n",
            "Epoch [90/250], Loss: 169.3592\n",
            "Epoch [91/250], Loss: 117.6378\n",
            "Epoch [92/250], Loss: 116.6089\n",
            "Epoch [93/250], Loss: 166.2639\n",
            "Epoch [94/250], Loss: 114.8757\n",
            "Epoch [95/250], Loss: 139.2409\n",
            "Epoch [96/250], Loss: 167.3971\n",
            "Epoch [97/250], Loss: 154.1421\n",
            "Epoch [98/250], Loss: 192.2749\n",
            "Epoch [99/250], Loss: 152.2447\n",
            "Epoch [100/250], Loss: 121.5074\n",
            "Epoch [101/250], Loss: 115.3648\n",
            "Epoch [102/250], Loss: 159.6946\n",
            "Epoch [103/250], Loss: 114.4614\n",
            "Epoch [104/250], Loss: 181.4480\n",
            "Epoch [105/250], Loss: 143.5230\n",
            "Epoch [106/250], Loss: 171.7824\n",
            "Epoch [107/250], Loss: 169.2950\n",
            "Epoch [108/250], Loss: 148.7901\n",
            "Epoch [109/250], Loss: 104.6719\n",
            "Epoch [110/250], Loss: 141.3404\n",
            "Epoch [111/250], Loss: 140.5700\n",
            "Epoch [112/250], Loss: 128.1096\n",
            "Epoch [113/250], Loss: 166.4625\n",
            "Epoch [114/250], Loss: 142.4179\n",
            "Epoch [115/250], Loss: 201.9048\n",
            "Epoch [116/250], Loss: 143.9518\n",
            "Epoch [117/250], Loss: 123.5854\n",
            "Epoch [118/250], Loss: 119.5184\n",
            "Epoch [119/250], Loss: 159.0062\n",
            "Epoch [120/250], Loss: 124.5998\n",
            "Epoch [121/250], Loss: 179.9237\n",
            "Epoch [122/250], Loss: 113.6804\n",
            "Epoch [123/250], Loss: 190.5835\n",
            "Epoch [124/250], Loss: 178.6856\n",
            "Epoch [125/250], Loss: 149.7094\n",
            "Epoch [126/250], Loss: 105.9451\n",
            "Epoch [127/250], Loss: 161.3254\n",
            "Epoch [128/250], Loss: 141.1254\n",
            "Epoch [129/250], Loss: 127.5946\n",
            "Epoch [130/250], Loss: 167.7981\n",
            "Epoch [131/250], Loss: 89.1622\n",
            "Epoch [132/250], Loss: 211.2206\n",
            "Epoch [133/250], Loss: 163.7513\n",
            "Epoch [134/250], Loss: 131.7864\n",
            "Epoch [135/250], Loss: 144.2477\n",
            "Epoch [136/250], Loss: 176.6329\n",
            "Epoch [137/250], Loss: 134.6107\n",
            "Epoch [138/250], Loss: 144.9323\n",
            "Epoch [139/250], Loss: 167.9801\n",
            "Epoch [140/250], Loss: 97.0232\n",
            "Epoch [141/250], Loss: 186.8718\n",
            "Epoch [142/250], Loss: 156.9927\n",
            "Epoch [143/250], Loss: 125.1967\n",
            "Epoch [144/250], Loss: 190.4582\n",
            "Epoch [145/250], Loss: 112.4066\n",
            "Epoch [146/250], Loss: 138.5553\n",
            "Epoch [147/250], Loss: 174.1817\n",
            "Epoch [148/250], Loss: 130.6008\n",
            "Epoch [149/250], Loss: 99.4117\n",
            "Epoch [150/250], Loss: 172.1530\n",
            "Epoch [151/250], Loss: 109.8746\n",
            "Epoch [152/250], Loss: 192.2907\n",
            "Epoch [153/250], Loss: 172.9163\n",
            "Epoch [154/250], Loss: 121.9120\n",
            "Epoch [155/250], Loss: 159.8396\n",
            "Epoch [156/250], Loss: 155.8014\n",
            "Epoch [157/250], Loss: 126.6863\n",
            "Epoch [158/250], Loss: 139.7257\n",
            "Epoch [159/250], Loss: 151.6134\n",
            "Epoch [160/250], Loss: 127.6429\n",
            "Epoch [161/250], Loss: 194.0400\n",
            "Epoch [162/250], Loss: 151.1296\n",
            "Epoch [163/250], Loss: 121.8407\n",
            "Epoch [164/250], Loss: 101.7603\n",
            "Epoch [165/250], Loss: 159.7508\n",
            "Epoch [166/250], Loss: 117.0395\n",
            "Epoch [167/250], Loss: 178.8959\n",
            "Epoch [168/250], Loss: 166.9331\n",
            "Epoch [169/250], Loss: 124.3748\n",
            "Epoch [170/250], Loss: 157.1832\n",
            "Epoch [171/250], Loss: 153.2147\n",
            "Epoch [172/250], Loss: 124.1605\n",
            "Epoch [173/250], Loss: 121.1149\n",
            "Epoch [174/250], Loss: 160.4393\n",
            "Epoch [175/250], Loss: 122.4633\n",
            "Epoch [176/250], Loss: 184.4729\n",
            "Epoch [177/250], Loss: 150.3323\n",
            "Epoch [178/250], Loss: 128.3767\n",
            "Epoch [179/250], Loss: 146.6213\n",
            "Epoch [180/250], Loss: 155.4784\n",
            "Epoch [181/250], Loss: 112.8884\n",
            "Epoch [182/250], Loss: 140.3921\n",
            "Epoch [183/250], Loss: 154.6963\n",
            "Epoch [184/250], Loss: 143.4002\n",
            "Epoch [185/250], Loss: 186.0174\n",
            "Epoch [186/250], Loss: 150.5188\n",
            "Epoch [187/250], Loss: 87.0308\n",
            "Epoch [188/250], Loss: 158.4589\n",
            "Epoch [189/250], Loss: 122.7809\n",
            "Epoch [190/250], Loss: 123.9334\n",
            "Epoch [191/250], Loss: 172.0764\n",
            "Epoch [192/250], Loss: 135.7916\n",
            "Epoch [193/250], Loss: 143.4644\n",
            "Epoch [194/250], Loss: 121.5994\n",
            "Epoch [195/250], Loss: 172.7873\n",
            "Epoch [196/250], Loss: 123.0495\n",
            "Epoch [197/250], Loss: 110.8190\n",
            "Epoch [198/250], Loss: 164.8253\n",
            "Epoch [199/250], Loss: 126.4567\n",
            "Epoch [200/250], Loss: 207.8992\n",
            "Epoch [201/250], Loss: 169.8458\n",
            "Epoch [202/250], Loss: 125.9195\n",
            "Epoch [203/250], Loss: 136.9644\n",
            "Epoch [204/250], Loss: 163.4011\n",
            "Epoch [205/250], Loss: 137.2363\n",
            "Epoch [206/250], Loss: 85.0874\n",
            "Epoch [207/250], Loss: 150.3671\n",
            "Epoch [208/250], Loss: 125.6705\n",
            "Epoch [209/250], Loss: 185.5266\n",
            "Epoch [210/250], Loss: 155.3405\n",
            "Epoch [211/250], Loss: 127.9254\n",
            "Epoch [212/250], Loss: 164.6757\n",
            "Epoch [213/250], Loss: 151.1590\n",
            "Epoch [214/250], Loss: 85.5648\n",
            "Epoch [215/250], Loss: 188.4084\n",
            "Epoch [216/250], Loss: 157.9812\n",
            "Epoch [217/250], Loss: 113.8486\n",
            "Epoch [218/250], Loss: 167.8607\n",
            "Epoch [219/250], Loss: 141.1489\n",
            "Epoch [220/250], Loss: 113.8377\n",
            "Epoch [221/250], Loss: 178.8801\n",
            "Epoch [222/250], Loss: 100.4252\n",
            "Epoch [223/250], Loss: 197.2952\n",
            "Epoch [224/250], Loss: 178.9365\n",
            "Epoch [225/250], Loss: 132.0388\n",
            "Epoch [226/250], Loss: 108.7120\n",
            "Epoch [227/250], Loss: 118.6329\n",
            "Epoch [228/250], Loss: 149.3281\n",
            "Epoch [229/250], Loss: 127.0066\n",
            "Epoch [230/250], Loss: 162.3526\n",
            "Epoch [231/250], Loss: 115.4561\n",
            "Epoch [232/250], Loss: 221.2468\n",
            "Epoch [233/250], Loss: 170.1655\n",
            "Epoch [234/250], Loss: 130.7693\n",
            "Epoch [235/250], Loss: 97.2404\n",
            "Epoch [236/250], Loss: 141.7471\n",
            "Epoch [237/250], Loss: 123.0281\n",
            "Epoch [238/250], Loss: 157.0187\n",
            "Epoch [239/250], Loss: 159.8304\n",
            "Epoch [240/250], Loss: 138.6500\n",
            "Epoch [241/250], Loss: 195.7990\n",
            "Epoch [242/250], Loss: 146.5871\n",
            "Epoch [243/250], Loss: 123.6836\n",
            "Epoch [244/250], Loss: 106.5711\n",
            "Epoch [245/250], Loss: 150.5780\n",
            "Epoch [246/250], Loss: 118.2906\n",
            "Epoch [247/250], Loss: 178.2782\n",
            "Epoch [248/250], Loss: 85.7219\n",
            "Epoch [249/250], Loss: 198.5366\n",
            "Epoch [250/250], Loss: 186.4116\n",
            "Test Accuracy: 67.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear"
      ],
      "metadata": {
        "id": "7zwJqcxDL6u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Linear(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers):\n",
        "        super(MLP_Linear, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.Identity())  # Linear (no activation)\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, len(np.unique(y))))  # Output layer with number of classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Experiment with Linear activation\n",
        "hidden_layers = [16,32,64]  # Example: hidden layer with 64 neurons\n",
        "epochs = 250\n",
        "lr = 10\n",
        "\n",
        "model = MLP_Linear(input_dim=X_train.shape[1], hidden_layers=hidden_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "train_losses_linear = train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_linear = evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "q-IJ5J7cJvzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "32883675-d4ac-45b4-a75d-d294ccf1db32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Loss: 181821909728.4973\n",
            "Epoch [2/250], Loss: nan\n",
            "Epoch [3/250], Loss: nan\n",
            "Epoch [4/250], Loss: nan\n",
            "Epoch [5/250], Loss: nan\n",
            "Epoch [6/250], Loss: nan\n",
            "Epoch [7/250], Loss: nan\n",
            "Epoch [8/250], Loss: nan\n",
            "Epoch [9/250], Loss: nan\n",
            "Epoch [10/250], Loss: nan\n",
            "Epoch [11/250], Loss: nan\n",
            "Epoch [12/250], Loss: nan\n",
            "Epoch [13/250], Loss: nan\n",
            "Epoch [14/250], Loss: nan\n",
            "Epoch [15/250], Loss: nan\n",
            "Epoch [16/250], Loss: nan\n",
            "Epoch [17/250], Loss: nan\n",
            "Epoch [18/250], Loss: nan\n",
            "Epoch [19/250], Loss: nan\n",
            "Epoch [20/250], Loss: nan\n",
            "Epoch [21/250], Loss: nan\n",
            "Epoch [22/250], Loss: nan\n",
            "Epoch [23/250], Loss: nan\n",
            "Epoch [24/250], Loss: nan\n",
            "Epoch [25/250], Loss: nan\n",
            "Epoch [26/250], Loss: nan\n",
            "Epoch [27/250], Loss: nan\n",
            "Epoch [28/250], Loss: nan\n",
            "Epoch [29/250], Loss: nan\n",
            "Epoch [30/250], Loss: nan\n",
            "Epoch [31/250], Loss: nan\n",
            "Epoch [32/250], Loss: nan\n",
            "Epoch [33/250], Loss: nan\n",
            "Epoch [34/250], Loss: nan\n",
            "Epoch [35/250], Loss: nan\n",
            "Epoch [36/250], Loss: nan\n",
            "Epoch [37/250], Loss: nan\n",
            "Epoch [38/250], Loss: nan\n",
            "Epoch [39/250], Loss: nan\n",
            "Epoch [40/250], Loss: nan\n",
            "Epoch [41/250], Loss: nan\n",
            "Epoch [42/250], Loss: nan\n",
            "Epoch [43/250], Loss: nan\n",
            "Epoch [44/250], Loss: nan\n",
            "Epoch [45/250], Loss: nan\n",
            "Epoch [46/250], Loss: nan\n",
            "Epoch [47/250], Loss: nan\n",
            "Epoch [48/250], Loss: nan\n",
            "Epoch [49/250], Loss: nan\n",
            "Epoch [50/250], Loss: nan\n",
            "Epoch [51/250], Loss: nan\n",
            "Epoch [52/250], Loss: nan\n",
            "Epoch [53/250], Loss: nan\n",
            "Epoch [54/250], Loss: nan\n",
            "Epoch [55/250], Loss: nan\n",
            "Epoch [56/250], Loss: nan\n",
            "Epoch [57/250], Loss: nan\n",
            "Epoch [58/250], Loss: nan\n",
            "Epoch [59/250], Loss: nan\n",
            "Epoch [60/250], Loss: nan\n",
            "Epoch [61/250], Loss: nan\n",
            "Epoch [62/250], Loss: nan\n",
            "Epoch [63/250], Loss: nan\n",
            "Epoch [64/250], Loss: nan\n",
            "Epoch [65/250], Loss: nan\n",
            "Epoch [66/250], Loss: nan\n",
            "Epoch [67/250], Loss: nan\n",
            "Epoch [68/250], Loss: nan\n",
            "Epoch [69/250], Loss: nan\n",
            "Epoch [70/250], Loss: nan\n",
            "Epoch [71/250], Loss: nan\n",
            "Epoch [72/250], Loss: nan\n",
            "Epoch [73/250], Loss: nan\n",
            "Epoch [74/250], Loss: nan\n",
            "Epoch [75/250], Loss: nan\n",
            "Epoch [76/250], Loss: nan\n",
            "Epoch [77/250], Loss: nan\n",
            "Epoch [78/250], Loss: nan\n",
            "Epoch [79/250], Loss: nan\n",
            "Epoch [80/250], Loss: nan\n",
            "Epoch [81/250], Loss: nan\n",
            "Epoch [82/250], Loss: nan\n",
            "Epoch [83/250], Loss: nan\n",
            "Epoch [84/250], Loss: nan\n",
            "Epoch [85/250], Loss: nan\n",
            "Epoch [86/250], Loss: nan\n",
            "Epoch [87/250], Loss: nan\n",
            "Epoch [88/250], Loss: nan\n",
            "Epoch [89/250], Loss: nan\n",
            "Epoch [90/250], Loss: nan\n",
            "Epoch [91/250], Loss: nan\n",
            "Epoch [92/250], Loss: nan\n",
            "Epoch [93/250], Loss: nan\n",
            "Epoch [94/250], Loss: nan\n",
            "Epoch [95/250], Loss: nan\n",
            "Epoch [96/250], Loss: nan\n",
            "Epoch [97/250], Loss: nan\n",
            "Epoch [98/250], Loss: nan\n",
            "Epoch [99/250], Loss: nan\n",
            "Epoch [100/250], Loss: nan\n",
            "Epoch [101/250], Loss: nan\n",
            "Epoch [102/250], Loss: nan\n",
            "Epoch [103/250], Loss: nan\n",
            "Epoch [104/250], Loss: nan\n",
            "Epoch [105/250], Loss: nan\n",
            "Epoch [106/250], Loss: nan\n",
            "Epoch [107/250], Loss: nan\n",
            "Epoch [108/250], Loss: nan\n",
            "Epoch [109/250], Loss: nan\n",
            "Epoch [110/250], Loss: nan\n",
            "Epoch [111/250], Loss: nan\n",
            "Epoch [112/250], Loss: nan\n",
            "Epoch [113/250], Loss: nan\n",
            "Epoch [114/250], Loss: nan\n",
            "Epoch [115/250], Loss: nan\n",
            "Epoch [116/250], Loss: nan\n",
            "Epoch [117/250], Loss: nan\n",
            "Epoch [118/250], Loss: nan\n",
            "Epoch [119/250], Loss: nan\n",
            "Epoch [120/250], Loss: nan\n",
            "Epoch [121/250], Loss: nan\n",
            "Epoch [122/250], Loss: nan\n",
            "Epoch [123/250], Loss: nan\n",
            "Epoch [124/250], Loss: nan\n",
            "Epoch [125/250], Loss: nan\n",
            "Epoch [126/250], Loss: nan\n",
            "Epoch [127/250], Loss: nan\n",
            "Epoch [128/250], Loss: nan\n",
            "Epoch [129/250], Loss: nan\n",
            "Epoch [130/250], Loss: nan\n",
            "Epoch [131/250], Loss: nan\n",
            "Epoch [132/250], Loss: nan\n",
            "Epoch [133/250], Loss: nan\n",
            "Epoch [134/250], Loss: nan\n",
            "Epoch [135/250], Loss: nan\n",
            "Epoch [136/250], Loss: nan\n",
            "Epoch [137/250], Loss: nan\n",
            "Epoch [138/250], Loss: nan\n",
            "Epoch [139/250], Loss: nan\n",
            "Epoch [140/250], Loss: nan\n",
            "Epoch [141/250], Loss: nan\n",
            "Epoch [142/250], Loss: nan\n",
            "Epoch [143/250], Loss: nan\n",
            "Epoch [144/250], Loss: nan\n",
            "Epoch [145/250], Loss: nan\n",
            "Epoch [146/250], Loss: nan\n",
            "Epoch [147/250], Loss: nan\n",
            "Epoch [148/250], Loss: nan\n",
            "Epoch [149/250], Loss: nan\n",
            "Epoch [150/250], Loss: nan\n",
            "Epoch [151/250], Loss: nan\n",
            "Epoch [152/250], Loss: nan\n",
            "Epoch [153/250], Loss: nan\n",
            "Epoch [154/250], Loss: nan\n",
            "Epoch [155/250], Loss: nan\n",
            "Epoch [156/250], Loss: nan\n",
            "Epoch [157/250], Loss: nan\n",
            "Epoch [158/250], Loss: nan\n",
            "Epoch [159/250], Loss: nan\n",
            "Epoch [160/250], Loss: nan\n",
            "Epoch [161/250], Loss: nan\n",
            "Epoch [162/250], Loss: nan\n",
            "Epoch [163/250], Loss: nan\n",
            "Epoch [164/250], Loss: nan\n",
            "Epoch [165/250], Loss: nan\n",
            "Epoch [166/250], Loss: nan\n",
            "Epoch [167/250], Loss: nan\n",
            "Epoch [168/250], Loss: nan\n",
            "Epoch [169/250], Loss: nan\n",
            "Epoch [170/250], Loss: nan\n",
            "Epoch [171/250], Loss: nan\n",
            "Epoch [172/250], Loss: nan\n",
            "Epoch [173/250], Loss: nan\n",
            "Epoch [174/250], Loss: nan\n",
            "Epoch [175/250], Loss: nan\n",
            "Epoch [176/250], Loss: nan\n",
            "Epoch [177/250], Loss: nan\n",
            "Epoch [178/250], Loss: nan\n",
            "Epoch [179/250], Loss: nan\n",
            "Epoch [180/250], Loss: nan\n",
            "Epoch [181/250], Loss: nan\n",
            "Epoch [182/250], Loss: nan\n",
            "Epoch [183/250], Loss: nan\n",
            "Epoch [184/250], Loss: nan\n",
            "Epoch [185/250], Loss: nan\n",
            "Epoch [186/250], Loss: nan\n",
            "Epoch [187/250], Loss: nan\n",
            "Epoch [188/250], Loss: nan\n",
            "Epoch [189/250], Loss: nan\n",
            "Epoch [190/250], Loss: nan\n",
            "Epoch [191/250], Loss: nan\n",
            "Epoch [192/250], Loss: nan\n",
            "Epoch [193/250], Loss: nan\n",
            "Epoch [194/250], Loss: nan\n",
            "Epoch [195/250], Loss: nan\n",
            "Epoch [196/250], Loss: nan\n",
            "Epoch [197/250], Loss: nan\n",
            "Epoch [198/250], Loss: nan\n",
            "Epoch [199/250], Loss: nan\n",
            "Epoch [200/250], Loss: nan\n",
            "Epoch [201/250], Loss: nan\n",
            "Epoch [202/250], Loss: nan\n",
            "Epoch [203/250], Loss: nan\n",
            "Epoch [204/250], Loss: nan\n",
            "Epoch [205/250], Loss: nan\n",
            "Epoch [206/250], Loss: nan\n",
            "Epoch [207/250], Loss: nan\n",
            "Epoch [208/250], Loss: nan\n",
            "Epoch [209/250], Loss: nan\n",
            "Epoch [210/250], Loss: nan\n",
            "Epoch [211/250], Loss: nan\n",
            "Epoch [212/250], Loss: nan\n",
            "Epoch [213/250], Loss: nan\n",
            "Epoch [214/250], Loss: nan\n",
            "Epoch [215/250], Loss: nan\n",
            "Epoch [216/250], Loss: nan\n",
            "Epoch [217/250], Loss: nan\n",
            "Epoch [218/250], Loss: nan\n",
            "Epoch [219/250], Loss: nan\n",
            "Epoch [220/250], Loss: nan\n",
            "Epoch [221/250], Loss: nan\n",
            "Epoch [222/250], Loss: nan\n",
            "Epoch [223/250], Loss: nan\n",
            "Epoch [224/250], Loss: nan\n",
            "Epoch [225/250], Loss: nan\n",
            "Epoch [226/250], Loss: nan\n",
            "Epoch [227/250], Loss: nan\n",
            "Epoch [228/250], Loss: nan\n",
            "Epoch [229/250], Loss: nan\n",
            "Epoch [230/250], Loss: nan\n",
            "Epoch [231/250], Loss: nan\n",
            "Epoch [232/250], Loss: nan\n",
            "Epoch [233/250], Loss: nan\n",
            "Epoch [234/250], Loss: nan\n",
            "Epoch [235/250], Loss: nan\n",
            "Epoch [236/250], Loss: nan\n",
            "Epoch [237/250], Loss: nan\n",
            "Epoch [238/250], Loss: nan\n",
            "Epoch [239/250], Loss: nan\n",
            "Epoch [240/250], Loss: nan\n",
            "Epoch [241/250], Loss: nan\n",
            "Epoch [242/250], Loss: nan\n",
            "Epoch [243/250], Loss: nan\n",
            "Epoch [244/250], Loss: nan\n",
            "Epoch [245/250], Loss: nan\n",
            "Epoch [246/250], Loss: nan\n",
            "Epoch [247/250], Loss: nan\n",
            "Epoch [248/250], Loss: nan\n",
            "Epoch [249/250], Loss: nan\n",
            "Epoch [250/250], Loss: nan\n",
            "Test Accuracy: 23.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax"
      ],
      "metadata": {
        "id": "KpWhX2GTJwdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Softmax(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers):\n",
        "        super(MLP_Softmax, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, h_dim))\n",
        "            layers.append(nn.ReLU())  # ReLU activation in hidden layers\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, len(np.unique(y))))  # Output layer with number of classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.Softmax(dim=1)(self.model(x))  # Apply Softmax on output\n",
        "\n",
        "# Experiment with Softmax activation\n",
        "hidden_layers = [16,32,64]  # Example: hidden layer with 64 neurons\n",
        "epochs = 250\n",
        "lr = 10\n",
        "\n",
        "model = MLP_Softmax(input_dim=X_train.shape[1], hidden_layers=hidden_layers)\n",
        "criterion = nn.CrossEntropyLoss()  # Corrected here\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Train model\n",
        "train_losses_softmax = train_model(model, criterion, optimizer, train_loader, epochs)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy_softmax = evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "MRYwno-sJ3FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49440f7-b7e3-450f-a2d1-12ec2ed2bb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/250], Loss: 1.1809\n",
            "Epoch [2/250], Loss: 1.0364\n",
            "Epoch [3/250], Loss: 1.0355\n",
            "Epoch [4/250], Loss: 1.0378\n",
            "Epoch [5/250], Loss: 1.0375\n",
            "Epoch [6/250], Loss: 1.0341\n",
            "Epoch [7/250], Loss: 1.0383\n",
            "Epoch [8/250], Loss: 1.0355\n",
            "Epoch [9/250], Loss: 1.0367\n",
            "Epoch [10/250], Loss: 1.0392\n",
            "Epoch [11/250], Loss: 1.0364\n",
            "Epoch [12/250], Loss: 1.0403\n",
            "Epoch [13/250], Loss: 1.0389\n",
            "Epoch [14/250], Loss: 1.0389\n",
            "Epoch [15/250], Loss: 1.0378\n",
            "Epoch [16/250], Loss: 1.0336\n",
            "Epoch [17/250], Loss: 1.0383\n",
            "Epoch [18/250], Loss: 1.0397\n",
            "Epoch [19/250], Loss: 1.0411\n",
            "Epoch [20/250], Loss: 1.0378\n",
            "Epoch [21/250], Loss: 1.0395\n",
            "Epoch [22/250], Loss: 1.0403\n",
            "Epoch [23/250], Loss: 1.0414\n",
            "Epoch [24/250], Loss: 1.0397\n",
            "Epoch [25/250], Loss: 1.0364\n",
            "Epoch [26/250], Loss: 1.0372\n",
            "Epoch [27/250], Loss: 1.0375\n",
            "Epoch [28/250], Loss: 1.0347\n",
            "Epoch [29/250], Loss: 1.0437\n",
            "Epoch [30/250], Loss: 1.0350\n",
            "Epoch [31/250], Loss: 1.0395\n",
            "Epoch [32/250], Loss: 1.0341\n",
            "Epoch [33/250], Loss: 1.0395\n",
            "Epoch [34/250], Loss: 1.0431\n",
            "Epoch [35/250], Loss: 1.0378\n",
            "Epoch [36/250], Loss: 1.0355\n",
            "Epoch [37/250], Loss: 1.0389\n",
            "Epoch [38/250], Loss: 1.0372\n",
            "Epoch [39/250], Loss: 1.0369\n",
            "Epoch [40/250], Loss: 1.0358\n",
            "Epoch [41/250], Loss: 1.0375\n",
            "Epoch [42/250], Loss: 1.0420\n",
            "Epoch [43/250], Loss: 1.0383\n",
            "Epoch [44/250], Loss: 1.0364\n",
            "Epoch [45/250], Loss: 1.0378\n",
            "Epoch [46/250], Loss: 1.0347\n",
            "Epoch [47/250], Loss: 1.0386\n",
            "Epoch [48/250], Loss: 1.0417\n",
            "Epoch [49/250], Loss: 1.0383\n",
            "Epoch [50/250], Loss: 1.0355\n",
            "Epoch [51/250], Loss: 1.0439\n",
            "Epoch [52/250], Loss: 1.0361\n",
            "Epoch [53/250], Loss: 1.0386\n",
            "Epoch [54/250], Loss: 1.0423\n",
            "Epoch [55/250], Loss: 1.0400\n",
            "Epoch [56/250], Loss: 1.0361\n",
            "Epoch [57/250], Loss: 1.0397\n",
            "Epoch [58/250], Loss: 1.0355\n",
            "Epoch [59/250], Loss: 1.0350\n",
            "Epoch [60/250], Loss: 1.0367\n",
            "Epoch [61/250], Loss: 1.0358\n",
            "Epoch [62/250], Loss: 1.0358\n",
            "Epoch [63/250], Loss: 1.0389\n",
            "Epoch [64/250], Loss: 1.0423\n",
            "Epoch [65/250], Loss: 1.0367\n",
            "Epoch [66/250], Loss: 1.0392\n",
            "Epoch [67/250], Loss: 1.0375\n",
            "Epoch [68/250], Loss: 1.0372\n",
            "Epoch [69/250], Loss: 1.0392\n",
            "Epoch [70/250], Loss: 1.0403\n",
            "Epoch [71/250], Loss: 1.0358\n",
            "Epoch [72/250], Loss: 1.0406\n",
            "Epoch [73/250], Loss: 1.0406\n",
            "Epoch [74/250], Loss: 1.0386\n",
            "Epoch [75/250], Loss: 1.0383\n",
            "Epoch [76/250], Loss: 1.0409\n",
            "Epoch [77/250], Loss: 1.0383\n",
            "Epoch [78/250], Loss: 1.0369\n",
            "Epoch [79/250], Loss: 1.0369\n",
            "Epoch [80/250], Loss: 1.0409\n",
            "Epoch [81/250], Loss: 1.0367\n",
            "Epoch [82/250], Loss: 1.0392\n",
            "Epoch [83/250], Loss: 1.0397\n",
            "Epoch [84/250], Loss: 1.0364\n",
            "Epoch [85/250], Loss: 1.0403\n",
            "Epoch [86/250], Loss: 1.0414\n",
            "Epoch [87/250], Loss: 1.0378\n",
            "Epoch [88/250], Loss: 1.0417\n",
            "Epoch [89/250], Loss: 1.0411\n",
            "Epoch [90/250], Loss: 1.0350\n",
            "Epoch [91/250], Loss: 1.0383\n",
            "Epoch [92/250], Loss: 1.0386\n",
            "Epoch [93/250], Loss: 1.0347\n",
            "Epoch [94/250], Loss: 1.0381\n",
            "Epoch [95/250], Loss: 1.0378\n",
            "Epoch [96/250], Loss: 1.0372\n",
            "Epoch [97/250], Loss: 1.0392\n",
            "Epoch [98/250], Loss: 1.0406\n",
            "Epoch [99/250], Loss: 1.0344\n",
            "Epoch [100/250], Loss: 1.0361\n",
            "Epoch [101/250], Loss: 1.0414\n",
            "Epoch [102/250], Loss: 1.0350\n",
            "Epoch [103/250], Loss: 1.0414\n",
            "Epoch [104/250], Loss: 1.0372\n",
            "Epoch [105/250], Loss: 1.0414\n",
            "Epoch [106/250], Loss: 1.0375\n",
            "Epoch [107/250], Loss: 1.0386\n",
            "Epoch [108/250], Loss: 1.0378\n",
            "Epoch [109/250], Loss: 1.0425\n",
            "Epoch [110/250], Loss: 1.0352\n",
            "Epoch [111/250], Loss: 1.0392\n",
            "Epoch [112/250], Loss: 1.0383\n",
            "Epoch [113/250], Loss: 1.0350\n",
            "Epoch [114/250], Loss: 1.0366\n",
            "Epoch [115/250], Loss: 1.0381\n",
            "Epoch [116/250], Loss: 1.0369\n",
            "Epoch [117/250], Loss: 1.0386\n",
            "Epoch [118/250], Loss: 1.0378\n",
            "Epoch [119/250], Loss: 1.0389\n",
            "Epoch [120/250], Loss: 1.0386\n",
            "Epoch [121/250], Loss: 1.0372\n",
            "Epoch [122/250], Loss: 1.0406\n",
            "Epoch [123/250], Loss: 1.0431\n",
            "Epoch [124/250], Loss: 1.0367\n",
            "Epoch [125/250], Loss: 1.0338\n",
            "Epoch [126/250], Loss: 1.0434\n",
            "Epoch [127/250], Loss: 1.0364\n",
            "Epoch [128/250], Loss: 1.0400\n",
            "Epoch [129/250], Loss: 1.0350\n",
            "Epoch [130/250], Loss: 1.0392\n",
            "Epoch [131/250], Loss: 1.0375\n",
            "Epoch [132/250], Loss: 1.0372\n",
            "Epoch [133/250], Loss: 1.0369\n",
            "Epoch [134/250], Loss: 1.0386\n",
            "Epoch [135/250], Loss: 1.0389\n",
            "Epoch [136/250], Loss: 1.0366\n",
            "Epoch [137/250], Loss: 1.0397\n",
            "Epoch [138/250], Loss: 1.0369\n",
            "Epoch [139/250], Loss: 1.0364\n",
            "Epoch [140/250], Loss: 1.0395\n",
            "Epoch [141/250], Loss: 1.0414\n",
            "Epoch [142/250], Loss: 1.0383\n",
            "Epoch [143/250], Loss: 1.0375\n",
            "Epoch [144/250], Loss: 1.0366\n",
            "Epoch [145/250], Loss: 1.0392\n",
            "Epoch [146/250], Loss: 1.0367\n",
            "Epoch [147/250], Loss: 1.0409\n",
            "Epoch [148/250], Loss: 1.0358\n",
            "Epoch [149/250], Loss: 1.0423\n",
            "Epoch [150/250], Loss: 1.0369\n",
            "Epoch [151/250], Loss: 1.0423\n",
            "Epoch [152/250], Loss: 1.0336\n",
            "Epoch [153/250], Loss: 1.0338\n",
            "Epoch [154/250], Loss: 1.0400\n",
            "Epoch [155/250], Loss: 1.0420\n",
            "Epoch [156/250], Loss: 1.0386\n",
            "Epoch [157/250], Loss: 1.0389\n",
            "Epoch [158/250], Loss: 1.0366\n",
            "Epoch [159/250], Loss: 1.0378\n",
            "Epoch [160/250], Loss: 1.0369\n",
            "Epoch [161/250], Loss: 1.0386\n",
            "Epoch [162/250], Loss: 1.0378\n",
            "Epoch [163/250], Loss: 1.0366\n",
            "Epoch [164/250], Loss: 1.0386\n",
            "Epoch [165/250], Loss: 1.0394\n",
            "Epoch [166/250], Loss: 1.0361\n",
            "Epoch [167/250], Loss: 1.0369\n",
            "Epoch [168/250], Loss: 1.0389\n",
            "Epoch [169/250], Loss: 1.0392\n",
            "Epoch [170/250], Loss: 1.0380\n",
            "Epoch [171/250], Loss: 1.0392\n",
            "Epoch [172/250], Loss: 1.0366\n",
            "Epoch [173/250], Loss: 1.0369\n",
            "Epoch [174/250], Loss: 1.0400\n",
            "Epoch [175/250], Loss: 1.0400\n",
            "Epoch [176/250], Loss: 1.0361\n",
            "Epoch [177/250], Loss: 1.0406\n",
            "Epoch [178/250], Loss: 1.0355\n",
            "Epoch [179/250], Loss: 1.0386\n",
            "Epoch [180/250], Loss: 1.0397\n",
            "Epoch [181/250], Loss: 1.0372\n",
            "Epoch [182/250], Loss: 1.0358\n",
            "Epoch [183/250], Loss: 1.0386\n",
            "Epoch [184/250], Loss: 1.0375\n",
            "Epoch [185/250], Loss: 1.0378\n",
            "Epoch [186/250], Loss: 1.0414\n",
            "Epoch [187/250], Loss: 1.0380\n",
            "Epoch [188/250], Loss: 1.0395\n",
            "Epoch [189/250], Loss: 1.0406\n",
            "Epoch [190/250], Loss: 1.0406\n",
            "Epoch [191/250], Loss: 1.0378\n",
            "Epoch [192/250], Loss: 1.0397\n",
            "Epoch [193/250], Loss: 1.0338\n",
            "Epoch [194/250], Loss: 1.0383\n",
            "Epoch [195/250], Loss: 1.0394\n",
            "Epoch [196/250], Loss: 1.0341\n",
            "Epoch [197/250], Loss: 1.0397\n",
            "Epoch [198/250], Loss: 1.0375\n",
            "Epoch [199/250], Loss: 1.0389\n",
            "Epoch [200/250], Loss: 1.0397\n",
            "Epoch [201/250], Loss: 1.0375\n",
            "Epoch [202/250], Loss: 1.0417\n",
            "Epoch [203/250], Loss: 1.0366\n",
            "Epoch [204/250], Loss: 1.0389\n",
            "Epoch [205/250], Loss: 1.0369\n",
            "Epoch [206/250], Loss: 1.0383\n",
            "Epoch [207/250], Loss: 1.0386\n",
            "Epoch [208/250], Loss: 1.0375\n",
            "Epoch [209/250], Loss: 1.0375\n",
            "Epoch [210/250], Loss: 1.0375\n",
            "Epoch [211/250], Loss: 1.0372\n",
            "Epoch [212/250], Loss: 1.0395\n",
            "Epoch [213/250], Loss: 1.0378\n",
            "Epoch [214/250], Loss: 1.0392\n",
            "Epoch [215/250], Loss: 1.0369\n",
            "Epoch [216/250], Loss: 1.0414\n",
            "Epoch [217/250], Loss: 1.0386\n",
            "Epoch [218/250], Loss: 1.0386\n",
            "Epoch [219/250], Loss: 1.0417\n",
            "Epoch [220/250], Loss: 1.0378\n",
            "Epoch [221/250], Loss: 1.0394\n",
            "Epoch [222/250], Loss: 1.0341\n",
            "Epoch [223/250], Loss: 1.0369\n",
            "Epoch [224/250], Loss: 1.0408\n",
            "Epoch [225/250], Loss: 1.0378\n",
            "Epoch [226/250], Loss: 1.0380\n",
            "Epoch [227/250], Loss: 1.0338\n",
            "Epoch [228/250], Loss: 1.0352\n",
            "Epoch [229/250], Loss: 1.0344\n",
            "Epoch [230/250], Loss: 1.0408\n",
            "Epoch [231/250], Loss: 1.0378\n",
            "Epoch [232/250], Loss: 1.0383\n",
            "Epoch [233/250], Loss: 1.0344\n",
            "Epoch [234/250], Loss: 1.0369\n",
            "Epoch [235/250], Loss: 1.0347\n",
            "Epoch [236/250], Loss: 1.0344\n",
            "Epoch [237/250], Loss: 1.0358\n",
            "Epoch [238/250], Loss: 1.0406\n",
            "Epoch [239/250], Loss: 1.0378\n",
            "Epoch [240/250], Loss: 1.0366\n",
            "Epoch [241/250], Loss: 1.0403\n",
            "Epoch [242/250], Loss: 1.0361\n",
            "Epoch [243/250], Loss: 1.0336\n",
            "Epoch [244/250], Loss: 1.0352\n",
            "Epoch [245/250], Loss: 1.0352\n",
            "Epoch [246/250], Loss: 1.0338\n",
            "Epoch [247/250], Loss: 1.0394\n",
            "Epoch [248/250], Loss: 1.0358\n",
            "Epoch [249/250], Loss: 1.0397\n",
            "Epoch [250/250], Loss: 1.0386\n",
            "Test Accuracy: 67.92%\n"
          ]
        }
      ]
    }
  ]
}